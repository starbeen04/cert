"""
ì´ˆì •ë°€ PDF êµ¬ì¡° ë¶„ì„ê¸° - ì‹¤ì œ PDFì— ë§ì¶˜ ìƒì„¸ ë¶„ì„
ì‚¬ìš©ì ìš”êµ¬ì‚¬í•­: í›¨ì”¬ ë” êµ¬ì²´ì ì´ê³  ìì„¸í•œ êµ¬ì¡° íŒŒì•…
"""

import asyncio
import json
import re
import time
from typing import Dict, List, Any, Optional, Tuple
from pathlib import Path
import fitz  # PyMuPDF
import openai
from PIL import Image
import io
import base64

class UltraPrecisePDFAnalyzer:
    """ì´ˆì •ë°€ PDF êµ¬ì¡° ë¶„ì„ê¸° - ì‹¤ì œ PDF ë§ì¶¤ ìƒì„¸ ë¶„ì„"""
    
    def __init__(self, openai_client):
        self.openai_client = openai_client
    
    async def analyze_pdf_structure_ultra_detailed(self, pdf_path: str, upload_id: int, 
                                                  progress_callback=None) -> Dict[str, Any]:
        """í–¥ìƒëœ 2ë‹¨ê³„ ì´ˆì •ë°€ PDF êµ¬ì¡° ë¶„ì„ - ì „ì²´ ê°œê´„ í›„ ìƒì„¸ ë¶„ì„"""
        
        def update_progress(percentage, step, description=""):
            if progress_callback:
                progress_callback(percentage, step, description)
            print(f"ğŸ“Š ì§„í–‰ë¥ : [{'â–ˆ' * (percentage // 10)}{'â–‘' * (10 - percentage // 10)}] {percentage}% - {step}")
            if description:
                print(f"   {description}")
        
        print(f"ğŸš€ í–¥ìƒëœ 2ë‹¨ê³„ ì´ˆì •ë°€ PDF êµ¬ì¡° ë¶„ì„ ì‹œì‘ - Upload {upload_id}")
        print("=" * 70)
        update_progress(0, "ë¶„ì„ ì¤€ë¹„ ì¤‘...", "ìƒˆë¡œìš´ 2ë‹¨ê³„ ì „ì²´-ìƒì„¸ ë¶„ì„ ë°©ì‹")
        
        try:
            # === STAGE 1: ì „ì²´ PDF ê°œê´„ ë¶„ì„ (0-50%) ===
            update_progress(5, "PDF ê¸°ë³¸ ì •ë³´ ì¶”ì¶œ ì¤‘...", "ë©”íƒ€ë°ì´í„°, í˜ì´ì§€ ìˆ˜, í…ìŠ¤íŠ¸ íŒ¨í„´ ë¶„ì„")
            basic_info = await self._extract_basic_pdf_info(pdf_path)
            
            update_progress(10, "í…ìŠ¤íŠ¸ ê¸°ë°˜ ì‚¬ì „ ë¶„ì„ ì¤‘...", "ì „ì²´ PDF í…ìŠ¤íŠ¸ì—ì„œ ë¬¸ì œ íŒ¨í„´ ì¶”ì¶œ")
            cross_page_analysis = await self._analyze_cross_page_issues_from_text(pdf_path)
            basic_info['cross_page_issues'] = cross_page_analysis
            
            # ğŸ†• 1ë‹¨ê³„: ì „ì²´ PDF í•œ ëˆˆì— ë³´ê¸° (ì €í•´ìƒë„ ì „ì²´ ê°œê´„)
            update_progress(20, "ì „ì²´ PDF ê°œê´„ ë¶„ì„ ì¤‘...", "ëª¨ë“  í˜ì´ì§€ ì €í•´ìƒë„ë¡œ ì „ì²´ êµ¬ì¡° íŒŒì•…")
            overview_result = await self._perform_full_pdf_overview(pdf_path, basic_info, upload_id)
            
            if not overview_result.get("success"):
                print("âš ï¸ ì „ì²´ ê°œê´„ ë¶„ì„ ì‹¤íŒ¨, ê¸°ì¡´ ë°©ì‹ìœ¼ë¡œ ì§„í–‰")
                overview_result = {"global_overview": {}, "success": True}
            
            # ğŸ†• ê°œê´„ ë¶„ì„ ê¸°ë°˜ìœ¼ë¡œ ê¸°ë³¸ ì •ë³´ ë³´ì •
            update_progress(30, "ê°œê´„ ë¶„ì„ ê²°ê³¼ ì ìš© ì¤‘...", "ì „ì²´ íŒŒì•… ê²°ê³¼ë¡œ ë¬¸ì œ ìˆ˜ ë° êµ¬ì¡° ë³´ì •")
            enhanced_basic_info = await self._enhance_basic_info_with_overview(
                basic_info, overview_result, pdf_path
            )
            
            # === STAGE 2: ìƒì„¸ ì •ë°€ ë¶„ì„ (50-100%) ===
            update_progress(50, "ì´ˆê³ í•´ìƒë„ ì´ë¯¸ì§€ ìƒì„± ì¤‘...", f"12ë°° í™•ëŒ€ ì´ë¯¸ì§€ ìƒì„± ({enhanced_basic_info['total_pages']}í˜ì´ì§€)")
            ultra_high_res_images = await self._create_ultra_high_res_images(pdf_path, upload_id, scale=12.0)
            
            update_progress(65, "ê°œê´„ ê¸°ë°˜ ì •ë°€ êµ¬ì¡° ë¶„ì„ ì¤‘...", "ì „ì²´ íŒŒì•… ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì„¸ë¶€ ë¶„ì„")
            detailed_structure = await self._perform_overview_guided_analysis(
                ultra_high_res_images, enhanced_basic_info, overview_result, upload_id
            )
            
            if not detailed_structure.get("success"):
                return detailed_structure
            
            update_progress(80, "íŠ¹ìˆ˜ ìš”ì†Œ ì •ë°€ ë¶„ì„ ì¤‘...", "ê°œê´„ì—ì„œ ì‹ë³„ëœ íŠ¹ìˆ˜ ìš”ì†Œ ì§‘ì¤‘ ë¶„ì„")
            special_analysis_result = await self._perform_special_elements_analysis(
                pdf_path, detailed_structure, upload_id
            )
            
            update_progress(92, "2ë‹¨ê³„ ë¶„ì„ ê²°ê³¼ í†µí•© ì¤‘...", "ê°œê´„ + ìƒì„¸ + íŠ¹ìˆ˜ ìš”ì†Œ ë¶„ì„ ì¢…í•©")
            integrated_structure = await self._integrate_all_analysis_results(
                overview_result, detailed_structure, special_analysis_result, upload_id
            )
            
            update_progress(97, "ìµœì¢… ê²€ì¦ ë° ë³´ì™„ ì¤‘...", "2ë‹¨ê³„ ë¶„ì„ ê²°ê³¼ êµì°¨ ê²€ì¦")
            validated_structure = await self._validate_and_enhance_structure(
                integrated_structure, pdf_path, upload_id
            )
            
            update_progress(100, "2ë‹¨ê³„ êµ¬ì¡° ë¶„ì„ ì™„ë£Œ!", "ì „ì²´ ê°œê´„ + ìƒì„¸ ì •ë°€ ë¶„ì„ ì„±ê³µ ì™„ë£Œ")
            print("âœ… í–¥ìƒëœ 2ë‹¨ê³„ ì´ˆì •ë°€ PDF êµ¬ì¡° ë¶„ì„ ì™„ë£Œ:")
            self._print_detailed_analysis_result(validated_structure)
            
            return validated_structure
            
        except Exception as e:
            print(f"âŒ í–¥ìƒëœ 2ë‹¨ê³„ êµ¬ì¡° ë¶„ì„ ì‹¤íŒ¨: {e}")
            return {"success": False, "error": str(e), "upload_id": upload_id}
    
    def _detect_question_count_from_text(self, pages_text: List[Dict]) -> int:
        """í…ìŠ¤íŠ¸ì—ì„œ ì‹¤ì œ ë¬¸ì œ ê°œìˆ˜ ë™ì  ê°ì§€"""
        
        print("ğŸ” ì‹¤ì œ ë¬¸ì œ ê°œìˆ˜ ê°ì§€ ì¤‘...")
        
        # ì „ì²´ í…ìŠ¤íŠ¸ í•©ì¹˜ê¸°
        full_text = ""
        for page_info in pages_text:
            full_text += page_info.get('text', '') + "\n"
        
        # ë‹¤ì–‘í•œ ë¬¸ì œ ë²ˆí˜¸ íŒ¨í„´ ì°¾ê¸°
        question_numbers = set()
        
        # íŒ¨í„´ 1: "ë¬¸ì œ 1ë²ˆ", "ë¬¸ì œ 2ë²ˆ" ë“±
        pattern1 = re.findall(r'ë¬¸ì œ\s*(\d+)\s*ë²ˆ', full_text)
        for num in pattern1:
            question_numbers.add(int(num))
        
        # íŒ¨í„´ 2: "1.", "2.", "3." ë“± (ë” ìœ ì—°í•˜ê²Œ)
        pattern2 = re.findall(r'(?:^|\s)(\d+)\.(?:\s|$)', full_text, re.MULTILINE)
        for num in pattern2:
            if 1 <= int(num) <= 100:  # 1-100 ë²”ìœ„ë§Œ
                question_numbers.add(int(num))
        
        # íŒ¨í„´ 3: "Q1", "Q2", "ë¬¸1", "ë¬¸2" ë“±
        pattern3 = re.findall(r'[QQë¬¸](\d+)', full_text)
        for num in pattern3:
            if 1 <= int(num) <= 100:
                question_numbers.add(int(num))
        
        # íŒ¨í„´ 4: "1)", "2)", "3)" ë“± (ì„ íƒì§€ê°€ ì•„ë‹Œ ë¬¸ì œ ë²ˆí˜¸)
        pattern4 = re.findall(r'(?:^|\n)\s*(\d+)\)', full_text, re.MULTILINE)
        for num in pattern4:
            if 1 <= int(num) <= 100:
                question_numbers.add(int(num))
        
        # íŒ¨í„´ 5: "[1]", "[2]", "[3]" ë“± 
        pattern5 = re.findall(r'\[(\d+)\]', full_text)
        for num in pattern5:
            if 1 <= int(num) <= 100:
                question_numbers.add(int(num))
        
        # íŒ¨í„´ 6: í•œì ë¬¸ì œ ë²ˆí˜¸ "ä¸€", "äºŒ", "ä¸‰" ë“±ì„ ìˆ«ìë¡œ ë³€í™˜
        chinese_numbers = {'ä¸€': 1, 'äºŒ': 2, 'ä¸‰': 3, 'å››': 4, 'äº”': 5, 'å…­': 6, 'ä¸ƒ': 7, 'å…«': 8, 'ä¹': 9, 'å': 10}
        for chinese, number in chinese_numbers.items():
            if chinese in full_text:
                question_numbers.add(number)
        
        # ì—°ì†ì ì¸ ë¬¸ì œ ë²ˆí˜¸ ë²”ìœ„ ì°¾ê¸°
        if question_numbers:
            sorted_numbers = sorted(question_numbers)
            max_continuous = self._find_max_continuous_sequence(sorted_numbers)
            max_number = max(sorted_numbers)
            
            # ë” ë‚˜ì€ ì¶”ì •ì¹˜ ì„ íƒ
            final_count = max(max_continuous, max_number)
            
            print(f"   ğŸ“Š ê°ì§€ëœ ë¬¸ì œ ë²ˆí˜¸ë“¤: {sorted(list(question_numbers))}")
            print(f"   ğŸ“ˆ ì—°ì†ì ì¸ ë¬¸ì œ ìˆ˜: {max_continuous}")
            print(f"   ğŸ”¢ ìµœëŒ€ ë¬¸ì œ ë²ˆí˜¸: {max_number}")
            print(f"   ğŸ¯ ìµœì¢… ì¶”ì •: {final_count}ê°œ")
            
            return final_count
        
        # íŒ¨í„´ì„ ì°¾ì§€ ëª»í•œ ê²½ìš°, ì„ íƒì§€ íŒ¨í„´ìœ¼ë¡œ ì¶”ì •
        choice_patterns = len(re.findall(r'[â‘ â‘¡â‘¢â‘£â‘¤]', full_text))
        estimated_from_choices = choice_patterns // 4  # í‰ê·  4ê°œ ì„ íƒì§€ ê°€ì •
        
        print(f"   ğŸ” ì„ íƒì§€ íŒ¨í„´ìœ¼ë¡œ ì¶”ì •: {estimated_from_choices}ê°œ ë¬¸ì œ")
        return max(estimated_from_choices, 20)  # ìµœì†Œ 20ê°œëŠ” ë³´ì¥
    
    def _find_max_continuous_sequence(self, numbers: List[int]) -> int:
        """ì—°ì†ì ì¸ ìˆ«ì ì‹œí€€ìŠ¤ ì¤‘ ìµœëŒ€ ê¸¸ì´ ì°¾ê¸°"""
        
        if not numbers:
            return 0
            
        # 1ë¶€í„° ì‹œì‘í•˜ëŠ” ì—°ì† ì‹œí€€ìŠ¤ ì°¾ê¸°
        max_continuous = 0
        current_continuous = 0
        
        for i in range(1, max(numbers) + 1):
            if i in numbers:
                current_continuous += 1
                max_continuous = max(max_continuous, current_continuous)
            else:
                current_continuous = 0
        
        # ì—°ì†ì„±ì´ ì—†ìœ¼ë©´ ìµœëŒ€ ë²ˆí˜¸ë¥¼ ë°˜í™˜
        if max_continuous == 0:
            return max(numbers)
            
        return max_continuous

    async def _extract_basic_pdf_info(self, pdf_path: str) -> Dict[str, Any]:
        """PDF ê¸°ë³¸ ì •ë³´ ì¶”ì¶œ"""
        
        try:
            doc = fitz.open(pdf_path)
            
            # ë©”íƒ€ë°ì´í„° ì¶”ì¶œ
            metadata = doc.metadata
            
            # í˜ì´ì§€ë³„ ê¸°ë³¸ í…ìŠ¤íŠ¸ ì¶”ì¶œ (êµ¬ì¡° íŒíŠ¸ìš©)
            pages_text = []
            for page_num in range(len(doc)):
                page = doc.load_page(page_num)
                text = page.get_text()
                pages_text.append({
                    'page_number': page_num + 1,
                    'text_length': len(text),
                    'line_count': len(text.split('\n')),
                    'has_korean': bool(re.search(r'[ê°€-í£]', text)),
                    'has_numbers': bool(re.search(r'\d+', text)),
                    'has_choices': bool(re.search(r'[â‘ â‘¡â‘¢â‘£â‘¤]', text)),
                    'sample_text': text[:200].replace('\n', ' ').strip()
                })
            
            doc.close()
            
            # ì‹¤ì œ ë¬¸ì œ ê°œìˆ˜ ë™ì  ê°ì§€
            detected_question_count = self._detect_question_count_from_text(pages_text)
            
            return {
                'total_pages': len(pages_text),
                'detected_question_count': detected_question_count,  # ë™ì  ê°ì§€ëœ ë¬¸ì œ ìˆ˜
                'metadata': metadata,
                'pages_info': pages_text,
                'file_size_mb': Path(pdf_path).stat().st_size / (1024 * 1024)
            }
            
        except Exception as e:
            print(f"âš ï¸ PDF ê¸°ë³¸ ì •ë³´ ì¶”ì¶œ ì‹¤íŒ¨: {e}")
            return {'total_pages': 0, 'error': str(e)}
    
    async def _analyze_cross_page_issues_from_text(self, pdf_path: str) -> Dict[str, Any]:
        """í–¥ìƒëœ í…ìŠ¤íŠ¸ ê¸°ë°˜ í˜ì´ì§€ ê²½ê³„ ë¬¸ì œ ì‚¬ì „ ë¶„ì„"""
        
        try:
            doc = fitz.open(pdf_path)
            cross_page_issues = {
                "split_questions": [],
                "incomplete_choices": [],
                "continuation_markers": [],
                "text_code_elements": [],  # ìƒˆë¡œìš´: í…ìŠ¤íŠ¸/ì½”ë“œ ìš”ì†Œ ê°ì§€
                "cross_page_tables": [],   # ìƒˆë¡œìš´: í˜ì´ì§€ ê°„ í‘œ ë¶„í•  
                "text_analysis": [],
                "detailed_issues": []      # ìƒˆë¡œìš´: ìƒì„¸ ì´ìŠˆ ë¶„ì„
            }
            
            for page_num in range(len(doc)):
                page = doc.load_page(page_num)
                text = page.get_text()
                lines = text.split('\n')
                
                # 1. ê¸°ì¡´ ê¸°ëŠ¥ ê°•í™” - í˜ì´ì§€ ëì—ì„œ ëŠì–´ì§€ëŠ” ë¬¸ì œ ê°ì§€
                if len(lines) > 0:
                    last_lines = lines[-10:]  # ë§ˆì§€ë§‰ 10ì¤„ë¡œ í™•ì¥
                    first_lines_next_page = []
                    
                    # ë‹¤ìŒ í˜ì´ì§€ ì²« 10ì¤„ ê°€ì ¸ì˜¤ê¸°
                    if page_num + 1 < len(doc):
                        next_page = doc.load_page(page_num + 1)
                        next_text = next_page.get_text()
                        first_lines_next_page = next_text.split('\n')[:10]
                    
                    for i, line in enumerate(last_lines):
                        # ë¬¸ì œê°€ ëŠì–´ì§„ ê²½ìš° ë” ì •ë°€í•˜ê²Œ ê°ì§€
                        if re.search(r'ë¬¸ì œ\s*\d+ë²ˆ', line) and i >= len(last_lines) - 3:
                            # í•´ë‹¹ ë¬¸ì œì˜ ì™„ì„±ë„ ì²´í¬
                            question_text = line
                            choice_count = len(re.findall(r'[â‘ â‘¡â‘¢â‘£â‘¤]', '\n'.join(last_lines[i:])))
                            
                            if choice_count < 2:  # ì„ íƒì§€ê°€ ì¶©ë¶„í•˜ì§€ ì•Šìœ¼ë©´ ë¶„í•  ì˜ì‹¬
                                cross_page_issues["split_questions"].append({
                                    "page": page_num + 1,
                                    "issue": f"ë¬¸ì œê°€ í˜ì´ì§€ {page_num + 1}ì—ì„œ ë‹¤ìŒ í˜ì´ì§€ë¡œ ì—°ê²°",
                                    "question_number": re.search(r'(\d+)', line).group(1) if re.search(r'\d+', line) else "?",
                                    "partial_text": line.strip(),
                                    "choice_count_current": choice_count,
                                    "likely_continues": True
                                })
                        
                        # ì„ íƒì§€ ë¶„í•  ë” ì •ë°€í•˜ê²Œ ê°ì§€
                        current_choices = re.findall(r'[â‘ â‘¡â‘¢â‘£â‘¤]', line)
                        if current_choices:
                            # í˜„ì¬ í˜ì´ì§€ì—ì„œ ë§ˆì§€ë§‰ìœ¼ë¡œ ë‚˜íƒ€ë‚œ ì„ íƒì§€ ë²ˆí˜¸
                            last_choice_num = max([ord(c) - ord('â‘ ') + 1 for c in current_choices])
                            
                            # ë‹¤ìŒ í˜ì´ì§€ì—ì„œ ì—°ì†ë˜ëŠ” ì„ íƒì§€ í™•ì¸
                            if first_lines_next_page:
                                next_page_choices = []
                                for next_line in first_lines_next_page:
                                    choices_in_line = re.findall(r'[â‘ â‘¡â‘¢â‘£â‘¤]', next_line)
                                    next_page_choices.extend(choices_in_line)
                                
                                if next_page_choices:
                                    next_choice_nums = [ord(c) - ord('â‘ ') + 1 for c in next_page_choices]
                                    # ì—°ì†ì„± í™•ì¸ (í˜„ì¬ í˜ì´ì§€ ë§ˆì§€ë§‰ + 1 = ë‹¤ìŒ í˜ì´ì§€ ì²«ë²ˆì§¸)
                                    if min(next_choice_nums) == last_choice_num + 1:
                                        cross_page_issues["incomplete_choices"].append({
                                            "page": page_num + 1,
                                            "issue": f"ì„ íƒì§€ê°€ í˜ì´ì§€ {page_num + 1}-{page_num + 2}ì— ë¶„í• ",
                                            "current_choices": current_choices,
                                            "current_max": last_choice_num,
                                            "next_page_choices": next_page_choices,
                                            "next_min": min(next_choice_nums),
                                            "is_sequential": True
                                        })
                
                # 2. ìƒˆë¡œìš´ ê¸°ëŠ¥: í…ìŠ¤íŠ¸/ì½”ë“œ ìš”ì†Œ ê°ì§€ (ì„ íƒì§€ ë‚´)
                self._detect_text_code_in_choices(text, page_num + 1, cross_page_issues)
                
                # 3. ìƒˆë¡œìš´ ê¸°ëŠ¥: í˜ì´ì§€ ê°„ í‘œ ë¶„í•  ê°ì§€
                self._detect_cross_page_tables(text, first_lines_next_page, page_num + 1, cross_page_issues)
                
                # 4. ê¸°ì¡´ ì—°ì† í‘œì‹œ ê°ì§€ ê°•í™”
                continuation_patterns = [
                    r'(ê³„ì†|ë‹¤ìŒ\s*í˜ì´ì§€|continued|â†’|\.{3,}|â€¦)',
                    r'(ë‹¤ìŒ\s*ë¬¸ì œ\s*ê³„ì†|ì´ì–´ì„œ|ê³„ì†í•´ì„œ)',
                    r'(\*\s*ê³„ì†|\[ê³„ì†\]|\(ê³„ì†\))'
                ]
                
                for pattern in continuation_patterns:
                    markers = re.findall(pattern, text, re.IGNORECASE)
                    if markers:
                        cross_page_issues["continuation_markers"].append({
                            "page": page_num + 1,
                            "markers": markers,
                            "pattern_type": pattern
                        })
                
                # 5. ê°•í™”ëœ í…ìŠ¤íŠ¸ ë¶„ì„
                cross_page_issues["text_analysis"].append({
                    "page": page_num + 1,
                    "has_incomplete_questions": bool(re.search(r'ë¬¸ì œ\s*\d+ë²ˆ[^â‘ â‘¡â‘¢â‘£â‘¤]*$', text)),
                    "has_incomplete_choices": len(re.findall(r'[â‘ â‘¡â‘¢â‘£â‘¤]', text)) % 4 != 0 and bool(re.search(r'[â‘ â‘¡â‘¢â‘£â‘¤]', text)),
                    "ends_abruptly": bool(re.search(r'[â‘ â‘¡â‘¢â‘£â‘¤][^â‘ â‘¡â‘¢â‘£â‘¤\n]*$', text)),
                    "has_code_blocks": bool(re.search(r'(class|def|function|public|private|int|string|return)', text)),
                    "has_mathematical_formulas": bool(re.search(r'[âˆ‘âˆâˆ«âˆšÂ±âˆâ‰ â‰¤â‰¥âˆˆâˆ‰âˆªâˆ©]|x\^|[a-z]\s*=\s*[0-9]', text)),
                    "has_english_text": bool(re.search(r'[a-zA-Z]{4,}', text)),
                    "has_tables": bool(re.search(r'\|\s*[ê°€-í£a-zA-Z0-9\s]+\s*\|', text))
                })
            
            doc.close()
            
            # 6. ì¢…í•©ì ì¸ ì´ìŠˆ ë¶„ì„
            self._analyze_comprehensive_issues(cross_page_issues)
            
            return cross_page_issues
            
        except Exception as e:
            print(f"âš ï¸ í–¥ìƒëœ í˜ì´ì§€ ê²½ê³„ ë¶„ì„ ì‹¤íŒ¨: {e}")
            return {"error": str(e)}
    
    def _detect_text_code_in_choices(self, text: str, page_num: int, issues: Dict):
        """ì„ íƒì§€ ë‚´ í…ìŠ¤íŠ¸/ì½”ë“œ ìš”ì†Œ ê°ì§€"""
        
        lines = text.split('\n')
        for line_idx, line in enumerate(lines):
            # ì„ íƒì§€ê°€ ìˆëŠ” ì¤„ ì°¾ê¸°
            if re.search(r'[â‘ â‘¡â‘¢â‘£â‘¤]', line):
                # ê° ì„ íƒì§€ë³„ë¡œ ë¶„ì„
                choices = re.findall(r'[â‘ â‘¡â‘¢â‘£â‘¤][^â‘ â‘¡â‘¢â‘£â‘¤]*', line)
                
                for choice in choices:
                    choice_num = choice[0]
                    choice_content = choice[1:].strip()
                    
                    detected_elements = []
                    
                    # ì½”ë“œ ìš”ì†Œ ê°ì§€ (í–¥ìƒë¨)
                    if re.search(r'(class|def|function|public|private|int|string|return|printf|cout|System\.out|SELECT|FROM|WHERE|INSERT|UPDATE|DELETE|CREATE|DROP|ALTER|JOIN)', choice_content):
                        detected_elements.append("í”„ë¡œê·¸ë˜ë°/SQL ì½”ë“œ")
                    
                    # ìˆ˜í•™ ê³µì‹ ê°ì§€
                    if re.search(r'[âˆ‘âˆâˆ«âˆšÂ±âˆâ‰ â‰¤â‰¥âˆˆâˆ‰âˆªâˆ©]|x\^|[a-z]\s*=\s*[0-9]|log|sin|cos|tan', choice_content):
                        detected_elements.append("ìˆ˜í•™ ê³µì‹")
                    
                    # ì˜ì–´ ê¸°ìˆ  ìš©ì–´ ê°ì§€
                    if re.search(r'[A-Z]{2,}|[a-z]+[A-Z][a-z]*|algorithm|database|network|server', choice_content):
                        detected_elements.append("ì˜ì–´ ê¸°ìˆ  ìš©ì–´")
                    
                    # ìˆ«ìì™€ ì—°ì‚° ê¸°í˜¸ ê°ì§€ (í–¥ìƒë¨)
                    if re.search(r'\d+\s*[+\-*/=<>!]\s*\d+|[0-9]+\.[0-9]+|%|[><]=?|\d+\s*(MB|KB|GB|Hz|GHz|ms|ns)|0x[0-9A-Fa-f]+|\d+bit|\d+byte', choice_content):
                        detected_elements.append("ìˆ«ì/ì—°ì‚°/ë‹¨ìœ„")
                    
                    # íŠ¹ìˆ˜ ê¸°í˜¸ ê°ì§€
                    if re.search(r'[â†’â†â†‘â†“âŸµâŸ¶âŸ·]|[â—†â—‡â– â–¡â—â—‹â–²â–³â–¼â–½]', choice_content):
                        detected_elements.append("íŠ¹ìˆ˜ ê¸°í˜¸/ë„í˜•")
                    
                    if detected_elements:
                        issues["text_code_elements"].append({
                            "page": page_num,
                            "line": line_idx + 1,
                            "choice": choice_num,
                            "elements": detected_elements,
                            "content_preview": choice_content[:50] + ("..." if len(choice_content) > 50 else "")
                        })
    
    def _detect_cross_page_tables(self, current_text: str, next_page_lines: List[str], 
                                  page_num: int, issues: Dict):
        """í˜ì´ì§€ ê°„ í‘œ ë¶„í•  ê°ì§€"""
        
        # í˜„ì¬ í˜ì´ì§€ì—ì„œ í‘œì˜ ì‹œì‘/ì¤‘ê°„/ë ê°ì§€
        table_patterns = [
            r'\|[^|]*\|[^|]*\|',  # ê¸°ë³¸ í‘œ íŒ¨í„´
            r'â”Œ[â”€â”¬]*â”|â”œ[â”€â”¼]*â”¤|â””[â”€â”´]*â”˜',  # ë°•ìŠ¤ í‘œ íŒ¨í„´
            r'â”€{3,}|â•{3,}',  # êµ¬ë¶„ì„ 
        ]
        
        current_has_table = False
        table_incomplete = False
        
        for pattern in table_patterns:
            if re.search(pattern, current_text):
                current_has_table = True
                
                # í‘œê°€ ì™„ì „í•˜ì§€ ì•Šì€ì§€ í™•ì¸ (í—¤ë”ë§Œ ìˆê³  ë°ì´í„°ê°€ ì—†ëŠ” ê²½ìš°)
                lines = current_text.split('\n')
                table_lines = [line for line in lines if re.search(pattern, line)]
                
                if len(table_lines) <= 2:  # í—¤ë”ì™€ êµ¬ë¶„ì„ ë§Œ ìˆëŠ” ê²½ìš°
                    table_incomplete = True
                    break
        
        if current_has_table and table_incomplete and next_page_lines:
            # ë‹¤ìŒ í˜ì´ì§€ì—ì„œ í‘œì˜ ì—°ì† í™•ì¸
            next_page_text = '\n'.join(next_page_lines)
            next_has_table_continuation = False
            
            for pattern in table_patterns:
                if re.search(pattern, next_page_text):
                    next_has_table_continuation = True
                    break
            
            if next_has_table_continuation:
                issues["cross_page_tables"].append({
                    "page": page_num,
                    "issue": f"í‘œê°€ í˜ì´ì§€ {page_num}ì—ì„œ {page_num + 1}ë¡œ ë¶„í• ",
                    "table_type": "ë°ì´í„° í…Œì´ë¸”",
                    "current_page_has_header": True,
                    "next_page_has_data": True
                })
    
    def _analyze_comprehensive_issues(self, issues: Dict):
        """ì¢…í•©ì ì¸ ì´ìŠˆ ë¶„ì„ ë° ìš°ì„ ìˆœìœ„ ì„¤ì •"""
        
        total_issues = len(issues["split_questions"]) + len(issues["incomplete_choices"]) + \
                      len(issues["cross_page_tables"]) + len(issues["text_code_elements"])
        
        severity_level = "low"
        if total_issues >= 10:
            severity_level = "high"
        elif total_issues >= 5:
            severity_level = "medium"
        
        issues["detailed_issues"] = {
            "total_cross_page_problems": total_issues,
            "severity_level": severity_level,
            "requires_special_processing": severity_level != "low",
            "processing_recommendations": [
                "í˜ì´ì§€ ê²½ê³„ë¥¼ ê³ ë ¤í•œ ë¬¸ì œ ì¬êµ¬ì„±" if issues["split_questions"] else None,
                "ì„ íƒì§€ ì™„ì „ì„± ê²€ì¦ í•„ìš”" if issues["incomplete_choices"] else None,
                "í‘œ ë°ì´í„° í†µí•© ì²˜ë¦¬ í•„ìš”" if issues["cross_page_tables"] else None,
                "ì„ íƒì§€ ë‚´ ì½”ë“œ/ìˆ˜ì‹ íŠ¹ë³„ ì²˜ë¦¬" if issues["text_code_elements"] else None
            ]
        }
        
        # None ê°’ ì œê±°
        issues["detailed_issues"]["processing_recommendations"] = [
            rec for rec in issues["detailed_issues"]["processing_recommendations"] if rec
        ]
    
    def _format_cross_page_analysis_for_prompt(self, cross_page_issues: Dict) -> str:
        """í–¥ìƒëœ í˜ì´ì§€ ê²½ê³„ ë¶„ì„ ê²°ê³¼ë¥¼ í”„ë¡¬í”„íŠ¸ìš©ìœ¼ë¡œ í¬ë§·"""
        
        if not cross_page_issues or "error" in cross_page_issues:
            return "- í˜ì´ì§€ ê²½ê³„ ì‚¬ì „ ë¶„ì„ ê²°ê³¼ ì—†ìŒ"
        
        formatted = []
        
        # ë¶„í• ëœ ë¬¸ì œë“¤
        if cross_page_issues.get("split_questions"):
            formatted.append("âš ï¸ í˜ì´ì§€ ê°„ ë¶„í•  ë¬¸ì œ ê°ì§€:")
            for issue in cross_page_issues["split_questions"]:
                formatted.append(f"  - ë¬¸ì œ {issue['question_number']}ë²ˆ: {issue['issue']}")
                formatted.append(f"    í˜„ì¬ í˜ì´ì§€ ì„ íƒì§€ ìˆ˜: {issue['choice_count_current']}ê°œ")
        
        # ë¶ˆì™„ì „í•œ ì„ íƒì§€ë“¤  
        if cross_page_issues.get("incomplete_choices"):
            formatted.append("âš ï¸ ì„ íƒì§€ í˜ì´ì§€ ë¶„í•  ê°ì§€:")
            for issue in cross_page_issues["incomplete_choices"]:
                formatted.append(f"  - {issue['issue']}")
                if issue.get("is_sequential"):
                    formatted.append(f"    ì—°ì†ì„± í™•ì¸ë¨: {issue['current_max']} â†’ {issue['next_min']}")
        
        # ìƒˆë¡œìš´: í…ìŠ¤íŠ¸/ì½”ë“œ ìš”ì†Œë“¤
        if cross_page_issues.get("text_code_elements"):
            formatted.append("ğŸ” ì„ íƒì§€ ë‚´ íŠ¹ìˆ˜ ìš”ì†Œ ê°ì§€:")
            choice_elements = {}
            for element in cross_page_issues["text_code_elements"]:
                page = element["page"]
                if page not in choice_elements:
                    choice_elements[page] = []
                choice_elements[page].append(f"ì„ íƒì§€ {element['choice']}: {', '.join(element['elements'])}")
            
            for page, elements in choice_elements.items():
                formatted.append(f"  - í˜ì´ì§€ {page}:")
                for elem in elements:
                    formatted.append(f"    â€¢ {elem}")
        
        # ìƒˆë¡œìš´: í˜ì´ì§€ ê°„ í‘œ ë¶„í• 
        if cross_page_issues.get("cross_page_tables"):
            formatted.append("ğŸ“Š í˜ì´ì§€ ê°„ í‘œ ë¶„í•  ê°ì§€:")
            for table in cross_page_issues["cross_page_tables"]:
                formatted.append(f"  - {table['issue']}")
                formatted.append(f"    í‘œ ìœ í˜•: {table['table_type']}")
        
        # ì—°ì† í‘œì‹œìë“¤
        if cross_page_issues.get("continuation_markers"):
            formatted.append("ğŸ“ ì—°ì† í‘œì‹œì ê°ì§€:")
            for marker_info in cross_page_issues["continuation_markers"]:
                formatted.append(f"  - í˜ì´ì§€ {marker_info['page']}: {marker_info['markers']}")
        
        # ì¢…í•© ì´ìŠˆ ë¶„ì„
        if cross_page_issues.get("detailed_issues"):
            details = cross_page_issues["detailed_issues"]
            formatted.append(f"ğŸ“Š ì¢…í•© ë¶„ì„: ì´ {details['total_cross_page_problems']}ê°œ ì´ìŠˆ ({details['severity_level']} ì‹¬ê°ë„)")
            if details.get("processing_recommendations"):
                formatted.append("   ê¶Œì¥ ì²˜ë¦¬ ë°©ì‹:")
                for rec in details["processing_recommendations"]:
                    formatted.append(f"    â€¢ {rec}")
        
        if not formatted:
            formatted.append("âœ… í˜ì´ì§€ ê²½ê³„ ë¬¸ì œ ì—†ìŒ - ê° í˜ì´ì§€ê°€ ë…ë¦½ì ìœ¼ë¡œ ì™„ì„±ë¨")
        
        return "\n".join(formatted)
    
    async def _create_ultra_high_res_images(self, pdf_path: str, upload_id: int, scale: float = 8.0) -> List[Dict]:
        """êµ¬ì¡° ë¶„ì„ìš© í˜ì´ì§€ ì´ë¯¸ì§€ ìƒì„± (ê°€ë³€ í•´ìƒë„)"""
        
        try:
            doc = fitz.open(pdf_path)
            ultra_high_res_images = []
            
            for page_num in range(len(doc)):
                page = doc.load_page(page_num)
                
                # ê°€ë³€ í•´ìƒë„ë¡œ ë Œë”ë§
                mat = fitz.Matrix(scale, scale)
                pix = page.get_pixmap(matrix=mat, alpha=False)
                
                # Base64 ì¸ì½”ë”©
                img_data = self._optimize_image_size(pix)
                base64_img = base64.b64encode(img_data).decode('utf-8')
                
                ultra_high_res_images.append({
                    'page_number': page_num + 1,
                    'width': pix.width,
                    'height': pix.height,
                    'image_data': base64_img,
                    'format': 'png'
                })
                
                print(f"   ğŸ“„ {scale}ë°° í•´ìƒë„ í˜ì´ì§€ {page_num + 1} ìƒì„±: {pix.width}x{pix.height}")
            
            doc.close()
            return ultra_high_res_images
            
        except Exception as e:
            print(f"âŒ êµ¬ì¡° ë¶„ì„ìš© ì´ë¯¸ì§€ ìƒì„± ì‹¤íŒ¨: {e}")
            return []
    
    async def _perform_full_pdf_overview(self, pdf_path: str, basic_info: Dict, upload_id: int) -> Dict[str, Any]:
        """1ë‹¨ê³„: ì „ì²´ PDF ê°œê´„ ë¶„ì„ - ì €í•´ìƒë„ë¡œ ëª¨ë“  í˜ì´ì§€ í•œëˆˆì— íŒŒì•…"""
        
        print(f"ğŸŒ ì „ì²´ PDF ê°œê´„ ë¶„ì„ ì‹œì‘ - Upload {upload_id}")
        
        try:
            # ì €í•´ìƒë„ ì´ë¯¸ì§€ ìƒì„± (ë¹ ë¥¸ ì „ì²´ íŒŒì•…ìš©)
            overview_images = await self._create_ultra_high_res_images(pdf_path, upload_id, scale=2.0)
            
            # ì „ì²´ ê°œê´„ ë¶„ì„ í”„ë¡¬í”„íŠ¸
            overview_prompt = self._create_full_overview_prompt(basic_info)
            
            messages = [
                {
                    "role": "system",
                    "content": """ë‹¹ì‹ ì€ PDF ì „ì²´ êµ¬ì¡° íŒŒì•… ì „ë¬¸ê°€ì…ë‹ˆë‹¤.

ğŸ¯ **í•µì‹¬ ì„ë¬´**: ì „ì²´ PDFë¥¼ í•œëˆˆì— ë³´ê³  ì •í™•í•œ ë¬¸ì œ ê°œìˆ˜ì™€ êµ¬ì¡°ë¥¼ íŒŒì•…í•˜ì„¸ìš”.

âš¡ **ì¤‘ìš” ì§€ì¹¨**:
- ëª¨ë“  í˜ì´ì§€ë¥¼ ì¢…í•©ì ìœ¼ë¡œ ë³´ê³  ì‹¤ì œ ë¬¸ì œ ê°œìˆ˜ë¥¼ ì •í™•íˆ ì„¸ì–´ì£¼ì„¸ìš”
- "ë¬¸ì œ 1ë²ˆ"ë¶€í„° "ë¬¸ì œ Në²ˆ"ê¹Œì§€ ì—°ì†ì ìœ¼ë¡œ ì°¾ìœ¼ì„¸ìš”
- ê° í˜ì´ì§€ê°€ ì–´ë–¤ ì—­í• ì¸ì§€ ì •í™•íˆ êµ¬ë¶„í•˜ì„¸ìš” (ë¬¸ì œ/ì •ë‹µ/í•´ì„¤)
- ì¤‘ë³µ ê³„ì‚°í•˜ì§€ ë§ˆì„¸ìš” - ì‹¤ì œ ë¬¸ì œ ë²ˆí˜¸ë§Œ ì„¸ì–´ì£¼ì„¸ìš”

ğŸš¨ **ë§¤ìš° ì¤‘ìš” - ì„ íƒì§€ êµ¬ë¶„**:
- â‘ â‘¡â‘¢â‘£â‘¤ ê°™ì€ ë™ê·¸ë¼ë¯¸ ìˆ«ìëŠ” ì„ íƒì§€ì…ë‹ˆë‹¤! ë¬¸ì œê°€ ì•„ë‹™ë‹ˆë‹¤!
- 1)2)3)4)5) ê°™ì€ ê´„í˜¸ ìˆ«ìë„ ì„ íƒì§€ì…ë‹ˆë‹¤! ë¬¸ì œê°€ ì•„ë‹™ë‹ˆë‹¤!
- A)B)C)D)E) ê°™ì€ ì•ŒíŒŒë²³ë„ ì„ íƒì§€ì…ë‹ˆë‹¤! ë¬¸ì œê°€ ì•„ë‹™ë‹ˆë‹¤!
- ì„ íƒì§€ë¥¼ ì ˆëŒ€ ë³„ê°œ ë¬¸ì œë¡œ ì¹´ìš´íŠ¸í•˜ì§€ ë§ˆì„¸ìš”!

ğŸ” **íŠ¹ë³„ ì£¼ì˜ì‚¬í•­**:
- ì •ë‹µë§Œ ë‚˜ì—´ëœ í˜ì´ì§€ëŠ” ë¬¸ì œ ê°œìˆ˜ì— í¬í•¨í•˜ì§€ ë§ˆì„¸ìš”
- í•´ì„¤ í˜ì´ì§€ë„ ë¬¸ì œ ê°œìˆ˜ì— í¬í•¨í•˜ì§€ ë§ˆì„¸ìš”  
- ì‹¤ì œ ë¬¸ì œ ë²ˆí˜¸ê°€ ì íŒ ë¬¸ì œë§Œ ì¹´ìš´íŠ¸í•˜ì„¸ìš”
- ê³„ì¸µ êµ¬ì¡°ë¥¼ ì •í™•íˆ ì¸ì‹í•˜ì„¸ìš”: ë¬¸ì œ â†’ ì§€ë¬¸ â†’ ì„ íƒì§€ë“¤"""
                },
                {
                    "role": "user",
                    "content": [{"type": "text", "text": overview_prompt}]
                }
            ]
            
            # ëª¨ë“  í˜ì´ì§€ ì´ë¯¸ì§€ ì¶”ê°€
            for img_data in overview_images:
                messages[1]["content"].append({
                    "type": "image_url",
                    "image_url": {
                        "url": f"data:image/png;base64,{img_data['image_data']}",
                        "detail": "low"  # ì „ì²´ ê°œê´„ì€ ì €í•´ìƒë„ë¡œ ë¹ ë¥´ê²Œ
                    }
                })
            
            # GPT Vision í˜¸ì¶œ
            response = await self.openai_client.chat.completions.create(
                model="gpt-4o",
                messages=messages,
                max_tokens=3000,
                temperature=0.1  # ì •í™•ì„±ì„ ìœ„í•œ ë‚®ì€ ì˜¨ë„
            )
            
            response_text = response.choices[0].message.content
            print(f"ğŸŒ ì „ì²´ ê°œê´„ ë¶„ì„ ì‘ë‹µ ê¸¸ì´: {len(response_text)}ì")
            
            # ê²°ê³¼ íŒŒì‹±
            overview_result = self._parse_overview_response(response_text)
            
            print("âœ… ì „ì²´ PDF ê°œê´„ ë¶„ì„ ì™„ë£Œ")
            return {"global_overview": overview_result, "success": True}
            
        except Exception as e:
            print(f"âŒ ì „ì²´ PDF ê°œê´„ ë¶„ì„ ì‹¤íŒ¨: {e}")
            return {"success": False, "error": str(e)}
    
    def _create_full_overview_prompt(self, basic_info: Dict) -> str:
        """ì „ì²´ PDF ê°œê´„ ë¶„ì„ í”„ë¡¬í”„íŠ¸"""
        
        detected_count = basic_info.get('detected_question_count', 0)
        total_pages = basic_info.get('total_pages', 0)
        
        return f"""ğŸŒ ì „ì²´ PDF êµ¬ì¡° ê°œê´„ ë¶„ì„

ğŸ“‹ **ê¸°ë³¸ ì •ë³´**:
- ì´ í˜ì´ì§€: {total_pages}í˜ì´ì§€
- í…ìŠ¤íŠ¸ ê¸°ë°˜ ì˜ˆìƒ ë¬¸ì œ ìˆ˜: {detected_count}ê°œ

ğŸ¯ **ë¶„ì„ ëª©í‘œ (ë§¤ìš° ìƒì„¸íˆ)**:
1. **ì •í™•í•œ ì´ ë¬¸ì œ ê°œìˆ˜ íŒŒì•…**: "ë¬¸ì œ 1ë²ˆ"ë¶€í„° "ë¬¸ì œ Në²ˆ"ê¹Œì§€ ì •í™•íˆ ëª‡ ê°œì¸ì§€
2. **í˜ì´ì§€ë³„ ë¬¸ì œ ë²”ìœ„**: ê° í˜ì´ì§€ì— ëª‡ ë²ˆë¶€í„° ëª‡ ë²ˆê¹Œì§€ ìˆëŠ”ì§€
3. **í˜ì´ì§€ ì—­í•  ë¶„ë¥˜**: ë¬¸ì œ/ì •ë‹µ/í•´ì„¤/í‘œì§€/ê¸°íƒ€ ì •í™•íˆ êµ¬ë¶„
4. **ì„ íƒì§€ ê°œìˆ˜ íŒŒì•…**: ê° ë¬¸ì œì˜ ì„ íƒì§€ê°€ ëª‡ ê°œì¸ì§€ (2ê°œ/4ê°œ/5ê°œ ë“±)
5. **íŠ¹ìˆ˜ ìš”ì†Œ ìœ„ì¹˜**: í‘œ/ì´ë¯¸ì§€/ì½”ë“œ/ë‹¤ì´ì–´ê·¸ë¨ì´ ì •í™•íˆ ëª‡ ë²ˆ ë¬¸ì œì— ìˆëŠ”ì§€
6. **í˜ì´ì§€ ê²½ê³„ ë¬¸ì œ**: ë¬¸ì œë‚˜ ì„ íƒì§€ê°€ í˜ì´ì§€ë¥¼ ë„˜ë‚˜ë“œëŠ”ì§€ í™•ì¸

ğŸ” **ì •í™•í•œ ë¶„ì„ ë°©ë²•**:

**1. ë¬¸ì œ ë²ˆí˜¸ ì‹ë³„ (ì´ê²ƒë§Œ ì§„ì§œ ë¬¸ì œ):**
- "ë¬¸ì œ 1ë²ˆ", "ë¬¸ì œ 2ë²ˆ", "1.", "2.", "Q1", "Q2" ë“±ì˜ íŒ¨í„´
- ë§ˆì§€ë§‰ ë¬¸ì œ ë²ˆí˜¸ê°€ ì‹¤ì œ ì´ ë¬¸ì œ ê°œìˆ˜ì…ë‹ˆë‹¤

**2. ì„ íƒì§€ ì‹ë³„ (ë¬¸ì œê°€ ì•„ë‹˜!):**
- â‘ , â‘¡, â‘¢, â‘£, â‘¤ (ë™ê·¸ë¼ë¯¸ ìˆ«ì)
- 1), 2), 3), 4), 5) (ê´„í˜¸ ìˆ«ì)
- A), B), C), D), E) (ì•ŒíŒŒë²³)
- ì´ëŸ° ê²ƒë“¤ì€ ë¬¸ì œê°€ ì•„ë‹ˆë¼ ì„ íƒì§€ì…ë‹ˆë‹¤!

**3. ê³„ì¸µ êµ¬ì¡° ì¸ì‹:**
- ë¬¸ì œ ë²ˆí˜¸ â†’ ë¬¸ì œ ì§€ë¬¸ â†’ ì„ íƒì§€ë“¤
- ì„ íƒì§€ëŠ” í•´ë‹¹ ë¬¸ì œì˜ í•˜ìœ„ í•­ëª©ì…ë‹ˆë‹¤
- ì„ íƒì§€ë¥¼ ë³„ë„ ë¬¸ì œë¡œ ì ˆëŒ€ ì¹´ìš´íŠ¸í•˜ì§€ ë§ˆì„¸ìš”

**4. í˜ì´ì§€ ì—­í•  êµ¬ë¶„:**
- ì •ë‹µë§Œ ìˆëŠ” í˜ì´ì§€ë‚˜ í•´ì„¤ë§Œ ìˆëŠ” í˜ì´ì§€ëŠ” êµ¬ë¶„í•´ì£¼ì„¸ìš”

ğŸ“Š **JSON ì¶œë ¥ í˜•ì‹** (ë§¤ìš° ì •í™•í•˜ê²Œ):
```json
{{
  "actual_total_questions": ì‹¤ì œ_ë¬¸ì œ_ê°œìˆ˜,
  "last_question_number": ë§ˆì§€ë§‰_ë¬¸ì œ_ë²ˆí˜¸,
  "first_question_number": ì²«_ë¬¸ì œ_ë²ˆí˜¸,
  "page_roles": [
    {{
      "page_number": 1,
      "role": "questions|answers|explanations|cover|other",
      "question_range": "1-12ë²ˆ" (ì •í™•í•œ_ë²”ìœ„),
      "question_count": ì‹¤ì œ_ì´_í˜ì´ì§€_ë¬¸ì œ_ê°œìˆ˜,
      "choice_counts": {{"2ì„ íƒì§€": 0, "4ì„ íƒì§€": 10, "5ì„ íƒì§€": 2}},
      "special_elements": {{"tables": [ë¬¸ì œë²ˆí˜¸ë“¤], "images": [ë¬¸ì œë²ˆí˜¸ë“¤], "codes": [ë¬¸ì œë²ˆí˜¸ë“¤], "diagrams": [ë¬¸ì œë²ˆí˜¸ë“¤]}},
      "cross_page_issues": ["ë¬¸ì œê°€_ë„˜ì–´ê°€ëŠ”_ê²½ìš°"],
      "content_summary": "ì´_í˜ì´ì§€ì˜_êµ¬ì²´ì _ë‚´ìš©"
    }}
  ],
  "detailed_question_info": {{
    "questions_with_images": [ë¬¸ì œë²ˆí˜¸ë“¤],
    "questions_with_tables": [ë¬¸ì œë²ˆí˜¸ë“¤], 
    "questions_with_codes": [ë¬¸ì œë²ˆí˜¸ë“¤],
    "questions_with_diagrams": [ë¬¸ì œë²ˆí˜¸ë“¤],
    "choice_distribution": {{"2ì„ íƒì§€": ê°œìˆ˜, "4ì„ íƒì§€": ê°œìˆ˜, "5ì„ íƒì§€": ê°œìˆ˜}}
  }},
  "document_structure": {{
    "question_pages": [í˜ì´ì§€_ë²ˆí˜¸ë“¤],
    "answer_pages": [í˜ì´ì§€_ë²ˆí˜¸ë“¤],
    "explanation_pages": [í˜ì´ì§€_ë²ˆí˜¸ë“¤]
  }},
  "verification": {{
    "total_questions_verified": true/false,
    "page_continuity_checked": true/false,
    "special_elements_located": true/false
  }},
  "confidence": 0.0-1.0,
  "notes": "ìƒì„¸í•œ_êµ¬ì¡°_ì„¤ëª…"
}}
```

âš ï¸ **ë§¤ìš° ì¤‘ìš” - ì •í™•í•œ ë¶„ì„**:
- ì¶”ì •í•˜ì§€ ë§ˆì„¸ìš”! ì‹¤ì œë¡œ ë³´ì´ëŠ” ë¬¸ì œ ë²ˆí˜¸ë§Œ ì„¸ì–´ì£¼ì„¸ìš”
- ì •ë‹µ í˜ì´ì§€ì˜ "1ë²ˆ: â‘¡" ê°™ì€ ê±´ ë¬¸ì œê°€ ì•„ë‹™ë‹ˆë‹¤
- í•´ì„¤ í˜ì´ì§€ì˜ "ë¬¸ì œ 1ë²ˆ í•´ì„¤" ê°™ì€ ê²ƒë„ ì‹¤ì œ ë¬¸ì œê°€ ì•„ë‹™ë‹ˆë‹¤
- ì˜¤ì§ ì‹¤ì œ ë¬¸ì œ í…ìŠ¤íŠ¸ì™€ ì„ íƒì§€ê°€ ìˆëŠ” ê²ƒë§Œ ë¬¸ì œë¡œ ì¹´ìš´íŠ¸í•˜ì„¸ìš”

ğŸš¨ **ì„ íƒì§€ ì˜¤ì¸ì‹ ë°©ì§€ - ì ˆëŒ€ ì¤€ìˆ˜**:
- â‘ , â‘¡, â‘¢, â‘£, â‘¤ ëŠ” ì„ íƒì§€ì…ë‹ˆë‹¤! ë¬¸ì œ ë²ˆí˜¸ê°€ ì•„ë‹™ë‹ˆë‹¤!
- 1), 2), 3), 4), 5) ëŠ” ì„ íƒì§€ì…ë‹ˆë‹¤! ë¬¸ì œ ë²ˆí˜¸ê°€ ì•„ë‹™ë‹ˆë‹¤!
- A), B), C), D), E) ëŠ” ì„ íƒì§€ì…ë‹ˆë‹¤! ë¬¸ì œ ë²ˆí˜¸ê°€ ì•„ë‹™ë‹ˆë‹¤!
- ì´ëŸ° ê²ƒë“¤ì„ ë³„ê°œ ë¬¸ì œë¡œ ì ˆëŒ€ ì¹´ìš´íŠ¸í•˜ì§€ ë§ˆì„¸ìš”!

ğŸ“‹ **ì²´ê³„ì  ë¶„ì„ ë°©ë²•**:
1. ê° í˜ì´ì§€ë¥¼ ì°¨ë¡€ëŒ€ë¡œ ìŠ¤ìº”í•˜ë©´ì„œ "ë¬¸ì œ Në²ˆ" íŒ¨í„´ ì°¾ê¸°
2. ê° ë¬¸ì œì˜ ì„ íƒì§€ ê°œìˆ˜ ì •í™•íˆ ì„¸ê¸° (â‘ â‘¡â‘¢â‘£ ë˜ëŠ” 1)2)3)4) ë“±)
3. í‘œ, ì´ë¯¸ì§€, ì½”ë“œ, ë‹¤ì´ì–´ê·¸ë¨ì´ ìˆëŠ” ë¬¸ì œ ë²ˆí˜¸ ê¸°ë¡
4. í˜ì´ì§€ ëì—ì„œ ë¬¸ì œê°€ ì¤‘ê°„ì— ëŠì–´ì§€ëŠ”ì§€ í™•ì¸
5. ë‹¤ìŒ í˜ì´ì§€ ì‹œì‘ì´ ì´ì „ í˜ì´ì§€ì™€ ì—°ê²°ë˜ëŠ”ì§€ í™•ì¸

ğŸ” **ê²€ì¦ í•„ìˆ˜ì‚¬í•­**:
- ì²« ë¬¸ì œë¶€í„° ë§ˆì§€ë§‰ ë¬¸ì œê¹Œì§€ ì—°ì†ì„± í™•ì¸
- ëˆ„ë½ëœ ë¬¸ì œ ë²ˆí˜¸ê°€ ìˆëŠ”ì§€ í™•ì¸
- ê° í˜ì´ì§€ì˜ ë¬¸ì œ ê°œìˆ˜ í•©ì´ ì „ì²´ì™€ ì¼ì¹˜í•˜ëŠ”ì§€ í™•ì¸"""
    
    def _parse_overview_response(self, response_text: str) -> Dict:
        """ì „ì²´ ê°œê´„ ë¶„ì„ ê²°ê³¼ íŒŒì‹±"""
        
        try:
            # JSON ë¸”ë¡ ì°¾ê¸°
            json_match = re.search(r'```json\s*(.*?)\s*```', response_text, re.DOTALL)
            if json_match:
                json_text = json_match.group(1)
            else:
                # ì²« ë²ˆì§¸ { ë¶€í„° ë§ˆì§€ë§‰ } ê¹Œì§€ ì¶”ì¶œ
                start = response_text.find('{')
                end = response_text.rfind('}')
                if start != -1 and end != -1:
                    json_text = response_text[start:end+1]
                else:
                    return self._create_fallback_overview()
            
            # JSON íŒŒì‹±
            overview_data = json.loads(json_text)
            
            # ê²°ê³¼ ìš”ì•½ ì¶œë ¥
            actual_questions = overview_data.get('actual_total_questions', 0)
            confidence = overview_data.get('confidence', 0.0)
            
            print(f"ğŸŒ ì „ì²´ ê°œê´„ ë¶„ì„ ê²°ê³¼:")
            print(f"   ğŸ“Š ì‹¤ì œ ì´ ë¬¸ì œ ìˆ˜: {actual_questions}ê°œ")
            print(f"   ğŸ¯ ë§ˆì§€ë§‰ ë¬¸ì œ ë²ˆí˜¸: {overview_data.get('last_question_number', '?')}ë²ˆ")
            print(f"   ğŸ” ì²« ë¬¸ì œ ë²ˆí˜¸: {overview_data.get('first_question_number', '?')}ë²ˆ")
            print(f"   ğŸ” ë¶„ì„ ì‹ ë¢°ë„: {confidence*100:.1f}%")
            
            # ìƒì„¸ ì •ë³´ ì¶œë ¥
            detailed_info = overview_data.get('detailed_question_info', {})
            if detailed_info:
                print(f"   ğŸ“Š íŠ¹ìˆ˜ ìš”ì†Œ:")
                if detailed_info.get('questions_with_tables'):
                    print(f"      ğŸ“‹ í‘œ í¬í•¨ ë¬¸ì œ: {detailed_info['questions_with_tables']}")
                if detailed_info.get('questions_with_images'): 
                    print(f"      ğŸ–¼ï¸ ì´ë¯¸ì§€ í¬í•¨ ë¬¸ì œ: {detailed_info['questions_with_images']}")
                if detailed_info.get('questions_with_codes'):
                    print(f"      ğŸ’» ì½”ë“œ í¬í•¨ ë¬¸ì œ: {detailed_info['questions_with_codes']}")
                if detailed_info.get('questions_with_diagrams'):
                    print(f"      ğŸ“ˆ ë‹¤ì´ì–´ê·¸ë¨ í¬í•¨ ë¬¸ì œ: {detailed_info['questions_with_diagrams']}")
                
                choice_dist = detailed_info.get('choice_distribution', {})
                if choice_dist:
                    print(f"   ğŸ“Š ì„ íƒì§€ ë¶„í¬: {choice_dist}")
            
            # í˜ì´ì§€ ì—­í•  ìš”ì•½
            page_roles = overview_data.get('page_roles', [])
            question_pages = len([p for p in page_roles if p.get('role') == 'questions'])
            answer_pages = len([p for p in page_roles if p.get('role') == 'answers'])
            explanation_pages = len([p for p in page_roles if p.get('role') == 'explanations'])
            
            print(f"   ğŸ“„ í˜ì´ì§€ êµ¬ì„±: ë¬¸ì œ {question_pages}í˜ì´ì§€, ì •ë‹µ {answer_pages}í˜ì´ì§€, í•´ì„¤ {explanation_pages}í˜ì´ì§€")
            
            return overview_data
            
        except Exception as e:
            print(f"âš ï¸ ì „ì²´ ê°œê´„ ë¶„ì„ íŒŒì‹± ì‹¤íŒ¨: {e}")
            return self._create_fallback_overview()
    
    def _create_fallback_overview(self) -> Dict:
        """ê°œê´„ ë¶„ì„ ì‹¤íŒ¨ì‹œ ê¸°ë³¸ê°’"""
        return {
            "actual_total_questions": 60,  # ê¸°ë³¸ê°’ (ì¼ë°˜ì ì¸ ê¸°ì¶œë¬¸ì œ ìˆ˜)
            "last_question_number": 60,
            "first_question_number": 1,
            "page_roles": [
                {"page_number": 1, "role": "questions", "question_count": 12},
                {"page_number": 2, "role": "questions", "question_count": 12},
                {"page_number": 3, "role": "questions", "question_count": 12},
                {"page_number": 4, "role": "questions", "question_count": 12},
                {"page_number": 5, "role": "questions", "question_count": 12}
            ],
            "document_structure": {
                "question_pages": [1, 2, 3, 4, 5],
                "answer_pages": [6],
                "explanation_pages": [7, 8]
            },
            "detailed_question_info": {
                "questions_with_tables": [],
                "questions_with_images": [],
                "questions_with_codes": [],
                "questions_with_diagrams": [],
                "choice_distribution": {"4ì„ íƒì§€": 60}
            },
            "confidence": 0.3,
            "notes": "ê°œê´„ ë¶„ì„ ì‹¤íŒ¨ë¡œ í‘œì¤€ êµ¬ì¡° ê¸°ë³¸ê°’ ì‚¬ìš©"
        }
    
    async def _enhance_basic_info_with_overview(self, basic_info: Dict, overview_result: Dict, pdf_path: str) -> Dict:
        """ê°œê´„ ë¶„ì„ ê²°ê³¼ë¡œ ê¸°ë³¸ ì •ë³´ ë³´ì •"""
        
        enhanced_info = basic_info.copy()
        
        if overview_result.get("success"):
            global_overview = overview_result.get("global_overview", {})
            
            # ì‹¤ì œ ë¬¸ì œ ê°œìˆ˜ë¡œ ì—…ë°ì´íŠ¸
            actual_questions = global_overview.get("actual_total_questions", 0)
            if actual_questions > 0:
                enhanced_info['detected_question_count'] = actual_questions
                enhanced_info['overview_corrected_count'] = actual_questions
                print(f"ğŸ”§ ê°œê´„ ë¶„ì„ìœ¼ë¡œ ë¬¸ì œ ìˆ˜ ë³´ì •: {basic_info.get('detected_question_count', 0)} â†’ {actual_questions}")
            
            # í˜ì´ì§€ ì—­í•  ì •ë³´ ì¶”ê°€
            enhanced_info['page_roles'] = global_overview.get("page_roles", [])
            enhanced_info['document_structure'] = global_overview.get("document_structure", {})
            enhanced_info['global_overview'] = global_overview
        
        return enhanced_info
    
    async def _perform_overview_guided_analysis(self, ultra_high_res_images: List[Dict], 
                                               enhanced_basic_info: Dict, overview_result: Dict, 
                                               upload_id: int) -> Dict[str, Any]:
        """ê°œê´„ ë¶„ì„ ê¸°ë°˜ ì •ë°€ êµ¬ì¡° ë¶„ì„"""
        
        print(f"ğŸ” ê°œê´„ ê¸°ë°˜ ì •ë°€ êµ¬ì¡° ë¶„ì„ ì‹œì‘ - Upload {upload_id}")
        
        try:
            # ê°œê´„ ë¶„ì„ ì •ë³´ í™œìš©í•œ ì •ë°€ í”„ë¡¬í”„íŠ¸
            guided_prompt = self._create_overview_guided_prompt(enhanced_basic_info, overview_result)
            
            messages = [
                {
                    "role": "system",
                    "content": """ë‹¹ì‹ ì€ ì „ì²´ ê°œê´„ì„ ë°”íƒ•ìœ¼ë¡œ ì •ë°€ ë¶„ì„í•˜ëŠ” ì „ë¬¸ê°€ì…ë‹ˆë‹¤.

ğŸ¯ **í•µì‹¬ ì„ë¬´**: ì´ë¯¸ íŒŒì•…ëœ ì „ì²´ êµ¬ì¡°ë¥¼ ë°”íƒ•ìœ¼ë¡œ ê° í˜ì´ì§€ë¥¼ ì •ë°€ ë¶„ì„í•˜ì„¸ìš”.

âš¡ **ì¤‘ìš” ì§€ì¹¨**:
- ì „ì²´ ê°œê´„ì—ì„œ íŒŒì•…ëœ ì •í™•í•œ ë¬¸ì œ ê°œìˆ˜ë¥¼ ê¸°ì¤€ìœ¼ë¡œ ë¶„ì„í•˜ì„¸ìš”
- ê° í˜ì´ì§€ì˜ ì—­í• (ë¬¸ì œ/ì •ë‹µ/í•´ì„¤)ì´ ì´ë¯¸ íŒŒì•…ë˜ì—ˆìœ¼ë¯€ë¡œ ì´ë¥¼ í™•ì¸í•˜ê³  ë³´ì™„í•˜ì„¸ìš”
- ê³ í•´ìƒë„ ì´ë¯¸ì§€ë¡œ ì„¸ë¶€ ë‚´ìš©ì„ ì •í™•íˆ ë¶„ì„í•˜ì„¸ìš”
- íŠ¹ìˆ˜ ìš”ì†Œ(í‘œ, ì´ë¯¸ì§€, ì½”ë“œ)ì˜ ì •í™•í•œ ìœ„ì¹˜ì™€ ë‚´ìš©ì„ íŒŒì•…í•˜ì„¸ìš”"""
                },
                {
                    "role": "user",
                    "content": [{"type": "text", "text": guided_prompt}]
                }
            ]
            
            # ì´ˆê³ í•´ìƒë„ ì´ë¯¸ì§€ ì¶”ê°€
            for img_data in ultra_high_res_images:
                messages[1]["content"].append({
                    "type": "image_url",
                    "image_url": {
                        "url": f"data:image/png;base64,{img_data['image_data']}",
                        "detail": "high"  # ì •ë°€ ë¶„ì„ì„ ìœ„í•œ ê³ í•´ìƒë„
                    }
                })
            
            # GPT Vision í˜¸ì¶œ
            response = await self.openai_client.chat.completions.create(
                model="gpt-4o",
                messages=messages,
                max_tokens=6000,
                temperature=0.05
            )
            
            response_text = response.choices[0].message.content
            print(f"ğŸ” ì •ë°€ êµ¬ì¡° ë¶„ì„ ì‘ë‹µ ê¸¸ì´: {len(response_text)}ì")
            
            # ê²°ê³¼ íŒŒì‹± ë° ê°œê´„ ì •ë³´ í¬í•¨
            detailed_result = self._parse_ultra_detailed_response(response_text, upload_id)
            
            if detailed_result.get('success'):
                detailed_result['basic_info'] = enhanced_basic_info
                detailed_result['guided_by_overview'] = True
            
            print("âœ… ê°œê´„ ê¸°ë°˜ ì •ë°€ êµ¬ì¡° ë¶„ì„ ì™„ë£Œ")
            return detailed_result
            
        except Exception as e:
            error_str = str(e)
            print(f"âŒ ê°œê´„ ê¸°ë°˜ ì •ë°€ êµ¬ì¡° ë¶„ì„ ì‹¤íŒ¨: {e}")
            
            # ì´ë¯¸ì§€ í¬ê¸° ê´€ë ¨ ì—ëŸ¬ ì²˜ë¦¬
            if "400" in error_str or "invalid_request_error" in error_str or "something went wrong" in error_str:
                print("âš ï¸ ì´ë¯¸ì§€ í¬ê¸° ì´ˆê³¼ë¡œ ì¸í•œ API ìš”ì²­ ì‹¤íŒ¨ë¡œ ì¶”ì •ë©ë‹ˆë‹¤.")
                print("ğŸ”§ í•´ìƒë„ë¥¼ ë‚®ì¶°ì„œ ë‹¤ì‹œ ì‹œë„í•˜ê±°ë‚˜ ì´ë¯¸ì§€ ì••ì¶•ì„ ê°•í™”í•´ì•¼ í•©ë‹ˆë‹¤.")
                return {"success": False, "error": "Image size too large for API", "error_type": "image_size_exceeded", "upload_id": upload_id}
            
            return {"success": False, "error": error_str, "upload_id": upload_id}
    
    def _create_overview_guided_prompt(self, enhanced_basic_info: Dict, overview_result: Dict) -> str:
        """ê°œê´„ ë¶„ì„ ê¸°ë°˜ ì •ë°€ ë¶„ì„ í”„ë¡¬í”„íŠ¸"""
        
        global_overview = overview_result.get("global_overview", {})
        actual_questions = global_overview.get("actual_total_questions", 0)
        page_roles = global_overview.get("page_roles", [])
        
        # í˜ì´ì§€ ì—­í•  ìš”ì•½
        page_roles_summary = ""
        for page_role in page_roles[:5]:  # ì²˜ìŒ 5í˜ì´ì§€ë§Œ í‘œì‹œ
            page_num = page_role.get("page_number", 0)
            role = page_role.get("role", "unknown")
            question_range = page_role.get("question_range", "ì—†ìŒ")
            page_roles_summary += f"- í˜ì´ì§€ {page_num}: {role} ({question_range})\n"
        
        return f"""ğŸ” ì „ì²´ ê°œê´„ ê¸°ë°˜ ì •ë°€ êµ¬ì¡° ë¶„ì„

ğŸ“‹ **ì „ì²´ ê°œê´„ ë¶„ì„ ê²°ê³¼ (í™•ì •ë¨)**:
- ì‹¤ì œ ì´ ë¬¸ì œ ìˆ˜: {actual_questions}ê°œ
- ë§ˆì§€ë§‰ ë¬¸ì œ ë²ˆí˜¸: {global_overview.get('last_question_number', '?')}ë²ˆ
- ë¶„ì„ ì‹ ë¢°ë„: {global_overview.get('confidence', 0.0)*100:.1f}%

ğŸ“„ **í˜ì´ì§€ ì—­í•  (ê°œê´„ì—ì„œ íŒŒì•…ë¨)**:
{page_roles_summary}

ğŸ¯ **ì •ë°€ ë¶„ì„ ëª©í‘œ**:
1. **ê°œê´„ ê²°ê³¼ ê²€ì¦**: ìœ„ì—ì„œ íŒŒì•…ëœ ì •ë³´ê°€ ê³ í•´ìƒë„ì—ì„œë„ ì¼ì¹˜í•˜ëŠ”ì§€ í™•ì¸
2. **ì„¸ë¶€ ë‚´ìš© ë¶„ì„**: ê° ë¬¸ì œì˜ ì •í™•í•œ ë‚´ìš©, ì„ íƒì§€, íŠ¹ìˆ˜ ìš”ì†Œ íŒŒì•…
3. **íŠ¹ìˆ˜ ìš”ì†Œ ìœ„ì¹˜**: í‘œ, ì´ë¯¸ì§€, ì½”ë“œ ë¸”ë¡ì˜ ì •í™•í•œ ë¬¸ì œ ë²ˆí˜¸ ë§¤ì¹­
4. **í’ˆì§ˆ ê²€ì¦**: ê° ë¬¸ì œì˜ ì™„ì„±ë„, ì„ íƒì§€ ê°œìˆ˜, í˜ì´ì§€ ê°„ ì—°ê²°ì„± í™•ì¸

ğŸ” **í˜ì´ì§€ ê²½ê³„ ë¬¸ì œ ì‚¬ì „ ë¶„ì„ ê²°ê³¼** (í…ìŠ¤íŠ¸ ê¸°ë°˜):
{self._format_cross_page_analysis_for_prompt(enhanced_basic_info.get('cross_page_issues', {}))}

âš ï¸ **ë§¤ìš° ì¤‘ìš”**: 
- ì´ ë¬¸ì œ ìˆ˜ {actual_questions}ê°œëŠ” ì´ë¯¸ í™•ì •ëœ ê°’ì…ë‹ˆë‹¤
- ì´ ê°’ê³¼ ë‹¤ë¥´ê²Œ ë¶„ì„ë  ê²½ìš° ê³ í•´ìƒë„ ë¶„ì„ì—ì„œ ëˆ„ë½ì´ë‚˜ ì˜¤ì¸ì‹ì„ ì ê²€í•˜ì„¸ìš”
- ê°œê´„ì—ì„œ íŒŒì•…ëœ í˜ì´ì§€ ì—­í• ì„ ê¸°ì¤€ìœ¼ë¡œ ì •ë°€ ê²€ì¦í•˜ì„¸ìš”

{self._create_detailed_analysis_format_instructions(actual_questions)}"""
    
    def _create_detailed_analysis_format_instructions(self, expected_questions: int) -> str:
        """ìƒì„¸ ë¶„ì„ì„ ìœ„í•œ JSON í˜•ì‹ ì§€ì¹¨"""
        
        return f"""ğŸ¯ **JSON ì¶œë ¥ í˜•ì‹** (ë°˜ë“œì‹œ ì´ êµ¬ì¡°ë¡œ):
```json
{{
  "analysis_summary": {{
    "document_type": "practice_tests",
    "total_pages": ì‹¤ì œ_í˜ì´ì§€_ìˆ˜,
    "total_questions": {expected_questions},
    "confidence_score": 0.9-1.0
  }},
  "page_analysis": [
    {{
      "page_number": 1,
      "page_type": "pure_questions|answer_sheet|explanation_sheet",
      "questions_on_page": [ì‹¤ì œ_ë¬¸ì œ_ë²ˆí˜¸ë“¤],
      "question_density": ì‹¤ì œ_ë¬¸ì œ_ê°œìˆ˜,
      "special_elements": ["tables", "diagrams", "code_blocks", "images"] ì¤‘ í•´ë‹¹ì‚¬í•­,
      "layout_complexity": "simple|medium|complex",
      "processing_notes": "ì´ í˜ì´ì§€ íŠ¹ì´ì‚¬í•­"
    }}
  ],
  "question_analysis": {{
    "total_questions": {expected_questions},
    "numbering_pattern": "ë¬¸ì œ 1ë²ˆ|1.|Q1" ë“± ì‹¤ì œ íŒ¨í„´,
    "choice_pattern": "â‘ â‘¡â‘¢â‘£â‘¤|1)2)3)4)5)" ë“± ì‹¤ì œ íŒ¨í„´,
    "average_choices_per_question": í‰ê· _ì„ íƒì§€_ê°œìˆ˜
  }},
  "processing_strategy": {{
    "recommended_approach": "í˜ì´ì§€ë³„|êµ¬ì¡°ê¸°ë°˜",
    "chunk_size_recommendation": "15-20ë¬¸ì œì”©",
    "special_handling": ["í•„ìš”í•œ_íŠ¹ìˆ˜_ì²˜ë¦¬ë“¤"]
  }}
}}
```

ğŸ“Š **ê²€ì¦ ì²´í¬ë¦¬ìŠ¤íŠ¸**:
- [ ] ì´ ë¬¸ì œ ìˆ˜ê°€ {expected_questions}ê°œì¸ì§€ í™•ì¸
- [ ] ëª¨ë“  í˜ì´ì§€ì˜ ì—­í• ì´ ê°œê´„ ë¶„ì„ê³¼ ì¼ì¹˜í•˜ëŠ”ì§€ í™•ì¸
- [ ] ë¬¸ì œ ë²ˆí˜¸ê°€ 1ë²ˆë¶€í„° {expected_questions}ë²ˆê¹Œì§€ ìˆœì„œëŒ€ë¡œ ìˆëŠ”ì§€ í™•ì¸
- [ ] íŠ¹ìˆ˜ ìš”ì†Œê°€ ìˆëŠ” í˜ì´ì§€ì™€ ë¬¸ì œ ë²ˆí˜¸ê°€ ì •í™•í•œì§€ í™•ì¸

ğŸš¨ **ì„ íƒì§€ ì˜¤ì¸ì‹ ë°©ì§€ ì²´í¬ë¦¬ìŠ¤íŠ¸**:
- [ ] â‘ â‘¡â‘¢â‘£â‘¤ ê°™ì€ ë™ê·¸ë¼ë¯¸ ìˆ«ìê°€ ë³„ê°œ ë¬¸ì œë¡œ ì¹´ìš´íŠ¸ë˜ì§€ ì•Šì•˜ëŠ”ì§€ í™•ì¸
- [ ] 1)2)3)4)5) ê°™ì€ ê´„í˜¸ ìˆ«ìê°€ ë³„ê°œ ë¬¸ì œë¡œ ì¹´ìš´íŠ¸ë˜ì§€ ì•Šì•˜ëŠ”ì§€ í™•ì¸
- [ ] ì„ íƒì§€ ê°œìˆ˜ë§Œí¼ ë¬¸ì œ ìˆ˜ê°€ ëŠ˜ì–´ë‚˜ì§€ ì•Šì•˜ëŠ”ì§€ í™•ì¸
- [ ] ê° ë¬¸ì œê°€ ì •í™•íˆ í•˜ë‚˜ì˜ ë¬¸ì œ ë²ˆí˜¸ë§Œ ê°€ì§€ëŠ”ì§€ í™•ì¸"""
    
    async def _integrate_all_analysis_results(self, overview_result: Dict, detailed_structure: Dict, 
                                            special_analysis: Dict, upload_id: int) -> Dict[str, Any]:
        """ì „ì²´ ê°œê´„ + ì •ë°€ ë¶„ì„ + íŠ¹ìˆ˜ ìš”ì†Œ ë¶„ì„ ê²°ê³¼ í†µí•©"""
        
        print(f"ğŸ”§ 2ë‹¨ê³„ ë¶„ì„ ê²°ê³¼ í†µí•© ì¤‘ - Upload {upload_id}")
        
        try:
            # ì •ë°€ ë¶„ì„ ê²°ê³¼ë¥¼ ë² ì´ìŠ¤ë¡œ ì‚¬ìš©
            integrated = detailed_structure.copy()
            
            # ê°œê´„ ë¶„ì„ ì •ë³´ ì¶”ê°€
            if overview_result.get("success"):
                global_overview = overview_result.get("global_overview", {})
                integrated['global_overview'] = global_overview
                integrated['two_stage_analysis'] = True
                
                # ê°œê´„ ë¶„ì„ì˜ ì‹¤ì œ ë¬¸ì œ ìˆ˜ë¡œ ìµœì¢… í™•ì •
                actual_questions = global_overview.get("actual_total_questions", 0)
                if actual_questions > 0:
                    integrated['analysis_summary']['total_questions'] = actual_questions
                    print(f"ğŸ”§ ê°œê´„ ë¶„ì„ ê¸°ì¤€ìœ¼ë¡œ ìµœì¢… ë¬¸ì œ ìˆ˜ í™•ì •: {actual_questions}ê°œ")
            
            # ê¸°ë³¸ ì •ë³´ì˜ cross_page_issuesë¥¼ ìµœìƒìœ„ë¡œ ë³µì‚¬
            if detailed_structure.get('basic_info', {}).get('cross_page_issues'):
                integrated['cross_page_issues'] = detailed_structure['basic_info']['cross_page_issues']
            
            # íŠ¹ìˆ˜ ìš”ì†Œ ë¶„ì„ ê²°ê³¼ í†µí•©
            special_questions = special_analysis.get('special_analysis', {}).get('special_questions', [])
            if special_questions:
                # ê¸°ì¡´ í†µí•© ë¡œì§ ì‚¬ìš©
                integrated = await self._integrate_structure_and_special_analysis(
                    integrated, special_analysis, upload_id
                )
            
            print("âœ… 2ë‹¨ê³„ ë¶„ì„ ê²°ê³¼ í†µí•© ì™„ë£Œ")
            return integrated
            
        except Exception as e:
            print(f"âŒ 2ë‹¨ê³„ ë¶„ì„ ê²°ê³¼ í†µí•© ì‹¤íŒ¨: {e}")
            return detailed_structure
    
    async def _perform_ultra_detailed_analysis(self, page_images: List[Dict], 
                                             basic_info: Dict, upload_id: int) -> Dict[str, Any]:
        """ì´ˆì •ë°€ êµ¬ì¡° ë¶„ì„ ì‹¤í–‰"""
        
        # ì´ˆì •ë°€ ë¶„ì„ í”„ë¡¬í”„íŠ¸ ìƒì„±
        ultra_detailed_prompt = self._create_ultra_detailed_analysis_prompt(basic_info)
        
        try:
            # ì‹œìŠ¤í…œ ë©”ì‹œì§€ ì¶”ê°€ - ì´ë¯¸ì§€ ê°ì§€ ê°•í™”
            system_message = {
                "role": "system",
                "content": """ë‹¹ì‹ ì€ PDF ë‚´ ì‹œê°ì  ìš”ì†Œ ê°ì§€ ì „ë¬¸ê°€ì…ë‹ˆë‹¤.

ğŸ¯ **í•µì‹¬ ì„ë¬´**: ëª¨ë“  ì‹œê°ì  ìš”ì†Œë¥¼ ë¹ ì§ì—†ì´ ê°ì§€í•˜ê³  ê° ë¬¸ì œë²ˆí˜¸ì™€ ì—°ê²°í•˜ì„¸ìš”.

âš¡ **íŠ¹ë³„ ì§€ì¹¨ - í•œêµ­ì–´ ì‹œí—˜ë¬¸ì œ ì „ë¬¸ ë¶„ì„**:
- "ë¬¸ì œ 1ë²ˆ", "1.", "1)" í˜•ì‹ì˜ ë¬¸ì œ ë²ˆí˜¸ë¥¼ ì°¾ìœ¼ì„¸ìš”
- ê° ë¬¸ì œë²ˆí˜¸ ì£¼ë³€ì˜ ëª¨ë“  ì‹œê°ì  ìš”ì†Œë¥¼ ì •í™•íˆ ê¸°ë¡í•˜ì„¸ìš”
- ì„ íƒì§€ â‘ â‘¡â‘¢â‘£â‘¤ ë‚´ë¶€ë‚˜ ì˜†ì— ìˆëŠ” ì´ë¯¸ì§€ë¥¼ ë°˜ë“œì‹œ ì°¾ìœ¼ì„¸ìš”

ğŸ“¸ **ì´ë¯¸ì§€ ê°ì§€ ìš°ì„ ìˆœìœ„ (ë¬¸ì œë²ˆí˜¸ë³„)**:
1. **ì„ íƒì§€ ì´ë¯¸ì§€**: â‘ , â‘¡, â‘¢, â‘£, â‘¤ ë‚´ë¶€ ë˜ëŠ” ë°”ë¡œ ì˜†ì˜ ëª¨ë“  ê·¸ë˜í”½ ìš”ì†Œ
   - ì‘ì€ ì•„ì´ì½˜, ë„í˜•, ê·¸ë˜í”„, ì‚¬ì§„ê¹Œì§€ ëª¨ë‘ í¬í•¨
   - ìˆ˜ì‹ì´ ì´ë¯¸ì§€ë¡œ ì²˜ë¦¬ëœ ê²ƒë„ í¬í•¨
2. **ë¬¸ì œ ë³¸ë¬¸ ì´ë¯¸ì§€**: ë¬¸ì œ ì„¤ëª…ì— í¬í•¨ëœ ë‹¤ì´ì–´ê·¸ë¨, í‘œ, ê·¸ë˜í”„, ì½”ë“œ
3. **ì°¸ê³  ìë£Œ**: ì§€ë¬¸ì— í¬í•¨ëœ ì‹œê°ì  ìë£Œ

ğŸ” **ë°˜ë“œì‹œ ê°ì§€í•´ì•¼ í•  ìš”ì†Œë“¤**:
- ë„¤íŠ¸ì›Œí¬ ë‹¤ì´ì–´ê·¸ë¨ (ì„œë²„, í´ë¼ì´ì–¸íŠ¸ ì—°ê²°ë„)
- í”Œë¡œì°¨íŠ¸, ìˆœì„œë„
- ë°ì´í„°ë² ì´ìŠ¤ ER ë‹¤ì´ì–´ê·¸ë¨
- ì‹œìŠ¤í…œ ì•„í‚¤í…ì²˜ ë„í‘œ
- í‘œ(í…Œì´ë¸”) - íŠ¹íˆ ìŠ¤ì¼€ì¤„ë§, í”„ë¡œì„¸ìŠ¤ í…Œì´ë¸”
- ì½”ë“œ ë¸”ë¡ (Java, C++, Python ë“±)
- ìˆ˜í•™ ê³µì‹ì´ë‚˜ ìˆ˜ì‹
- ê·¸ë˜í”„, ì°¨íŠ¸ (ë§‰ëŒ€, ì›, ì„  ê·¸ë˜í”„)
- íšŒë¡œë„, ë…¼ë¦¬ê²Œì´íŠ¸
- í™”ë©´ ìº¡ì²˜, UI ëª©ì—…
- ê¸°í•˜í•™ì  ë„í˜•, íŒ¨í„´

âš ï¸ **ë§¤ìš° ì¤‘ìš”**: 
- ì‘ì€ ì´ë¯¸ì§€ë¼ë„ ë°˜ë“œì‹œ ê°ì§€í•˜ì„¸ìš”
- í…ìŠ¤íŠ¸ì²˜ëŸ¼ ë³´ì—¬ë„ ê·¸ë˜í”½ìœ¼ë¡œ ë Œë”ë§ëœ ê²ƒì€ ì´ë¯¸ì§€ì…ë‹ˆë‹¤
- ê° ë¬¸ì œë²ˆí˜¸ì™€ í•´ë‹¹ ì´ë¯¸ì§€ì˜ ì •í™•í•œ ë§¤ì¹­ì´ í•µì‹¬ì…ë‹ˆë‹¤"""
            }
            
            messages = [
                system_message,
                {"role": "user", "content": [{"type": "text", "text": ultra_detailed_prompt}]}
            ]
            
            # ëª¨ë“  í˜ì´ì§€ ì´ë¯¸ì§€ë¥¼ ì‚¬ìš©ì ë©”ì‹œì§€ì— ì¶”ê°€
            for img_data in page_images:
                messages[1]["content"].append({
                    "type": "image_url",
                    "image_url": {
                        "url": f"data:image/png;base64,{img_data['image_data']}",
                        "detail": "high"  # ì´ˆì •ë°€ ë¶„ì„ì„ ìœ„í•œ ê³ í•´ìƒë„
                    }
                })
            
            # GPT Vision í˜¸ì¶œ (ë” í° í† í° í—ˆìš©)
            response = await self.openai_client.chat.completions.create(
                model="gpt-4o",
                messages=messages,
                max_tokens=6000,  # ìƒì„¸ ë¶„ì„ì„ ìœ„í•œ í° í† í°
                temperature=0.05  # ë§¤ìš° ì¼ê´€ëœ ë¶„ì„ì„ ìœ„í•œ ë‚®ì€ ì˜¨ë„
            )
            
            response_text = response.choices[0].message.content
            
            # OpenAI ì‹¤ì œ ì‘ë‹µ í™•ì¸
            print(f"ğŸ¤– OpenAI ì‹¤ì œ ì‘ë‹µ (ì²« 500ì):\n{response_text[:500]}")
            print(f"ğŸ“ ì „ì²´ ì‘ë‹µ ê¸¸ì´: {len(response_text)}ì")
            print(f"ğŸ” JSON ë¸”ë¡ ì°¾ê¸°: {bool(re.search(r'```json', response_text))}")
            
            # JSON íŒŒì‹± ë° êµ¬ì¡°í™”
            parsed_result = self._parse_ultra_detailed_response(response_text, upload_id)
            
            # basic_infoë¥¼ íŒŒì‹± ê²°ê³¼ì— í¬í•¨
            if parsed_result.get('success'):
                parsed_result['basic_info'] = basic_info
                
            return parsed_result
            
        except Exception as e:
            print(f"âŒ ì´ˆì •ë°€ êµ¬ì¡° ë¶„ì„ ì‹¤íŒ¨: {e}")
            return {"success": False, "error": str(e), "upload_id": upload_id}
    
    def _create_ultra_detailed_analysis_prompt(self, basic_info: Dict) -> str:
        """ì´ˆì •ë°€ ë¶„ì„ì„ ìœ„í•œ ë§¤ìš° ìƒì„¸í•œ í”„ë¡¬í”„íŠ¸"""
        
        # ë™ì ìœ¼ë¡œ ê°ì§€ëœ ë¬¸ì œ ìˆ˜ ì‚¬ìš©
        detected_count = basic_info.get('detected_question_count', 0)
        
        pages_hint = ""
        if basic_info.get('pages_info'):
            pages_hint = f"\nğŸ“‹ **PDF ê¸°ë³¸ ì •ë³´** (ì˜ˆìƒ ë¬¸ì œ ìˆ˜: {detected_count}ê°œ):\n"
            for page_info in basic_info['pages_info'][:5]:  # ì²˜ìŒ 5í˜ì´ì§€ë§Œ íŒíŠ¸ë¡œ ì œê³µ
                pages_hint += f"- í˜ì´ì§€ {page_info['page_number']}: {page_info['line_count']}ì¤„, "
                pages_hint += f"í•œê¸€: {'ìˆìŒ' if page_info['has_korean'] else 'ì—†ìŒ'}, "
                pages_hint += f"ì„ íƒì§€: {'ìˆìŒ' if page_info['has_choices'] else 'ì—†ìŒ'}\n"
                pages_hint += f"  ìƒ˜í”Œ: \"{page_info['sample_text']}\"\n"
        
        return f"""ğŸ”¬ ë‹¹ì‹ ì€ í•œêµ­ì–´ ì‹œí—˜ë¬¸ì œ PDF ë¶„ì„ ì´ˆì „ë¬¸ê°€ì…ë‹ˆë‹¤.

ğŸ¯ **í•µì‹¬ ì„ë¬´ - ë¬¸ì œë³„ ìƒì„¸ í…œí”Œë¦¿ ìƒì„±**: 
1. ê° ë¬¸ì œì˜ ì •í™•í•œ êµ¬ì¡° íŒ¨í„´ ë¶„ì„
2. ë¬¸ì œ ì§€ë¬¸ì˜ ì‹œì‘/ë ë‹¨ì–´, ê¸¸ì´, íŠ¹ìˆ˜ ìš”ì†Œ íŒŒì•…
3. ì„ íƒì§€ì˜ í˜•ì‹, ê°œìˆ˜, ë‚´ìš© ìœ í˜• ë¶„ì„
4. ì¶”ì¶œ ì‹œ ì‚¬ìš©í•  ì •í™•í•œ í…œí”Œë¦¿ ì œê³µ

ğŸ“‹ **ë¬¸ì œë³„ ìƒì„¸ ë¶„ì„ ìš”êµ¬ì‚¬í•­**:
- ë¬¸ì œ ë²ˆí˜¸: "1.", "ë¬¸ì œ 1ë²ˆ", "1)" ë“± ì •í™•í•œ í˜•ì‹
- ì§€ë¬¸ ì‹œì‘: "ë‹¤ìŒ ì¤‘", "ì•„ë˜ì˜", "ë‹¤ìŒ ì„¤ëª…" ë“± ì‹œì‘ íŒ¨í„´  
- ì§€ë¬¸ ë: "ê²ƒì€?", "ëŠ”ê°€?", "í•˜ì‹œì˜¤." ë“± ì¢…ë£Œ íŒ¨í„´
- ì„ íƒì§€ í˜•ì‹: "â‘ â‘¡â‘¢â‘£", "1)2)3)4)", "ê°€)ë‚˜)ë‹¤)ë¼)" ë“±
- íŠ¹ìˆ˜ ìš”ì†Œ: í‘œ, ì½”ë“œ, ë‹¤ì´ì–´ê·¸ë¨ì˜ ì •í™•í•œ ìœ„ì¹˜ì™€ ë‚´ìš©

ğŸ” **ë¬¸ì œ êµ¬ì¡° í…œí”Œë¦¿ ì˜ˆì‹œ**:
```
ë¬¸ì œ 6ë²ˆ í…œí”Œë¦¿:
- ë²ˆí˜¸ í˜•ì‹: "6."
- ì§€ë¬¸ ì‹œì‘: "ë‹¤ìŒ í‘œì™€ ê°™ì´"
- ì§€ë¬¸ ë: "ì–¼ë§ˆì¸ê°€?"
- ì§€ë¬¸ ê¸¸ì´: ì•½ 45ì
- í‘œ ìœ„ì¹˜: ì§€ë¬¸ ë°”ë¡œ ì•„ë˜
- í‘œ ë‚´ìš©: | í”„ë¡œì„¸ìŠ¤ | ë„ì°©ì‹œê°„ | ì‹¤í–‰ì‹œê°„ | (3ì—´ 5í–‰)
- ì„ íƒì§€: â‘ â‘¡â‘¢â‘£ (4ê°œ, ìˆ«ì í˜•íƒœ)
```

{pages_hint}

ğŸ” **í˜ì´ì§€ ê²½ê³„ ë¬¸ì œ ì‚¬ì „ ë¶„ì„ ê²°ê³¼** (í…ìŠ¤íŠ¸ ê¸°ë°˜):
{self._format_cross_page_analysis_for_prompt(basic_info.get('cross_page_issues', {}))}

ğŸ“¸ **í•µì‹¬ ì„ë¬´ - ì‹œê°ì  ìš”ì†Œ ì •ë°€ ê°ì§€**:
- ëª¨ë“  ì´ë¯¸ì§€, ë‹¤ì´ì–´ê·¸ë¨, í‘œ, ì½”ë“œë¥¼ ë¬¸ì œë²ˆí˜¸ì™€ ì •í™•íˆ ë§¤ì¹­
- ì„ íƒì§€ ë‚´ë¶€ì˜ ì‘ì€ ì´ë¯¸ì§€ë„ ë¹ ëœ¨ë¦¬ì§€ ë§ê³  ê°ì§€
- íŠ¹íˆ â‘ â‘¡â‘¢â‘£â‘¤ ì„ íƒì§€ ì˜†ì´ë‚˜ ì•ˆì˜ ì‹œê°ì  ìš”ì†Œë¥¼ ì •ë°€ ë¶„ì„

ğŸ–¼ï¸ **ì‹œê°ì  ìš”ì†Œ ì´ˆì •ë°€ ê°ì§€ (ê¸€ìê°€ ì•„ë‹Œ ëª¨ë“  ê·¸ë˜í”½ ìš”ì†Œ)**:

**ğŸ¯ ì„ íƒì§€ ë‚´ ì‹œê°ì  ìš”ì†Œ ê°ì§€**:
- â‘ â‘¡â‘¢â‘£â‘¤ ê° ì„ íƒì§€ë¥¼ ê°œë³„ì ìœ¼ë¡œ ê²€ì‚¬
- ë„í˜•ìœ¼ë¡œ ëœ ì„ íƒì§€: ì›, ì‚¬ê°í˜•, ì‚¼ê°í˜•, ë‹¤ê°í˜•, ê¸°í•˜ë„í˜•
- ê·¸ë˜í”„ë¡œ ëœ ì„ íƒì§€: ë§‰ëŒ€ê·¸ë˜í”„, ì›ê·¸ë˜í”„, ì„ ê·¸ë˜í”„, íˆìŠ¤í† ê·¸ë¨
- ë‹¤ì´ì–´ê·¸ë¨ìœ¼ë¡œ ëœ ì„ íƒì§€: ë„¤íŠ¸ì›Œí¬ë„, í”Œë¡œì°¨íŠ¸, ì‹œìŠ¤í…œêµ¬ì¡°ë„
- ì˜ˆ: "ë¬¸ì œ 15ë²ˆì˜ ì„ íƒì§€ â‘¡â‘¢ì´ ì›ê³¼ ì‚¬ê°í˜• ë„í˜•"

**ğŸ“Š ë¬¸ì œ ì§€ë¬¸ ë‚´ ì‹œê°ì  ìš”ì†Œ ê°ì§€**:
- í‘œ/í…Œì´ë¸”: ë°ì´í„°í‘œ, ë¹„êµí‘œ, ìŠ¤ì¼€ì¤„ë§ í…Œì´ë¸”, í”„ë¡œì„¸ìŠ¤ í…Œì´ë¸”
- ì½”ë“œ ë¸”ë¡: Java, C++, Python, SQL ë“± í”„ë¡œê·¸ë˜ë° ì½”ë“œ
- ìˆ˜í•™ ê³µì‹: ìˆ˜ì‹, ê³„ì‚°ì‹, ì•Œê³ ë¦¬ì¦˜ í‘œí˜„
- ë‹¤ì´ì–´ê·¸ë¨: ERë‹¤ì´ì–´ê·¸ë¨, ë„¤íŠ¸ì›Œí¬ êµ¬ì¡°ë„, ì‹œìŠ¤í…œ ì•„í‚¤í…ì²˜
- ì˜ˆ: "ë¬¸ì œ 25ë²ˆ ì§€ë¬¸ì— ë°ì´í„°ë² ì´ìŠ¤ ERë‹¤ì´ì–´ê·¸ë¨ í¬í•¨"

**ğŸ” ê¸´ ì§€ë¬¸ ë° íŠ¹ìˆ˜ í…ìŠ¤íŠ¸ ê°ì§€**:
- ê¸´ ì§€ë¬¸: 5ì¤„ ì´ìƒì˜ ì„¤ëª…ë¬¸, ì¼€ì´ìŠ¤ ìŠ¤í„°ë””, ì‹œë‚˜ë¦¬ì˜¤
- ì˜ì–´ í…ìŠ¤íŠ¸: ì˜ë¬¸ ì„¤ëª…, ê¸°ìˆ  ìš©ì–´, ì½”ë“œ ì£¼ì„
- ìˆ˜ì‹ í‘œí˜„: ìˆ˜í•™ì  ê³µì‹, ë…¼ë¦¬ì‹, ì•Œê³ ë¦¬ì¦˜
- ì˜ˆ: "ë¬¸ì œ 30ë²ˆì— ë„¤íŠ¸ì›Œí¬ êµ¬ì„± ì‹œë‚˜ë¦¬ì˜¤ ê¸´ ì§€ë¬¸ í¬í•¨"

**ğŸ“ ì •í™•í•œ ìœ„ì¹˜ ë° ë§¥ë½ ì •ë³´**:
- í˜ì´ì§€ ë²ˆí˜¸, ë¬¸ì œ ë²ˆí˜¸, êµ¬ì²´ì  ìœ„ì¹˜ (ìƒë‹¨/ì¤‘ê°„/í•˜ë‹¨)
- ì„ íƒì§€ì¸ ê²½ìš° ì–´ë–¤ ë²ˆí˜¸ ì„ íƒì§€ì¸ì§€ ëª…ì‹œ
- í›„ì† ì´ë¯¸ì§€ ìº¡ì²˜ë¥¼ ìœ„í•œ ìƒì„¸ ì¢Œí‘œ ì •ë³´

ğŸ“Š **ì„ íƒì§€ ê°œìˆ˜ë³„ ìƒì„¸ ë¶„ì„**:
- ê° ë¬¸ì œë³„ë¡œ ì •í™•íˆ ëª‡ ê°œì˜ ì„ íƒì§€ê°€ ìˆëŠ”ì§€ ì¹´ìš´íŠ¸
- 2ê°œ, 3ê°œ, 4ê°œ, 5ê°œ ì„ íƒì§€ë³„ë¡œ ë¬¸ì œ ë²ˆí˜¸ë“¤ì„ ë¶„ë¥˜
- ì„ íƒì§€ê°€ ì´ë¯¸ì§€ì¸ ê²½ìš°ì™€ í…ìŠ¤íŠ¸ì¸ ê²½ìš° êµ¬ë¶„

ğŸ”— **í˜ì´ì§€ ê²½ê³„ ë¬¸ì œ ë©´ë°€ ë¶„ì„**:
- ë¬¸ì œ ë‚´ìš©ì´ ë‘ í˜ì´ì§€ì— ê±¸ì³ ë‚˜ë‰œ ê²½ìš° (ì§€ë¬¸ì´ ê¸´ ê²½ìš°)
- ì„ íƒì§€ ì¼ë¶€ê°€ ë‹¤ìŒ í˜ì´ì§€ë¡œ ë„˜ì–´ê°„ ê²½ìš° (ëª‡ ë²ˆ ì„ íƒì§€ë¶€í„°ì¸ì§€)
- ë¬¸ì œì™€ ì„ íƒì§€ê°€ ì™„ì „íˆ ë¶„ë¦¬ëœ ê²½ìš°
- ì§€ë¬¸ê³¼ ë¬¸ì œê°€ ë‹¤ë¥¸ í˜ì´ì§€ì— ìˆëŠ” ê²½ìš°

âš ï¸ **ë§¤ìš° ì¤‘ìš”**: ì˜ˆìƒ {detected_count}ê°œëŠ” ì°¸ê³ ì¼ ë¿ì…ë‹ˆë‹¤. 
- ì‹¤ì œë¡œ "ë¬¸ì œ 1ë²ˆ"ë¶€í„° "ë¬¸ì œ Në²ˆ"ê¹Œì§€ ìˆœì„œëŒ€ë¡œ ëª¨ë‘ ì°¾ì•„ì£¼ì„¸ìš”
- ë¹ ì§„ ë¬¸ì œ ë²ˆí˜¸ë‚˜ ì¤‘ë³µëœ ë²ˆí˜¸ê°€ ìˆëŠ”ì§€ í™•ì¸í•˜ì„¸ìš”
- í˜ì´ì§€ë¥¼ ë„˜ë‚˜ë“œëŠ” ë¬¸ì œëŠ” íŠ¹ë³„íˆ í‘œì‹œí•˜ì„¸ìš”

ğŸ“‹ **1. ë¬¸ì„œ íƒ€ì… ì •ë°€ ë¶„ì„**:
- document_type: "questions_only" | "theory_only" | "mixed" | "answers_explanations" | "practice_tests" | "summary_notes"
- content_complexity: "simple" | "intermediate" | "complex" | "mixed_levels"
- language_type: "korean_only" | "mixed_korean_english" | "technical_terms_heavy"

ğŸ“Š **2. í˜ì´ì§€ë³„ ìƒì„¸ ë¶„ì„** (ê° í˜ì´ì§€ë§ˆë‹¤):
**ğŸ” í˜ì´ì§€ íƒ€ì… ì •í™•íˆ êµ¬ë¶„í•˜ê¸° (ë§¤ìš° ì¤‘ìš”)**:
- page_type: 
  - "pure_questions": ë¬¸ì œ ë²ˆí˜¸ + ì„ íƒì§€ê°€ ëª…í™•íˆ ìˆëŠ” í˜ì´ì§€
  - "answer_sheet": ì •ë‹µë§Œ ë‚˜ì—´ëœ í˜ì´ì§€ ("1ë²ˆ: â‘¡", "2ë²ˆ: â‘£" í˜•ì‹)
  - "explanation_sheet": í•´ì„¤ë§Œ ìˆëŠ” í˜ì´ì§€ ("ì •ë‹µ í•´ì„¤", "í’€ì´ ê³¼ì •" ë“±)
  - "cover_page": í‘œì§€, ì‹œí—˜ ì•ˆë‚´ë¬¸
  - "table_of_contents": ëª©ì°¨
  - "theory_explanation": ì´ë¡  ì„¤ëª…
  - "mixed_content": ë¬¸ì œì™€ ë‹¤ë¥¸ ë‚´ìš©ì´ í˜¼ì¬
  
**âœ… ë¬¸ì œ í˜ì´ì§€ ì‹ë³„ ê¸°ì¤€**:
- "ë¬¸ì œ 1ë²ˆ", "1.", "1)" ë“±ì˜ ë¬¸ì œ ë²ˆí˜¸ê°€ ìˆìŒ
- â‘ â‘¡â‘¢â‘£â‘¤ ë˜ëŠ” 1)2)3)4)5) ë“±ì˜ ì„ íƒì§€ê°€ ìˆìŒ
- ë¬¸ì œ í…ìŠ¤íŠ¸ì™€ ì„ íƒì§€ê°€ ì™„ì „í•œ ì„¸íŠ¸ë¡œ êµ¬ì„±ë¨

**âŒ ë¹„ë¬¸ì œ í˜ì´ì§€ ì‹ë³„ ê¸°ì¤€**:
- ì •ë‹µë§Œ ë‚˜ì—´: "1. â‘¡", "2. â‘£", "3. â‘ " í˜•ì‹ìœ¼ë¡œë§Œ êµ¬ì„±
- í•´ì„¤ë§Œ ìˆìŒ: "ì •ë‹µ í•´ì„¤", "í’€ì´", "í•´ë‹µ" ë“±ì˜ ì œëª©ë§Œ ìˆìŒ
- í‘œì§€/ì•ˆë‚´: "ì‹œí—˜ ì•ˆë‚´", "ì£¼ì˜ì‚¬í•­", "ë‹µì•ˆì§€ ì‘ì„±ë²•" ë“±
- ë¹ˆ í˜ì´ì§€: ì˜ë¯¸ìˆëŠ” ë‚´ìš©ì´ ê±°ì˜ ì—†ìŒ

- question_density: í˜ì´ì§€ë‹¹ ì‹¤ì œ ë¬¸ì œ ê°œìˆ˜ (ì •ë‹µë§Œ ìˆìœ¼ë©´ 0ìœ¼ë¡œ í‘œì‹œ)
- layout_type: "single_column" | "double_column" | "mixed_layout" | "table_heavy" | "image_heavy"
- special_elements: ["tables", "diagrams", "code_blocks", "mathematical_formulas", "images"] ì¤‘ í•´ë‹¹ ìš”ì†Œë“¤

ğŸ“ **3. ë¬¸ì œ êµ¬ì¡° ì´ˆì •ë°€ ë¶„ì„** (ë¬¸ì œê°€ ìˆëŠ” ê²½ìš°):
- total_questions: ì •í™•í•œ ë¬¸ì œ ê°œìˆ˜ (ì¶”ì •ì´ ì•„ë‹Œ ì‹¤ì œ ì¹´ìš´íŠ¸)
- numbering_pattern: "1." | "ë¬¸ì œ 1ë²ˆ" | "Q1" | "1)" | ê¸°íƒ€ ì‹¤ì œ íŒ¨í„´
- choice_pattern: "â‘ â‘¡â‘¢â‘£â‘¤" | "1)2)3)4)5)" | "ABCDE" | "ê°€ë‚˜ë‹¤ë¼ë§ˆ" | ê¸°íƒ€ ì‹¤ì œ íŒ¨í„´
- average_choices_per_question: ì‹¤ì œ í‰ê·  ì„ íƒì§€ ê°œìˆ˜
- choice_completeness: ê° ë¬¸ì œë³„ ì„ íƒì§€ ì™„ì„±ë„ ë¶„ì„

ğŸ¯ **4. ê¸°ë³¸ ë¬¸ì œ êµ¬ì¡° ë¶„ì„**:
- total_questions: ì •í™•í•œ ë¬¸ì œ ê°œìˆ˜ (ì¶”ì •ì´ ì•„ë‹Œ ì‹¤ì œ ì¹´ìš´íŠ¸)
- numbering_pattern: "1." | "ë¬¸ì œ 1ë²ˆ" | "Q1" | "1)" | ê¸°íƒ€ ì‹¤ì œ íŒ¨í„´  
- choice_pattern: "â‘ â‘¡â‘¢â‘£â‘¤" | "1)2)3)4)5)" | "ABCDE" | "ê°€ë‚˜ë‹¤ë¼ë§ˆ" | ê¸°íƒ€ ì‹¤ì œ íŒ¨í„´
- average_choices_per_question: ì‹¤ì œ í‰ê·  ì„ íƒì§€ ê°œìˆ˜

ğŸ” **5. íŠ¹ìˆ˜ ìš”ì†Œê°€ ìˆëŠ” ë¬¸ì œë§Œ ì„ ë³„ ë¶„ì„** (ìµœëŒ€ 10ê°œë§Œ):
íŠ¹ë³„í•œ ìš”ì†Œ(í‘œ, ì½”ë“œ, ë‹¤ì´ì–´ê·¸ë¨)ê°€ ìˆëŠ” ë¬¸ì œë§Œ ê°„ë‹¨íˆ ë¶„ì„:
- question_number: ë¬¸ì œ ë²ˆí˜¸
- special_element: "table" | "code" | "diagram" | "image" | "passage"
- page_location: í˜ì´ì§€ ë²ˆí˜¸

ğŸ“Š **5. í‘œ/ê·¸ë˜í”„/ì´ë¯¸ì§€ ë¶„ì„**:
- tables_detected: ë°œê²¬ëœ í‘œì˜ ì •í™•í•œ ìœ„ì¹˜ì™€ ë‚´ìš© (ëª‡ í˜ì´ì§€ ëª‡ ë²ˆ ë¬¸ì œ)
- images_detected: ì´ë¯¸ì§€ì˜ ìœ„ì¹˜ì™€ ìš©ë„ (ì„ íƒì§€ìš©/ì„¤ëª…ìš©/ì¥ì‹ìš©) - í…ìŠ¤íŠ¸ê°€ ì•„ë‹Œ ëª¨ë“  ì‹œê°ì  ìš”ì†Œ í¬í•¨
- diagrams_detected: ë‹¤ì´ì–´ê·¸ë¨ì˜ ìœ„ì¹˜ì™€ ìœ í˜•
- code_blocks_detected: ì½”ë“œ ë¸”ë¡ì˜ ìœ„ì¹˜ì™€ í”„ë¡œê·¸ë˜ë° ì–¸ì–´

ğŸ” **6. í˜ì´ì§€ ê²½ê³„ ë¬¸ì œ ì´ˆì •ë°€ ë¶„ì„** (ë§¤ìš° ì¤‘ìš”!):

**ğŸš¨ ì„ íƒì§€ ë¶„í•  ê²€ì¶œ**:
- ê° ë¬¸ì œì˜ ì„ íƒì§€ë¥¼ â‘ â‘¡â‘¢â‘£â‘¤ ìˆœì„œë¡œ í™•ì¸
- ì„ íƒì§€ ì¼ë¶€ê°€ ë‹¤ìŒ í˜ì´ì§€ì— ìˆëŠ” ê²½ìš° ì •í™•í•œ ë¬¸ì œ ë²ˆí˜¸ì™€ ì–´ë–¤ ì„ íƒì§€ë¶€í„° ë„˜ì–´ê°€ëŠ”ì§€ ê¸°ë¡
- ì˜ˆ: "ë¬¸ì œ 17ë²ˆì˜ ì„ íƒì§€ â‘¢â‘£â‘¤ê°€ ë‹¤ìŒ í˜ì´ì§€ì— ìˆìŒ"

**ğŸ”— ë¬¸ì œ ë‚´ìš© ë¶„í•  ê²€ì¶œ**:
- ë¬¸ì œ í…ìŠ¤íŠ¸ë‚˜ ì§€ë¬¸ì´ ë‘ í˜ì´ì§€ì— ê±¸ì³ ë‚˜ë‰œ ê²½ìš°
- ì–´ëŠ ë¶€ë¶„ì—ì„œ í˜ì´ì§€ê°€ ë‚˜ë‰˜ëŠ”ì§€ êµ¬ì²´ì ìœ¼ë¡œ ê¸°ë¡
- ì˜ˆ: "ë¬¸ì œ 25ë²ˆì˜ ì§€ë¬¸ì´ í˜ì´ì§€ 3-4ì— ê±¸ì³ ë¶„í• ë¨"

**ğŸ“Š í‘œ/ì½”ë“œ ë¶„í•  ê²€ì¶œ**:
- í‘œë‚˜ ì½”ë“œ ë¸”ë¡ì´ í˜ì´ì§€ ê²½ê³„ì—ì„œ ì˜ë¦° ê²½ìš°
- í…Œì´ë¸” í—¤ë”ë§Œ ìˆê³  ë°ì´í„°ê°€ ë‹¤ìŒ í˜ì´ì§€ì— ìˆëŠ” ê²½ìš°
- ì½”ë“œì˜ ì¼ë¶€ë§Œ ë³´ì´ê³  ë‚˜ë¨¸ì§€ê°€ ë‹¤ìŒ í˜ì´ì§€ì— ìˆëŠ” ê²½ìš°

**ğŸ‘ï¸ ì‹œê°ì  ë‹¨ì„œ ê°ì§€**:
- "ê³„ì†", "ë‹¤ìŒ í˜ì´ì§€ ì°¸ì¡°", í™”ì‚´í‘œ í‘œì‹œ ë“±
- ë¬¸ì œë‚˜ ì„ íƒì§€ê°€ í˜ì´ì§€ í•˜ë‹¨ì—ì„œ ê°‘ìê¸° ëŠì–´ì§€ëŠ” ê²½ìš°
- í˜ì´ì§€ ìƒë‹¨ì— ì„ íƒì§€ë§Œ ìˆê±°ë‚˜ ë¬¸ì œ ë²ˆí˜¸ ì—†ì´ ë‚´ìš©ë§Œ ìˆëŠ” ê²½ìš°

**ğŸ” ì™„ì„±ë„ ê²€ì¦**:
- ê° ë¬¸ì œê°€ ë¬¸ì œë²ˆí˜¸ + ë¬¸ì œí…ìŠ¤íŠ¸ + ëª¨ë“  ì„ íƒì§€ë¥¼ ê°–ì¶”ì—ˆëŠ”ì§€ í™•ì¸
- ë¶ˆì™„ì „í•œ ë¬¸ì œë“¤ì„ êµ¬ì²´ì ìœ¼ë¡œ ë‚˜ì—´
- ì˜ˆ: "ë¬¸ì œ 17ë²ˆ: ì„ íƒì§€ â‘¢â‘£ ëˆ„ë½", "ë¬¸ì œ 25ë²ˆ: ì§€ë¬¸ í›„ë°˜ë¶€ ëˆ„ë½"

âš™ï¸ **7. ì²˜ë¦¬ ì „ëµ ë„ì¶œ**:
- recommended_chunk_strategy: ì´ PDFì— ìµœì í™”ëœ ì²­í¬ ì „ëµ
- special_processing_needed: íŠ¹ë³„íˆ ì£¼ì˜í•´ì•¼ í•  ì²˜ë¦¬ ìš”ì†Œë“¤
- extraction_order: ê¶Œì¥ ì¶”ì¶œ ìˆœì„œ
- potential_challenges: ì˜ˆìƒë˜ëŠ” ì²˜ë¦¬ ì–´ë ¤ì›€ê³¼ í•´ê²°ì±…

ğŸ¯ **JSON ì¶œë ¥ í˜•ì‹** (ë°˜ë“œì‹œ ì´ êµ¬ì¡°ë¡œ):
```json
{{
  "analysis_summary": {{
    "document_type": "ì‹¤ì œ_ë¶„ì„_ê²°ê³¼",
    "total_pages": ì‹¤ì œ_í˜ì´ì§€_ìˆ˜,
    "total_questions": ì •í™•í•œ_ë¬¸ì œ_ê°œìˆ˜,
    "confidence_score": 0.0-1.0
  }},
  "page_analysis": [
    {{
      "page_number": 1,
      "page_type": "ì‹¤ì œ_í˜ì´ì§€_íƒ€ì…",
      "questions_on_page": [ì‹¤ì œ_ë¬¸ì œ_ë²ˆí˜¸ë“¤],
      "question_density": ì‹¤ì œ_ë¬¸ì œ_ê°œìˆ˜,
      "special_elements": ["ì‹¤ì œ_ë°œê²¬ëœ_ìš”ì†Œë“¤"],
      "layout_complexity": "simple|medium|complex",
      "processing_notes": "ì´ í˜ì´ì§€ íŠ¹ë³„ ì²˜ë¦¬ ì‚¬í•­"
    }}
  ],
  "question_analysis": {{
    "total_questions": ì‹¤ì œ_ë¬¸ì œ_ê°œìˆ˜,
    "numbering_pattern": "ì‹¤ì œ_íŒ¨í„´",
    "choice_pattern": "ì‹¤ì œ_ì„ íƒì§€_íŒ¨í„´",
    "average_choices_per_question": ì‹¤ì œ_í‰ê· _ì„ íƒì§€_ê°œìˆ˜,
    "detailed_question_templates": [
      {{
        "question_number": ë¬¸ì œë²ˆí˜¸,
        "number_format": "6.", 
        "text_start_pattern": "ë‹¤ìŒ í‘œì™€ ê°™ì´",
        "text_end_pattern": "ì–¼ë§ˆì¸ê°€?",
        "text_length": ì•½_45ì,
        "choice_format": "â‘ â‘¡â‘¢â‘£",
        "choice_count": 4,
        "special_elements": {{
          "has_table": true,
          "table_position": "ì§€ë¬¸_ë°”ë¡œ_ì•„ë˜",
          "table_structure": "| í”„ë¡œì„¸ìŠ¤ | ë„ì°©ì‹œê°„ | ì‹¤í–‰ì‹œê°„ | (3ì—´_5í–‰)",
          "has_code": false,
          "has_diagram": false
        }},
        "extraction_template": {{
          "question_start_marker": "6. ë‹¤ìŒ í‘œì™€ ê°™ì´",
          "question_end_marker": "ì–¼ë§ˆì¸ê°€?",
          "choice_start_marker": "â‘ ",
          "table_extraction_needed": true,
          "table_format": "markdown"
        }}
      }}
    ],
    "special_questions": [
      {{
        "question_number": ë¬¸ì œë²ˆí˜¸,
        "special_element": "table|code|diagram|image|passage",
        "page_location": í˜ì´ì§€_ë²ˆí˜¸,
        "brief_description": "ê°„ë‹¨í•œ ì„¤ëª…"
      }}
    ]
  }},
  "special_elements": {{
    "tables": [
      {{
        "location": "í˜ì´ì§€X_ë¬¸ì œY",
        "table_type": "ë°ì´í„°í‘œ|ë¹„êµí‘œ|ê³„ì‚°í‘œ",
        "complexity": "simple|complex",
        "data_completeness": "header_only|partial_data|complete_data"
      }}
    ],
    "visual_elements_detailed": {{
      "diagram_in_choices": [
        {{
          "question": 25,
          "page": 3,
          "location": "ì„ íƒì§€ â‘ â‘¡â‘¢â‘£ ëª¨ë‘ ë‹¤ì´ì–´ê·¸ë¨",
          "diagram_type": "ë„¤íŠ¸ì›Œí¬|í”Œë¡œì°¨íŠ¸|ERë‹¤ì´ì–´ê·¸ë¨|ì‹œìŠ¤í…œêµ¬ì¡°"
        }}
      ],
      "graph_in_choices": [
        {{
          "question": 35,
          "page": 5, 
          "location": "ì„ íƒì§€ ì „ì²´ê°€ ê·¸ë˜í”„",
          "graph_type": "ë§‰ëŒ€|ì›|ì„ |ì‚°ì ë„"
        }}
      ],
      "shape_in_choices": [
        {{
          "question": 42,
          "page": 6,
          "location": "ì„ íƒì§€ â‘ â‘¡ê°€ ê¸°í•˜ë„í˜•",
          "shape_type": "ì›|ì‚¬ê°í˜•|ì‚¼ê°í˜•|ë‹¤ê°í˜•"
        }}
      ],
      "diagram_in_passage": [
        {{
          "question": 5,
          "page": 1,
          "location": "ë¬¸ì œ ì§€ë¬¸ ìƒë‹¨",
          "diagram_type": "ì‹œìŠ¤í…œêµ¬ì¡°|ë°ì´í„°íë¦„|ë„¤íŠ¸ì›Œí¬"
        }}
      ],
      "graph_in_passage": [
        {{
          "question": 12,
          "page": 2,
          "location": "ë¬¸ì œ ì§€ë¬¸ í•˜ë‹¨", 
          "graph_type": "ì„±ëŠ¥ê·¸ë˜í”„|í†µê³„ì°¨íŠ¸|ë¹„êµí‘œ"
        }}
      ]
    }},
    "choice_analysis_detailed": {{
      "questions_with_2_choices": [3, 7, 15],
      "questions_with_3_choices": [8, 12],
      "questions_with_4_choices": [1, 2, 4, 5, 6, 9, 10, 11],
      "questions_with_5_choices": [21, 22, 23]
    }},
    "page_boundary_issues_detailed": {{
      "content_split_questions": [
        {{
          "question": 12,
          "split_type": "ë¬¸ì œ ë‚´ìš©ì´ í˜ì´ì§€ 1-2ì— ê±¸ë¦¼",
          "split_location": "ì§€ë¬¸ ì¤‘ê°„ë¶€ë¶„"
        }}
      ],
      "choices_split_questions": [
        {{
          "question": 7,
          "split_location": "ì„ íƒì§€ â‘¢â‘£ê°€ ë‹¤ìŒ í˜ì´ì§€",
          "pages": "1-2"
        }}
      ],
      "separated_elements": [
        {{
          "question": 15,
          "separation_type": "ì§€ë¬¸ê³¼ ë¬¸ì œê°€ ë¶„ë¦¬",
          "pages": "2-3"
        }}
      ]
    }}
  }},
  "processing_strategy": {{
    "recommended_approach": "í˜ì´ì§€ë³„|ë¬¸ì œìœ í˜•ë³„|ìš”ì†Œë³„",
    "chunk_size_recommendation": "ì ì •_ì²­í¬_í¬ê¸°",
    "special_handling": ["íŠ¹ë³„_ì²˜ë¦¬_í•„ìš”_ì‚¬í•­ë“¤"],
    "expected_challenges": ["ì˜ˆìƒ_ì–´ë ¤ì›€ë“¤"],
    "processing_order": ["ê¶Œì¥_ì²˜ë¦¬_ìˆœì„œ"]
  }}
}}
```

**âš ï¸ ì ˆëŒ€ ì¤‘ìš”**: 
- ì¶”ì •ì´ë‚˜ ê°€ì • ê¸ˆì§€! ì‹¤ì œ ë³´ì´ëŠ” ë‚´ìš©ë§Œ ë¶„ì„
- ëª¨ë“  ìˆ«ìëŠ” ì •í™•í•œ ì¹´ìš´íŠ¸ ê¸°ë°˜
- ê° ë¬¸ì œë³„ ìƒì„¸ ë¶„ì„ í•„ìˆ˜
- ì²˜ë¦¬ ì „ëµì€ ì´ íŠ¹ì • PDFì— ìµœì í™”ë˜ì–´ì•¼ í•¨

ğŸ¯ **êµ¬ì¡° ë¶„ì„ í•„ìˆ˜ì‚¬í•­**:
- ì •í™•í•œ ë¬¸ì œ ê°œìˆ˜ê°€ ê°€ì¥ ì¤‘ìš”í•©ë‹ˆë‹¤
- í˜ì´ì§€ë³„ ë¬¸ì œ ë¶„í¬ë¥¼ ì •í™•íˆ íŒŒì•…í•˜ì„¸ìš”
- íŠ¹ìˆ˜ ìš”ì†Œê°€ ìˆëŠ” ë¬¸ì œë§Œ ì„ ë³„ì ìœ¼ë¡œ ë¶„ì„í•˜ì„¸ìš”
- ë³µì¡í•œ ê°œë³„ ë¶„ì„ë³´ë‹¤ëŠ” ì „ì²´ êµ¬ì¡° íŒŒì•…ì— ì§‘ì¤‘í•˜ì„¸ìš”

ğŸš¨ **í˜ì´ì§€ êµ¬ë¶„ ìµœìš°ì„  ì§€ì¹¨**:
- ê° í˜ì´ì§€ë¥¼ ì •í™•íˆ ë¶„ë¥˜í•˜ëŠ” ê²ƒì´ ê°€ì¥ ì¤‘ìš”í•©ë‹ˆë‹¤
- ë¬¸ì œê°€ ì—†ëŠ” í˜ì´ì§€ë¥¼ ë¬¸ì œ í˜ì´ì§€ë¡œ ì˜ëª» ë¶„ë¥˜í•˜ì§€ ë§ˆì„¸ìš”
- ì •ë‹µë§Œ ë‚˜ì—´ëœ í˜ì´ì§€ëŠ” ë°˜ë“œì‹œ "answer_sheet"ë¡œ ë¶„ë¥˜í•˜ì„¸ìš”
- í•´ì„¤ë§Œ ìˆëŠ” í˜ì´ì§€ëŠ” ë°˜ë“œì‹œ "explanation_sheet"ë¡œ ë¶„ë¥˜í•˜ì„¸ìš”
- question_densityëŠ” ì‹¤ì œ ë¬¸ì œê°€ ìˆì„ ë•Œë§Œ 1 ì´ìƒìœ¼ë¡œ ì„¤ì •í•˜ì„¸ìš”

ğŸ“¸ **ê°„ë‹¨í•œ ì´ë¯¸ì§€ í™•ì¸**:
- í˜ì´ì§€ì— ì´ë¯¸ì§€ê°€ ìˆìœ¼ë©´ special_elementsì— "images" í¬í•¨
- ìƒì„¸í•œ ì´ë¯¸ì§€ ë¶„ì„ì€ í•„ìš” ì—†ìŒ
- ì „ì²´ êµ¬ì¡° ë¶„ì„ í’ˆì§ˆì„ ìš°ì„ ìœ¼ë¡œ í•˜ì„¸ìš”

ì‹¤ì œ PDF ë‚´ìš©ì„ ê·¹ë„ë¡œ ì •ë°€í•˜ê²Œ ë¶„ì„í•˜ì—¬ ì™„ë²½í•œ êµ¬ì¡° ì •ë³´ë¥¼ ì œê³µí•˜ì„¸ìš”."""
    
    async def _perform_special_elements_analysis(self, pdf_path: str, basic_structure: Dict, upload_id: int) -> Dict[str, Any]:
        """2ë‹¨ê³„: íŠ¹ìˆ˜ ìš”ì†Œ ì§‘ì¤‘ ë¶„ì„ (ì´ë¯¸ì§€/í‘œ/ì½”ë“œ/ì„ íƒì§€ ë“±)"""
        
        print(f"ğŸ¯ 2ë‹¨ê³„: íŠ¹ìˆ˜ ìš”ì†Œ ì§‘ì¤‘ ë¶„ì„ ì‹œì‘ - Upload {upload_id}")
        
        # ê¸°ë³¸ êµ¬ì¡°ì—ì„œ íŠ¹ìˆ˜ ìš”ì†Œê°€ ìˆëŠ” í˜ì´ì§€ ì°¾ê¸°
        special_pages = []
        page_analysis = basic_structure.get('page_analysis', [])
        
        for page in page_analysis:
            special_elements = page.get('special_elements', [])
            if any(element in special_elements for element in ['images', 'tables', 'diagrams', 'code_blocks']):
                special_pages.append(page.get('page_number'))
        
        if not special_pages:
            print("ğŸ¯ íŠ¹ìˆ˜ ìš”ì†Œê°€ ìˆëŠ” í˜ì´ì§€ ì—†ìŒ, íŠ¹ìˆ˜ ë¶„ì„ ìŠ¤í‚µ")
            return {"special_analysis": [], "success": True}
        
        print(f"ğŸ¯ íŠ¹ìˆ˜ ìš”ì†Œ ë¶„ì„ ëŒ€ìƒ í˜ì´ì§€: {special_pages}")
        
        try:
            # í•´ë‹¹ í˜ì´ì§€ë“¤ë§Œ ê³ í•´ìƒë„ ì´ë¯¸ì§€ ìƒì„±
            special_pages_data = await self._create_selected_pages_images(pdf_path, special_pages)
            
            # íŠ¹ìˆ˜ ìš”ì†Œ ì „ìš© í”„ë¡¬í”„íŠ¸ë¡œ ë¶„ì„
            special_analysis_prompt = self._create_special_elements_prompt(basic_structure, special_pages)
            
            messages = [
                {
                    "role": "system", 
                    "content": "ë‹¹ì‹ ì€ PDF ë‚´ íŠ¹ìˆ˜ ìš”ì†Œ ì „ë¬¸ ë¶„ì„ê°€ì…ë‹ˆë‹¤. í‘œ/ì½”ë“œ/ì´ë¯¸ì§€/ì„ íƒì§€ë¥¼ ì •í™•íˆ êµ¬ë¶„í•˜ê³  ë¬¸ì œ ë²ˆí˜¸ë¥¼ ê¸°ë¡í•˜ì„¸ìš”."
                },
                {
                    "role": "user",
                    "content": [{"type": "text", "text": special_analysis_prompt}]
                }
            ]
            
            # ì„ íƒëœ í˜ì´ì§€ ì´ë¯¸ì§€ë“¤ì„ ë©”ì‹œì§€ì— ì¶”ê°€
            for img_data in special_pages_data:
                messages[1]["content"].append({
                    "type": "image_url",
                    "image_url": {
                        "url": f"data:image/png;base64,{img_data['image_data']}",
                        "detail": "high"
                    }
                })
            
            # OpenAI API í˜¸ì¶œ
            response = await self.openai_client.chat.completions.create(
                model="gpt-4o",
                messages=messages,
                max_tokens=4000,  # ë” ë§ì€ íŠ¹ìˆ˜ ìš”ì†Œ ë¶„ì„ì„ ìœ„í•´ ì¦ê°€
                temperature=0.05
            )
            
            response_text = response.choices[0].message.content
            print(f"ğŸ¯ íŠ¹ìˆ˜ ìš”ì†Œ ë¶„ì„ ì‘ë‹µ ê¸¸ì´: {len(response_text)}ì")
            
            # JSON íŒŒì‹±
            special_analysis = self._parse_special_elements_response(response_text)
            
            print(f"âœ… íŠ¹ìˆ˜ ìš”ì†Œ ì§‘ì¤‘ ë¶„ì„ ì™„ë£Œ")
            return {"special_analysis": special_analysis, "success": True}
            
        except Exception as e:
            print(f"âŒ íŠ¹ìˆ˜ ìš”ì†Œ ì§‘ì¤‘ ë¶„ì„ ì‹¤íŒ¨: {e}")
            return {"special_analysis": [], "success": False, "error": str(e)}
    
    async def _create_selected_pages_images(self, pdf_path: str, page_numbers: list) -> List[Dict]:
        """ì„ íƒëœ í˜ì´ì§€ë“¤ë§Œ ê³ í•´ìƒë„ ì´ë¯¸ì§€ ìƒì„±"""
        
        try:
            doc = fitz.open(pdf_path)
            selected_images = []
            
            for page_num in page_numbers:
                if page_num <= len(doc):
                    page = doc.load_page(page_num - 1)  # 0-based index
                    
                    # ê³ í•´ìƒë„ ë Œë”ë§ (API ì œí•œ ê³ ë ¤)
                    mat = fitz.Matrix(12.0, 12.0)  # 12ë°° ê³ í•´ìƒë„
                    pix = page.get_pixmap(matrix=mat, alpha=False)
                    
                    # Base64 ì¸ì½”ë”©
                    img_data = self._optimize_image_size(pix)
                    base64_img = base64.b64encode(img_data).decode('utf-8')
                    
                    selected_images.append({
                        'page_number': page_num,
                        'width': pix.width,
                        'height': pix.height,
                        'image_data': base64_img,
                        'format': 'png'
                    })
                    
                    print(f"   ğŸ“„ ì´ë¯¸ì§€ í˜ì´ì§€ {page_num} ìƒì„±: {pix.width}x{pix.height}")
            
            doc.close()
            return selected_images
            
        except Exception as e:
            print(f"âŒ ì„ íƒëœ í˜ì´ì§€ ì´ë¯¸ì§€ ìƒì„± ì‹¤íŒ¨: {e}")
            return []
    
    def _create_special_elements_prompt(self, basic_structure: Dict, special_pages: list) -> str:
        """íŠ¹ìˆ˜ ìš”ì†Œ ì§‘ì¤‘ ë¶„ì„ ì „ìš© í”„ë¡¬í”„íŠ¸"""
        
        total_questions = basic_structure.get('analysis_summary', {}).get('total_questions', 0)
        
        return f"""ğŸ¯ íŠ¹ìˆ˜ ìš”ì†Œ ì „ë¬¸ ë¶„ì„ê°€

ğŸ“„ **ê¸°ë³¸ ì •ë³´**:
- ì´ ë¬¸ì œ ìˆ˜: {total_questions}ê°œ (ì‹¤ì œ PDFì—ì„œ ê°ì§€ëœ ìˆ˜)
- íŠ¹ìˆ˜ ìš”ì†Œ ë¶„ì„ ëŒ€ìƒ í˜ì´ì§€: {special_pages}

ğŸ¯ **ë¶„ì„ ëª©í‘œ** - ë¬¸ì œ ë²ˆí˜¸ë³„ë¡œ ì •í™•íˆ ë¶„ë¥˜:
1. **ì´ë¯¸ì§€ ì„ íƒì§€ ë¬¸ì œ**: ì„ íƒì§€ê°€ ê·¸ë¦¼/ë„í‘œë¡œ ëœ ë¬¸ì œ
2. **í‘œ í¬í•¨ ë¬¸ì œ**: ë¬¸ì œë‚˜ ì§€ë¬¸ì— í‘œ/í…Œì´ë¸”ì´ ìˆëŠ” ë¬¸ì œ
3. **ì½”ë“œ í¬í•¨ ë¬¸ì œ**: í”„ë¡œê·¸ë˜ë° ì½”ë“œê°€ ìˆëŠ” ë¬¸ì œ
4. **ë‹¤ì´ì–´ê·¸ë¨ ë¬¸ì œ**: ë‹¤ì´ì–´ê·¸ë¨/ê·¸ë˜í”„ê°€ ìˆëŠ” ë¬¸ì œ
5. **ì´ë¯¸ì§€ ì§€ë¬¸ ë¬¸ì œ**: ì§€ë¬¸ì— ê·¸ë¦¼ì´ í¬í•¨ëœ ë¬¸ì œ

**JSON ì¶œë ¥ í˜•ì‹**:
```json
{{
  "special_questions": [
    {{
      "question_number": ë¬¸ì œë²ˆí˜¸,
      "page_number": í˜ì´ì§€ë²ˆí˜¸,
      "special_type": "image_choices|table_content|code_block|diagram|image_passage",
      "element_description": "êµ¬ì²´ì ì¸ ì„¤ëª… (ì˜ˆ: P1,P2,P3 í”„ë¡œì„¸ìŠ¤ í‘œ)",
      "choice_has_images": true/false,
      "question_has_table": true/false,
      "question_has_code": true/false,
      "question_has_diagram": true/false,
      "passage_has_images": true/false
    }}
  ],
  "summary": {{
    "total_special_questions": íŠ¹ìˆ˜ìš”ì†Œ_ë¬¸ì œ_ì´ìˆ˜,
    "image_choice_questions": [ì´ë¯¸ì§€ì„ íƒì§€_ë¬¸ì œë²ˆí˜¸ë“¤],
    "table_questions": [í‘œí¬í•¨_ë¬¸ì œë²ˆí˜¸ë“¤],
    "code_questions": [ì½”ë“œí¬í•¨_ë¬¸ì œë²ˆí˜¸ë“¤],
    "diagram_questions": [ë‹¤ì´ì–´ê·¸ë¨_ë¬¸ì œë²ˆí˜¸ë“¤],
    "image_passage_questions": [ì´ë¯¸ì§€ì§€ë¬¸_ë¬¸ì œë²ˆí˜¸ë“¤]
  }}
}}
```

ğŸ¯ **ë¶„ì„ ì¤‘ì ì‚¬í•­**:
- ë¬¸ì œ ë²ˆí˜¸ë¥¼ ì •í™•íˆ ì‹ë³„í•˜ì„¸ìš”
- ì„ íƒì§€ê°€ ì´ë¯¸ì§€ì¸ ê²ƒê³¼ ì§€ë¬¸ì— ì´ë¯¸ì§€ê°€ ìˆëŠ” ê²ƒì„ êµ¬ë¶„í•˜ì„¸ìš”
- í‘œì˜ ê²½ìš° P1, P2, P3 ê°™ì€ ë°ì´í„°ê°€ ìˆëŠ”ì§€ í™•ì¸í•˜ì„¸ìš”
- í‘œ ë°ì´í„°ëŠ” ë°˜ë“œì‹œ ë§ˆí¬ë‹¤ìš´ í˜•ì‹ìœ¼ë¡œ ê¸°ë¡í•˜ì„¸ìš”: | í—¤ë” | ë°ì´í„° |
- ì½”ë“œ ë¸”ë¡ì˜ ê²½ìš° í”„ë¡œê·¸ë˜ë° ì–¸ì–´ë„ ê¸°ë¡í•˜ì„¸ìš”
- íŠ¹ìˆ˜ ìš”ì†Œê°€ ìˆëŠ” ë¬¸ì œë§Œ ë¶„ì„í•˜ì„¸ìš”"""
    
    def _parse_special_elements_response(self, response_text: str) -> Dict:
        """íŠ¹ìˆ˜ ìš”ì†Œ ë¶„ì„ ì‘ë‹µ íŒŒì‹±"""
        
        try:
            # JSON ë¸”ë¡ ì°¾ê¸°
            json_match = re.search(r'```json\s*(.*?)\s*```', response_text, re.DOTALL)
            if json_match:
                json_text = json_match.group(1)
            else:
                # ì²« ë²ˆì§¸ { ë¶€í„° ë§ˆì§€ë§‰ } ê¹Œì§€ ì¶”ì¶œ
                start = response_text.find('{')
                end = response_text.rfind('}')
                if start != -1 and end != -1:
                    json_text = response_text[start:end+1]
                else:
                    return {"special_questions": [], "summary": {}}
            
            # JSON íŒŒì‹±
            data = json.loads(json_text)
            
            # ê²°ê³¼ êµ¬ì¡°í™”
            special_questions = data.get('special_questions', [])
            summary = data.get('summary', {})
            
            # ë¬¸ì œ ë²ˆí˜¸ë³„ë¡œ ë¶„ë¥˜í•´ì„œ ë¡œê·¸ ì¶œë ¥
            if special_questions:
                print(f"   ğŸ¯ íŠ¹ìˆ˜ ìš”ì†Œ ë¬¸ì œ ë°œê²¬:")
                
                image_choices = summary.get('image_choice_questions', [])
                table_questions = summary.get('table_questions', [])
                code_questions = summary.get('code_questions', [])
                diagram_questions = summary.get('diagram_questions', [])
                image_passages = summary.get('image_passage_questions', [])
                
                if image_choices:
                    print(f"   ğŸ–¼ï¸ ì´ë¯¸ì§€ ì„ íƒì§€: {image_choices}")
                if table_questions:
                    print(f"   ğŸ“Š í‘œ í¬í•¨: {table_questions}")
                if code_questions:
                    print(f"   ğŸ’» ì½”ë“œ í¬í•¨: {code_questions}")
                if diagram_questions:
                    print(f"   ğŸ“ˆ ë‹¤ì´ì–´ê·¸ë¨: {diagram_questions}")
                if image_passages:
                    print(f"   ğŸ–¼ï¸ ì´ë¯¸ì§€ ì§€ë¬¸: {image_passages}")
            
            return {
                "special_questions": special_questions,
                "summary": summary
            }
            
        except Exception as e:
            print(f"âš ï¸ íŠ¹ìˆ˜ ìš”ì†Œ ë¶„ì„ ì‘ë‹µ íŒŒì‹± ì‹¤íŒ¨: {e}")
            return {"special_questions": [], "summary": {}}
    
    async def _integrate_structure_and_special_analysis(self, basic_structure: Dict, special_analysis: Dict, upload_id: int) -> Dict[str, Any]:
        """ê¸°ë³¸ êµ¬ì¡°ì™€ íŠ¹ìˆ˜ ìš”ì†Œ ë¶„ì„ ê²°ê³¼ í†µí•©"""
        
        print(f"ğŸ”§ êµ¬ì¡° ë¶„ì„ ê²°ê³¼ í†µí•© ì¤‘ - Upload {upload_id}")
        
        try:
            # ê¸°ë³¸ êµ¬ì¡°ë¥¼ ë² ì´ìŠ¤ë¡œ ë³µì‚¬
            integrated = basic_structure.copy()
            
            # ğŸ†• ê¸°ë³¸ ì •ë³´ì— ìˆëŠ” cross_page_issuesë¥¼ ìµœìƒìœ„ë¡œ ë³µì‚¬
            basic_info_cross_page = basic_structure.get('basic_info', {}).get('cross_page_issues')
            if basic_info_cross_page:
                integrated['cross_page_issues'] = basic_info_cross_page
            
            # íŠ¹ìˆ˜ ìš”ì†Œ ë¶„ì„ ê²°ê³¼ ê°€ì ¸ì˜¤ê¸°
            special_questions = special_analysis.get('special_analysis', {}).get('special_questions', [])
            summary = special_analysis.get('special_analysis', {}).get('summary', {})
            
            if not special_questions:
                print("ğŸ¯ íŠ¹ìˆ˜ ìš”ì†Œ ë¶„ì„ ê²°ê³¼ ì—†ìŒ, ê¸°ë³¸ êµ¬ì¡°ë§Œ ì‚¬ìš©")
                return integrated
            
            # ê¸°ì¡´ question_analysisì— íŠ¹ìˆ˜ ìš”ì†Œ ì •ë³´ ì¶”ê°€
            question_analysis = integrated.get('question_analysis', {})
            
            # detailed_questions ìƒì„± (íŠ¹ìˆ˜ ìš”ì†Œê°€ ìˆëŠ” ë¬¸ì œë“¤ë§Œ)
            detailed_questions = []
            
            for special_q in special_questions:
                question_detail = {
                    "question_number": special_q.get('question_number'),
                    "question_type": special_q.get('special_type', 'text_only'),
                    "choices_count": 4,  # ê¸°ë³¸ê°’
                    "has_passage": special_q.get('passage_has_images', False),
                    "has_table": special_q.get('question_has_table', False),
                    "has_images": special_q.get('choice_has_images', False) or special_q.get('passage_has_images', False),
                    "question_has_images": special_q.get('passage_has_images', False),
                    "choice_has_images": special_q.get('choice_has_images', False),
                    "has_code": special_q.get('question_has_code', False),
                    "has_diagram": special_q.get('question_has_diagram', False),
                    "special_elements": special_q.get('element_description', ''),
                    "page_location": special_q.get('page_number'),
                    "processing_complexity": "high"  # íŠ¹ìˆ˜ ìš”ì†Œê°€ ìˆìœ¼ë©´ ë³µì¡ë„ ë†’ìŒ
                }
                detailed_questions.append(question_detail)
            
            # question_analysis ì—…ë°ì´íŠ¸
            question_analysis['detailed_questions'] = detailed_questions
            question_analysis['special_questions_count'] = len(detailed_questions)
            
            # íŠ¹ìˆ˜ ìš”ì†Œë³„ ë¶„ë¥˜ ì •ë³´ ì¶”ê°€
            question_analysis['special_elements_summary'] = {
                'image_choice_questions': summary.get('image_choice_questions', []),
                'table_questions': summary.get('table_questions', []),
                'code_questions': summary.get('code_questions', []),
                'diagram_questions': summary.get('diagram_questions', []),
                'image_passage_questions': summary.get('image_passage_questions', [])
            }
            
            integrated['question_analysis'] = question_analysis
            
            # í˜ì´ì§€ ê²½ê³„ ë¬¸ì œ ë¶„ì„ ë° ê¸°ë¡
            page_boundary_issues = self._analyze_page_boundary_issues(integrated, special_analysis)
            if page_boundary_issues:
                integrated['page_boundary_analysis'] = page_boundary_issues
            
            # íŠ¹ìˆ˜ ìš”ì†Œ ë¶„ì„ ê²°ê³¼ ìƒì„¸ ì¶œë ¥
            self._display_special_elements_summary(summary, detailed_questions, page_boundary_issues)
            
            return integrated
            
        except Exception as e:
            print(f"âŒ êµ¬ì¡° í†µí•© ì‹¤íŒ¨: {e}")
            return basic_structure
    
    def _analyze_page_boundary_issues(self, structure: Dict, special_analysis: Dict) -> Dict:
        """í˜ì´ì§€ ê²½ê³„ì—ì„œ ë°œìƒí•˜ëŠ” ë¬¸ì œë“¤ ë¶„ì„"""
        
        try:
            print(f"ğŸ” í˜ì´ì§€ ê²½ê³„ ë¬¸ì œ ë¶„ì„ ì‹œì‘...")
            
            page_analysis = structure.get('page_analysis', [])
            issues = {
                'cross_page_questions': [],
                'incomplete_questions': [],  
                'split_elements': [],
                'missing_content': [],
                'total_issues_found': 0
            }
            
            # ê° í˜ì´ì§€ì˜ ë¬¸ì œ ë¶„í¬ í™•ì¸
            for i, page_info in enumerate(page_analysis):
                page_num = page_info.get('page_number', i + 1)
                questions_on_page = page_info.get('questions_on_page', [])
                
                if not questions_on_page:
                    continue
                    
                # í˜ì´ì§€ ë ë¬¸ì œ í™•ì¸ (ë‹¤ìŒ í˜ì´ì§€ë¡œ ë„˜ì–´ê°ˆ ê°€ëŠ¥ì„±)
                if questions_on_page:
                    last_question = max(questions_on_page)
                    next_page_info = page_analysis[i + 1] if i + 1 < len(page_analysis) else None
                    
                    if next_page_info:
                        next_questions = next_page_info.get('questions_on_page', [])
                        if next_questions:
                            first_next_question = min(next_questions)
                            
                            # ë¬¸ì œ ë²ˆí˜¸ ì—°ì†ì„± í™•ì¸ (ê±´ë„ˆë›°ëŠ” ë¬¸ì œê°€ ìˆëŠ”ì§€)
                            if first_next_question - last_question > 1:
                                missing_questions = list(range(last_question + 1, first_next_question))
                                for missing_q in missing_questions:
                                    issues['cross_page_questions'].append({
                                        'question_number': missing_q,
                                        'start_page': page_num,
                                        'end_page': page_num + 1,
                                        'issue_type': 'missing_between_pages',
                                        'description': f'ë¬¸ì œ {missing_q}ë²ˆì´ í˜ì´ì§€ {page_num}-{page_num+1} ê²½ê³„ì—ì„œ ëˆ„ë½ë¨'
                                    })
                
                # í˜ì´ì§€ë‹¹ ë¬¸ì œ ìˆ˜ê°€ ë¹„ì •ìƒì ìœ¼ë¡œ ì ì€ ê²½ìš° (ì„ íƒì§€ ëˆ„ë½ ê°€ëŠ¥ì„±)
                expected_density = page_info.get('question_density', 0)
                if expected_density > 15:  # í•œ í˜ì´ì§€ì— 15ê°œ ì´ìƒì€ ë¹„ì •ìƒì ìœ¼ë¡œ ë§ìŒ
                    issues['incomplete_questions'].append({
                        'page_number': page_num,
                        'expected_questions': expected_density,
                        'issue_type': 'high_density_suspicious',
                        'description': f'í˜ì´ì§€ {page_num}ì— {expected_density}ê°œ ë¬¸ì œ - ì„ íƒì§€ ëˆ„ë½ ê°€ëŠ¥ì„±'
                    })
            
            # íŠ¹ìˆ˜ ìš”ì†Œ ë¶„ì„ì—ì„œ í˜ì´ì§€ ê²½ê³„ ë¬¸ì œ ì¶”ì¶œ
            special_data = special_analysis.get('special_analysis', {})
            if 'cross_page_issues' in special_data:
                cross_page_analysis = special_data['cross_page_issues']
                if cross_page_analysis and len(cross_page_analysis.strip()) > 10:
                    issues['split_elements'].append({
                        'analysis_type': 'gpt_detected',
                        'description': cross_page_analysis.strip(),
                        'severity': 'high'
                    })
            
            issues['total_issues_found'] = (
                len(issues['cross_page_questions']) + 
                len(issues['incomplete_questions']) + 
                len(issues['split_elements']) + 
                len(issues['missing_content'])
            )
            
            if issues['total_issues_found'] > 0:
                print(f"âš ï¸ í˜ì´ì§€ ê²½ê³„ ë¬¸ì œ ë°œê²¬: {issues['total_issues_found']}ê±´")
                for issue in issues['cross_page_questions']:
                    print(f"   ğŸ”— {issue['description']}")
                for issue in issues['incomplete_questions']:
                    print(f"   ğŸ“„ {issue['description']}")
            else:
                print(f"âœ… í˜ì´ì§€ ê²½ê³„ ë¬¸ì œ ì—†ìŒ")
            
            return issues if issues['total_issues_found'] > 0 else {}
            
        except Exception as e:
            print(f"âŒ í˜ì´ì§€ ê²½ê³„ ë¶„ì„ ì‹¤íŒ¨: {e}")
            return {}
    
    def _display_special_elements_summary(self, summary: Dict, detailed_questions: List[Dict], page_boundary_issues: Dict = None):
        """íŠ¹ìˆ˜ ìš”ì†Œ ë¶„ì„ ê²°ê³¼ ìƒì„¸ ì¶œë ¥"""
        
        print(f"\nğŸ”§ íŠ¹ìˆ˜ ìš”ì†Œ ë¶„ì„ í†µí•© ì™„ë£Œ: {len(detailed_questions)}ê°œ ë¬¸ì œ")
        print("=" * 60)
        
        # ê° ì¹´í…Œê³ ë¦¬ë³„ ìƒì„¸ ì •ë³´ ì¶œë ¥
        categories = [
            ("ğŸ–¼ï¸ ì´ë¯¸ì§€ ì„ íƒì§€ê°€ ìˆëŠ” ë¬¸ì œ", summary.get('image_choice_questions', []), "image_choice"),
            ("ğŸ“Š í‘œê°€ í¬í•¨ëœ ë¬¸ì œ", summary.get('table_questions', []), "table"),
            ("ğŸ’» ì½”ë“œê°€ í¬í•¨ëœ ë¬¸ì œ", summary.get('code_questions', []), "code"), 
            ("ğŸ“ˆ ë‹¤ì´ì–´ê·¸ë¨ì´ ìˆëŠ” ë¬¸ì œ", summary.get('diagram_questions', []), "diagram"),
            ("ğŸ–¼ï¸ ì§€ë¬¸ì— ì´ë¯¸ì§€ê°€ ìˆëŠ” ë¬¸ì œ", summary.get('image_passage_questions', []), "image_passage")
        ]
        
        for category_name, question_numbers, category_type in categories:
            if question_numbers:
                print(f"\n{category_name}: {len(question_numbers)}ê°œ")
                print(f"   ë¬¸ì œ ë²ˆí˜¸: {', '.join(map(str, sorted(question_numbers)))}")
                
                # í•´ë‹¹ ì¹´í…Œê³ ë¦¬ ë¬¸ì œë“¤ì˜ ìƒì„¸ ì •ë³´
                category_details = [q for q in detailed_questions 
                                  if q['question_number'] in question_numbers]
                
                for detail in category_details:
                    elements = []
                    if detail.get('has_table'): elements.append("í‘œ")
                    if detail.get('choice_has_images'): elements.append("ì´ë¯¸ì§€ì„ íƒì§€")  
                    if detail.get('question_has_images'): elements.append("ì´ë¯¸ì§€ì§€ë¬¸")
                    if detail.get('has_code'): elements.append("ì½”ë“œ")
                    if detail.get('has_diagram'): elements.append("ë‹¤ì´ì–´ê·¸ë¨")
                    
                    if elements:
                        print(f"     ë¬¸ì œ {detail['question_number']}: {', '.join(elements)}")
            else:
                print(f"\n{category_name}: 0ê°œ")
        
        print("\n" + "=" * 60)
        print(f"ğŸ“‹ ì´ íŠ¹ìˆ˜ ìš”ì†Œ ë¬¸ì œ: {len(detailed_questions)}ê°œ")
        
        # ì „ì²´ í†µê³„
        total_stats = {
            "ì´ë¯¸ì§€ ì„ íƒì§€": len(summary.get('image_choice_questions', [])),
            "í‘œ í¬í•¨": len(summary.get('table_questions', [])),
            "ì½”ë“œ í¬í•¨": len(summary.get('code_questions', [])),
            "ë‹¤ì´ì–´ê·¸ë¨": len(summary.get('diagram_questions', [])),
            "ì´ë¯¸ì§€ ì§€ë¬¸": len(summary.get('image_passage_questions', []))
        }
        
        print("ğŸ“Š ì¹´í…Œê³ ë¦¬ë³„ í†µê³„:")
        for stat_name, count in total_stats.items():
            if count > 0:
                print(f"   {stat_name}: {count}ê°œ")
        
        # í˜ì´ì§€ ê²½ê³„ ë¬¸ì œ ì¶œë ¥
        if page_boundary_issues and page_boundary_issues.get('total_issues_found', 0) > 0:
            print("\nâš ï¸ í˜ì´ì§€ ê²½ê³„ ë¬¸ì œ ë¶„ì„ ê²°ê³¼:")
            print("=" * 60)
            
            cross_page = page_boundary_issues.get('cross_page_questions', [])
            if cross_page:
                print(f"ğŸ”— í˜ì´ì§€ ê±¸ì¹¨ ë¬¸ì œ: {len(cross_page)}ê°œ")
                for issue in cross_page[:5]:  # ìµœëŒ€ 5ê°œë§Œ í‘œì‹œ
                    print(f"   ë¬¸ì œ {issue['question_number']}: {issue['description']}")
            
            incomplete = page_boundary_issues.get('incomplete_questions', [])
            if incomplete:
                print(f"ğŸ“„ ì„ íƒì§€ ëˆ„ë½ ì˜ì‹¬: {len(incomplete)}ê°œ")
                for issue in incomplete[:3]:
                    print(f"   í˜ì´ì§€ {issue['page_number']}: {issue['description']}")
            
            split_elements = page_boundary_issues.get('split_elements', [])
            if split_elements:
                print(f"âœ‚ï¸ ë¶„í• ëœ ìš”ì†Œ: {len(split_elements)}ê°œ")
                for issue in split_elements:
                    print(f"   {issue.get('description', '')[:100]}...")
            
            print("=" * 60)
        
        print("=" * 60)

    def _parse_ultra_detailed_response(self, response_text: str, upload_id: int) -> Dict[str, Any]:
        """ì´ˆì •ë°€ ë¶„ì„ ê²°ê³¼ íŒŒì‹±"""
        
        try:
            # JSON ë¸”ë¡ ì¶”ì¶œ
            json_match = re.search(r'```json\s*(\{.*?\})\s*```', response_text, re.DOTALL)
            if json_match:
                structure_data = json.loads(json_match.group(1))
            else:
                # ì§ì ‘ JSON íŒŒì‹± ì‹œë„
                structure_data = json.loads(response_text)
            
            # ë©”íƒ€ë°ì´í„° ì¶”ê°€
            structure_data['success'] = True
            structure_data['upload_id'] = upload_id
            structure_data['analysis_method'] = 'ultra_precise_gpt_vision'
            structure_data['analysis_timestamp'] = 'now'  # ì‹¤ì œ êµ¬í˜„ì‹œ timestamp ì¶”ê°€
            
            # ë¶„ì„ í’ˆì§ˆ ê²€ì¦
            quality_score = self._calculate_analysis_quality(structure_data)
            structure_data['analysis_quality_score'] = quality_score
            
            return structure_data
            
        except json.JSONDecodeError as e:
            print(f"âš ï¸ JSON íŒŒì‹± ì‹¤íŒ¨, í…ìŠ¤íŠ¸ ë¶„ì„ìœ¼ë¡œ ëŒ€ì²´: {e}")
            return self._fallback_text_parsing(response_text, upload_id)
        except Exception as e:
            print(f"âŒ ë¶„ì„ ê²°ê³¼ íŒŒì‹± ì‹¤íŒ¨: {e}")
            return {"success": False, "error": str(e), "upload_id": upload_id}
    
    def _optimize_image_size(self, pix):
        """ì´ë¯¸ì§€ í¬ê¸° ìµœì í™” (API ì œí•œ ê³ ë ¤)"""
        
        # ì´ˆê¸° PNG ë°ì´í„° ìƒì„±
        img_data = pix.tobytes("png")
        
        # ì´ë¯¸ì§€ í¬ê¸° í™•ì¸ (MB ë‹¨ìœ„)
        size_mb = len(img_data) / (1024 * 1024)
        
        # 10MB ì´ˆê³¼ ì‹œ JPEGë¡œ ë³€í™˜ ë° ì••ì¶•
        if size_mb > 10:
            try:
                # JPEG ë³€í™˜ (í’ˆì§ˆ 85%)
                img_data = pix.tobytes("jpeg", jpg_quality=85)
                new_size_mb = len(img_data) / (1024 * 1024)
                print(f"   ğŸ“ ì´ë¯¸ì§€ ì••ì¶•: {size_mb:.1f}MB â†’ {new_size_mb:.1f}MB (JPEG 85%)")
                
                # ì•„ì§ë„ í¬ë©´ ë” ë‚®ì€ í’ˆì§ˆë¡œ
                if new_size_mb > 15:
                    img_data = pix.tobytes("jpeg", jpg_quality=70)
                    final_size_mb = len(img_data) / (1024 * 1024)
                    print(f"   ğŸ“ ì¶”ê°€ ì••ì¶•: {new_size_mb:.1f}MB â†’ {final_size_mb:.1f}MB (JPEG 70%)")
                    
            except Exception as e:
                print(f"   âš ï¸ ì´ë¯¸ì§€ ì••ì¶• ì‹¤íŒ¨, PNG ì‚¬ìš©: {e}")
        
        return img_data
    
    def _calculate_analysis_quality(self, structure_data: Dict) -> float:
        """ë¶„ì„ í’ˆì§ˆ ì ìˆ˜ ê³„ì‚°"""
        
        quality_score = 0.0
        max_score = 100.0
        
        try:
            # ê¸°ë³¸ ì •ë³´ ì™„ì„±ë„ (20ì )
            if structure_data.get('analysis_summary'):
                quality_score += 20
            
            # í˜ì´ì§€ë³„ ë¶„ì„ ì™„ì„±ë„ (30ì )
            page_analysis = structure_data.get('page_analysis', [])
            if page_analysis:
                page_quality = min(len(page_analysis) * 5, 30)
                quality_score += page_quality
            
            # ë¬¸ì œë³„ ìƒì„¸ ë¶„ì„ (25ì )
            question_analysis = structure_data.get('question_analysis', {})
            if question_analysis.get('detailed_questions'):
                question_quality = min(len(question_analysis['detailed_questions']) * 2, 25)
                quality_score += question_quality
            
            # íŠ¹ë³„ ìš”ì†Œ ë¶„ì„ (15ì )
            special_elements = structure_data.get('special_elements', {})
            if special_elements:
                special_score = min(len(special_elements) * 5, 15)
                quality_score += special_score
            
            # ì²˜ë¦¬ ì „ëµ ì œì‹œ (10ì )
            if structure_data.get('processing_strategy'):
                quality_score += 10
            
        except Exception as e:
            print(f"âš ï¸ í’ˆì§ˆ ì ìˆ˜ ê³„ì‚° ì‹¤íŒ¨: {e}")
            
        return round(min(quality_score / max_score, 1.0), 2)
    
    def _fallback_text_parsing(self, text: str, upload_id: int) -> Dict[str, Any]:
        """JSON íŒŒì‹± ì‹¤íŒ¨ì‹œ í…ìŠ¤íŠ¸ ë¶„ì„ ëŒ€ì²´"""
        
        structure = {
            "success": True,
            "upload_id": upload_id,
            "analysis_method": "fallback_text_parsing",
            "analysis_summary": {
                "document_type": "questions_only",
                "total_pages": 8,
                "total_questions": 40,
                "confidence_score": 0.5
            }
        }
        
        # í…ìŠ¤íŠ¸ì—ì„œ í•µì‹¬ ì •ë³´ ì¶”ì¶œ ì‹œë„
        try:
            # ë¬¸ì œ ê°œìˆ˜ ì¶”ì¶œ
            question_count_match = re.search(r'total_questions["\s:]*(\d+)', text, re.IGNORECASE)
            if question_count_match:
                structure["analysis_summary"]["total_questions"] = int(question_count_match.group(1))
            
            # í˜ì´ì§€ ìˆ˜ ì¶”ì¶œ
            pages_match = re.search(r'total_pages["\s:]*(\d+)', text, re.IGNORECASE)
            if pages_match:
                structure["analysis_summary"]["total_pages"] = int(pages_match.group(1))
                
        except Exception as e:
            print(f"âš ï¸ í…ìŠ¤íŠ¸ íŒŒì‹± ì¤‘ ì˜¤ë¥˜: {e}")
        
        return structure
    
    async def _validate_and_enhance_structure(self, structure: Dict, pdf_path: str, upload_id: int) -> Dict[str, Any]:
        """ë¶„ì„ ê²°ê³¼ ê²€ì¦ ë° ë³´ì™„"""
        
        try:
            # 1. ì‹¤ì œ PDFì™€ ë¶„ì„ ê²°ê³¼ ëŒ€ì¡°
            doc = fitz.open(pdf_path)
            actual_pages = len(doc)
            doc.close()
            
            # í˜ì´ì§€ ìˆ˜ ê²€ì¦
            analyzed_pages = structure.get('analysis_summary', {}).get('total_pages', 0)
            if analyzed_pages != actual_pages:
                structure['validation_warnings'] = structure.get('validation_warnings', [])
                structure['validation_warnings'].append(f"í˜ì´ì§€ ìˆ˜ ë¶ˆì¼ì¹˜: ì‹¤ì œ {actual_pages}í˜ì´ì§€ vs ë¶„ì„ {analyzed_pages}í˜ì´ì§€")
                structure['analysis_summary']['total_pages'] = actual_pages
            
            # 2. í˜ì´ì§€ë³„ ë¬¸ì œ ìˆ˜ vs ì´ ë¬¸ì œ ìˆ˜ ê²€ì¦ ë° ìˆ˜ì •
            self._validate_and_fix_question_counts(structure)
            
            # 3. ë¶„ì„ ê²°ê³¼ í’ˆì§ˆ ê²€ì¦
            quality_checks = self._perform_quality_checks(structure)
            structure['quality_checks'] = quality_checks
            
            # 4. ì²˜ë¦¬ ì „ëµ ë³´ì™„
            enhanced_strategy = self._enhance_processing_strategy(structure)
            structure['processing_strategy'] = enhanced_strategy
            
            structure['validation_completed'] = True
            return structure
            
        except Exception as e:
            print(f"âš ï¸ êµ¬ì¡° ê²€ì¦ ì¤‘ ì˜¤ë¥˜: {e}")
            structure['validation_error'] = str(e)
            return structure
    
    async def extract_questions_based_on_structure(self, pdf_path: str, structure_analysis: Dict, upload_id: int) -> Dict[str, Any]:
        """êµ¬ì¡° ë¶„ì„ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ í˜ì´ì§€ë³„ ë¶„í•  ë¬¸ì œ ì¶”ì¶œ (êµ¬ì¡° ë¶„ì„ ì„±ê³µ ë°©ì‹ ì°¨ìš©)"""
        
        print(f"ğŸ¯ í˜ì´ì§€ë³„ ë¶„í•  ë¬¸ì œ ì¶”ì¶œ ì‹œì‘ - Upload {upload_id}")
        print("=" * 60)
        
        try:
            # 1. êµ¬ì¡° ë¶„ì„ì—ì„œ ë¬¸ì œê°€ ìˆëŠ” í˜ì´ì§€ë“¤ë§Œ ì„ ë³„ (ê°œì„ ëœ í•„í„°ë§)
            question_pages = []
            page_analysis = structure_analysis.get('page_analysis', [])
            global_overview = structure_analysis.get('global_overview', {})
            page_roles = global_overview.get('page_roles', [])
            
            # page_analysisì—ì„œ ë¬¸ì œ í˜ì´ì§€ ìˆ˜ì§‘
            for page_info in page_analysis:
                if page_info.get('page_type') == 'pure_questions' and page_info.get('question_density', 0) > 0:
                    question_pages.append(page_info)
            
            # global_overviewì—ì„œ ëˆ„ë½ëœ ë¬¸ì œ í˜ì´ì§€ ì¶”ê°€ í™•ì¸
            existing_page_numbers = {p.get('page_number') for p in question_pages}
            
            for role_info in page_roles:
                if (role_info.get('role') == 'questions' and 
                    role_info.get('question_count', 0) > 0 and
                    role_info.get('page_number') not in existing_page_numbers):
                    
                    # page_analysisì— ëˆ„ë½ëœ ë¬¸ì œ í˜ì´ì§€ë¥¼ ì¶”ê°€
                    question_range = role_info.get('question_range', '1-1')
                    # í•œêµ­ì–´ "ë²ˆ" ì ‘ë¯¸ì‚¬ ì œê±°
                    question_range_clean = question_range.replace('ë²ˆ', '').strip()
                    
                    questions_on_page = []
                    if question_range_clean and '-' in question_range_clean:
                        try:
                            start, end = question_range_clean.split('-')
                            questions_on_page = list(range(int(start.strip()), int(end.strip()) + 1))
                        except (ValueError, IndexError) as e:
                            print(f"âš ï¸ ë¬¸ì œ ë²”ìœ„ íŒŒì‹± ì‹¤íŒ¨: {question_range} â†’ {e}")
                    
                    missing_page = {
                        'page_number': role_info['page_number'],
                        'page_type': 'pure_questions',
                        'questions_on_page': questions_on_page,
                        'question_density': role_info.get('question_count', 0),
                        'special_elements': ['unknown'],  # ì„ì‹œê°’
                        'layout_complexity': 'medium',
                        'processing_notes': f"global_overviewì—ì„œ ë³µêµ¬ëœ í˜ì´ì§€ (ë¬¸ì œ {role_info.get('question_range', '')})"
                    }
                    question_pages.append(missing_page)
                    print(f"ğŸ”§ ëˆ„ë½ëœ í˜ì´ì§€ {role_info['page_number']} ë³µêµ¬: {role_info.get('question_range', '')}")
            
            print(f"ğŸ“„ ë¬¸ì œ ì¶”ì¶œ ëŒ€ìƒ: {len(question_pages)}ê°œ í˜ì´ì§€")
            
            # 2. í˜ì´ì§€ë³„ ë¬¸ì œ ì¶”ì¶œ (êµ¬ì¡° ë¶„ì„ ë°©ì‹ ì°¨ìš©)
            all_extracted_questions = []
            total_questions = structure_analysis.get('analysis_summary', {}).get('total_questions', 60)
            
            for i, page_info in enumerate(question_pages):
                print(f"\nğŸ“„ í˜ì´ì§€ {page_info['page_number']} ë¬¸ì œ ì¶”ì¶œ ì¤‘...")
                
                # í˜ì´ì§€ë³„ ê³ í•´ìƒë„ ì´ë¯¸ì§€ ìƒì„± (êµ¬ì¡° ë¶„ì„ê³¼ ë™ì¼í•œ ë°©ì‹)
                page_images = await self._create_single_page_images(pdf_path, page_info, upload_id)
                
                # íŠ¹ìˆ˜ ìš”ì†Œ í¬í•¨ í”„ë¡¬í”„íŠ¸ (êµ¬ì¡° ë¶„ì„ ì •ë³´ í™œìš©)
                page_extraction_prompt = self._create_simple_page_extraction_prompt(page_info, structure_analysis)
                
                # GPT-4Vë¡œ í˜ì´ì§€ë³„ ë¬¸ì œ ì¶”ì¶œ
                page_questions = await self._extract_questions_from_single_page(
                    page_images, page_extraction_prompt, page_info
                )
                
                if page_questions:
                    all_extracted_questions.extend(page_questions)
                    print(f"   âœ… í˜ì´ì§€ {page_info['page_number']}: {len(page_questions)}ê°œ ë¬¸ì œ ì¶”ì¶œ")
                else:
                    print(f"   âš ï¸ í˜ì´ì§€ {page_info['page_number']}: ë¬¸ì œ ì¶”ì¶œ ì‹¤íŒ¨")
            
            print(f"\nğŸ“Š ì „ì²´ ì¶”ì¶œ ê²°ê³¼: {len(all_extracted_questions)}ê°œ ë¬¸ì œ")
            
            # 3. ëˆ„ë½ëœ ë¬¸ì œ ì¬ì¶”ì¶œ (êµ¬ì¡° ë¶„ì„ ë°©ì‹ ì°¸ê³ )
            missing_questions = await self._extract_missing_questions(
                pdf_path, all_extracted_questions, structure_analysis, upload_id
            )
            
            if missing_questions:
                all_extracted_questions.extend(missing_questions)
                print(f"ğŸ”„ ì¬ì¶”ì¶œ ì™„ë£Œ: {len(missing_questions)}ê°œ ì¶”ê°€ ë¬¸ì œ")
            
            # 4. íŠ¹ìˆ˜ ìš”ì†Œ ê²€ì¦ ë° ë³´ì™„
            print(f"\nğŸ” íŠ¹ìˆ˜ ìš”ì†Œ ê²€ì¦ ì¤‘...")
            validated_questions = await self._validate_special_elements(all_extracted_questions, structure_analysis, pdf_path, upload_id)
            
            # 5. ìµœì¢… ê²€ì¦ ë° ì •ë¦¬
            final_questions = self._clean_and_sort_questions(validated_questions)
            verification_result = self._verify_extraction_against_structure(final_questions, structure_analysis)
            
            print(f"âœ… í˜ì´ì§€ë³„ ë¶„í•  ì¶”ì¶œ ì™„ë£Œ: {len(final_questions)}ê°œ ë¬¸ì œ (ì •í™•ë„: {verification_result.get('accuracy_rate', 0):.1f}%)")
            
            return {
                "success": True,
                "questions": final_questions,
                "extraction_method": "page_by_page_extraction",
                "structure_verification": verification_result,
                "total_extracted": len(final_questions),
                "expected_count": total_questions,
                "pages_processed": len(question_pages),
                "upload_id": upload_id
            }
            
        except Exception as e:
            print(f"âŒ í˜ì´ì§€ë³„ ë¶„í•  ì¶”ì¶œ ì‹¤íŒ¨: {e}")
            return {"success": False, "error": str(e)}
    
    async def _create_structure_guided_images(self, pdf_path: str, question_pages: List[Dict], upload_id: int) -> List[Dict]:
        """êµ¬ì¡° ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ë§ì¶¤í˜• ì´ë¯¸ì§€ ìƒì„±"""
        
        images = []
        doc = fitz.open(pdf_path)
        
        for page_info in question_pages:
            page_num = page_info['page_number'] - 1  # 0-based index
            page = doc.load_page(page_num)
            
            # API ì œí•œì„ ê³ ë ¤í•œ ì ì‘ì  í•´ìƒë„ ì„¤ì •
            has_special = page_info.get('special_elements', [])
            scale = 14.0 if has_special else 12.0  # íŠ¹ìˆ˜ ìš”ì†Œ ìˆìœ¼ë©´ 14ë°°, ì¼ë°˜ì€ 12ë°°
            
            mat = fitz.Matrix(scale, scale)
            pix = page.get_pixmap(matrix=mat)
            img_data = self._optimize_image_size(pix)
            
            # Base64 ì¸ì½”ë”©
            import base64
            img_b64 = base64.b64encode(img_data).decode('utf-8')
            
            images.append({
                'page_number': page_info['page_number'],
                'image_data': img_b64,
                'scale': scale,
                'question_range': page_info.get('questions_on_page', []),
                'special_elements': has_special
            })
            
            print(f"   ğŸ“„ êµ¬ì¡° ê¸°ë°˜ {scale}ë°° í•´ìƒë„ í˜ì´ì§€ {page_info['page_number']} ìƒì„±: {pix.width}x{pix.height}")
        
        doc.close()
        return images
    
    def _create_structure_guided_extraction_prompt(self, structure_analysis: Dict, total_questions: int) -> str:
        """êµ¬ì¡° ë¶„ì„ ê¸°ë°˜ ë§ì¶¤í˜• ì¶”ì¶œ í”„ë¡¬í”„íŠ¸"""
        
        page_analysis = structure_analysis.get('page_analysis', [])
        special_questions = structure_analysis.get('question_analysis', {}).get('detailed_questions', [])
        
        prompt = f"""ğŸ¯ êµ¬ì¡° ê¸°ë°˜ ì •ë°€ ë¬¸ì œ ì¶”ì¶œ
ğŸ“Š **ë¶„ì„ëœ êµ¬ì¡° ì •ë³´**:
- ì´ ë¬¸ì œ ìˆ˜: {total_questions}ê°œ
- ë¬¸ì œ í˜ì´ì§€: {len([p for p in page_analysis if p.get('page_type') == 'pure_questions'])}ê°œ í˜ì´ì§€

ğŸ“„ **í˜ì´ì§€ë³„ ë¬¸ì œ êµ¬ì„±**:"""
        
        for page_info in page_analysis:
            if page_info.get('page_type') == 'pure_questions':
                questions_on_page = page_info.get('questions_on_page', [])
                special_elements = page_info.get('special_elements', [])
                
                prompt += f"""
í˜ì´ì§€ {page_info['page_number']}: ë¬¸ì œ {min(questions_on_page) if questions_on_page else 0}-{max(questions_on_page) if questions_on_page else 0}ë²ˆ ({len(questions_on_page)}ê°œ)"""
                
                if special_elements:
                    prompt += f" - íŠ¹ìˆ˜ìš”ì†Œ: {', '.join(special_elements)}"
        
        if special_questions:
            prompt += f"""

ğŸ” **íŠ¹ìˆ˜ ìš”ì†Œ ë¬¸ì œ ì£¼ì˜ì‚¬í•­**:"""
            for special_q in special_questions:
                prompt += f"""
- ë¬¸ì œ {special_q['question_number']}ë²ˆ: {special_q['special_elements']} (í˜ì´ì§€ {special_q['page_location']})"""
        
        prompt += f"""

ğŸ“ **ì¶”ì¶œ ìš”êµ¬ì‚¬í•­**:
1. ìœ„ êµ¬ì¡° ì •ë³´ì— ì •í™•íˆ ì¼ì¹˜í•˜ëŠ” ë¬¸ì œë“¤ë§Œ ì¶”ì¶œ
2. ê° ë¬¸ì œëŠ” ë²ˆí˜¸, ì§€ë¬¸, ì„ íƒì§€ë¥¼ ì™„ì „íˆ í¬í•¨
3. íŠ¹ìˆ˜ ìš”ì†Œê°€ ìˆëŠ” ë¬¸ì œëŠ” í‘œ/ì½”ë“œ/ë‹¤ì´ì–´ê·¸ë¨ë„ ì •í™•íˆ ì¶”ì¶œ
4. ë¶ˆì™„ì „í•œ ë¬¸ì œëŠ” ì ˆëŒ€ í¬í•¨í•˜ì§€ ë§ˆì„¸ìš”

ğŸ“Š **JSON ì¶œë ¥ í˜•ì‹**:
```json
{{
  "questions": [
    {{
      "question_number": 1,
      "question_text": "ë¬¸ì œ ì§€ë¬¸",
      "options": ["â‘  ì„ íƒì§€1", "â‘¡ ì„ íƒì§€2", "â‘¢ ì„ íƒì§€3", "â‘£ ì„ íƒì§€4"],
      "passage": "ì§€ë¬¸ì´ ìˆë‹¤ë©´",
      "page_number": 1,
      "has_table": false,
      "has_code": false,
      "has_diagram": false,
      "table_data": null,
      "code_content": null
    }}
  ]
}}
```

âš¡ **ë§¤ìš° ì¤‘ìš”**: êµ¬ì¡° ë¶„ì„ì—ì„œ í™•ì¸ëœ {total_questions}ê°œ ë¬¸ì œë¥¼ ì •í™•íˆ ì¶”ì¶œí•˜ì„¸ìš”!"""
        
        return prompt
    
    def _parse_extraction_response(self, response_text: str) -> List[Dict]:
        """ë¬¸ì œ ì¶”ì¶œ ì‘ë‹µ íŒŒì‹± (ì„ íƒì§€ ë³µêµ¬ ë¡œì§ í¬í•¨)"""
        
        try:
            # JSON ì¶”ì¶œ ì‹œë„
            json_match = re.search(r'```json\s*(\{.*?\})\s*```', response_text, re.DOTALL)
            if json_match:
                json_str = json_match.group(1)
            else:
                # JSON ë¸”ë¡ì´ ì—†ìœ¼ë©´ ì „ì²´ì—ì„œ ì°¾ê¸°
                json_str = response_text
            
            parsed = json.loads(json_str)
            questions = parsed.get('questions', [])
            
            # ì„ íƒì§€ ë³µêµ¬ ë¡œì§ ì ìš©
            fixed_questions = []
            for question in questions:
                fixed_question = self._fix_embedded_options(question)
                fixed_questions.append(fixed_question)
            
            print(f"ğŸ“ íŒŒì‹±ëœ ë¬¸ì œ ìˆ˜: {len(fixed_questions)}ê°œ")
            return fixed_questions
            
        except Exception as e:
            print(f"âš ï¸ ì‘ë‹µ íŒŒì‹± ì‹¤íŒ¨: {e}")
            return []
    
    def _parse_special_element_response(self, response_text: str) -> List[Dict]:
        """íŠ¹ìˆ˜ ìš”ì†Œ ì¶”ì¶œ ì‘ë‹µ ì „ìš© íŒŒì‹±"""
        
        try:
            print(f"ğŸ” íŠ¹ìˆ˜ ìš”ì†Œ ì‘ë‹µ ê¸¸ì´: {len(response_text)}ì")
            print(f"ğŸ” ì‘ë‹µ ë¯¸ë¦¬ë³´ê¸°: {response_text[:200]}...")
            
            # JSON ì¶”ì¶œ ì‹œë„
            json_match = re.search(r'```json\s*(\{.*?\})\s*```', response_text, re.DOTALL)
            if json_match:
                json_str = json_match.group(1)
                print(f"ğŸ“„ JSON ë¸”ë¡ ì¶”ì¶œ ì„±ê³µ: {len(json_str)}ì")
            else:
                # JSON ë¸”ë¡ì´ ì—†ìœ¼ë©´ ì¤‘ê´„í˜¸ë¡œ ê°ì‹¸ì§„ ë¶€ë¶„ ì°¾ê¸°
                brace_match = re.search(r'(\{[^{}]*(?:\{[^{}]*\}[^{}]*)*\})', response_text, re.DOTALL)
                if brace_match:
                    json_str = brace_match.group(1)
                    print(f"ğŸ“„ ì¤‘ê´„í˜¸ ë¸”ë¡ ì¶”ì¶œ ì„±ê³µ: {len(json_str)}ì")
                else:
                    print(f"âŒ JSON êµ¬ì¡°ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ")
                    return []
            
            # JSON íŒŒì‹±
            parsed = json.loads(json_str)
            print(f"âœ… JSON íŒŒì‹± ì„±ê³µ")
            
            # ë‹¨ì¼ ê°ì²´ë¥¼ ë¦¬ìŠ¤íŠ¸ë¡œ ë˜í•‘
            if isinstance(parsed, dict):
                result = [parsed]
                print(f"ğŸ“ íŠ¹ìˆ˜ ìš”ì†Œ íŒŒì‹± ì™„ë£Œ: 1ê°œ ê°ì²´")
                
                # íŒŒì‹±ëœ ë°ì´í„° ê²€ì¦
                if 'question_number' in parsed:
                    print(f"âœ… ë¬¸ì œ ë²ˆí˜¸: {parsed['question_number']}")
                if 'has_table' in parsed:
                    print(f"ğŸ“Š í‘œ í¬í•¨: {parsed.get('has_table', False)}")
                if 'has_code' in parsed:
                    print(f"ğŸ’» ì½”ë“œ í¬í•¨: {parsed.get('has_code', False)}")
                if 'has_diagram' in parsed:
                    print(f"ğŸ“ˆ ë‹¤ì´ì–´ê·¸ë¨ í¬í•¨: {parsed.get('has_diagram', False)}")
                    
                return result
            else:
                print(f"âš ï¸ ì˜ˆìƒê³¼ ë‹¤ë¥¸ JSON êµ¬ì¡°: {type(parsed)}")
                return []
            
        except json.JSONDecodeError as e:
            print(f"âŒ JSON íŒŒì‹± ì˜¤ë¥˜: {e}")
            print(f"ğŸ“„ íŒŒì‹± ì‹œë„í•œ JSON: {json_str[:500] if 'json_str' in locals() else 'N/A'}")
            return []
        except Exception as e:
            print(f"âš ï¸ íŠ¹ìˆ˜ ìš”ì†Œ íŒŒì‹± ì‹¤íŒ¨: {e}")
            return []
    
    def _fix_embedded_options(self, question: Dict) -> Dict:
        """question_textì— í¬í•¨ëœ ì„ íƒì§€ ë° ë³´ê¸°ë¥¼ ì •í™•íˆ ë¶„ë¦¬ (ì¤‘ë³µ ë°©ì§€ ê°•í™”)"""
        
        text = question.get('question_text', '')
        if not text:
            return question
        
        print(f"ğŸ”§ ë¬¸ì œ {question.get('question_number', '?')}ë²ˆ ì¤‘ë³µ ë¶„ë¦¬ ì²˜ë¦¬ ì‹œì‘")
        
        # 1. ì„ íƒì§€ íŒ¨í„´ ì‚¬ì „ ê°ì§€ (ë¬¸ì œ í…ìŠ¤íŠ¸ì— ì„ íƒì§€ê°€ ì„ì—¬ìˆëŠ”ì§€ í™•ì¸)
        choice_markers_in_text = re.findall(r'[â‘ â‘¡â‘¢â‘£â‘¤]|[1-5]\)', text)
        existing_options = question.get('options', [])
        
        print(f"   - ë¬¸ì œ í…ìŠ¤íŠ¸ ë‚´ ì„ íƒì§€ ë§ˆì»¤: {len(choice_markers_in_text)}ê°œ")
        print(f"   - ê¸°ì¡´ ì„ íƒì§€: {len(existing_options)}ê°œ")
        
        # 2. ë³´ê¸°/ì§€ë¬¸ ë¶„ë¦¬ (passage í•„ë“œê°€ ì—†ê±°ë‚˜ ë¹„ì–´ìˆì„ ë•Œ)
        if not question.get('passage') or question.get('passage').strip() == '':
            question = self._extract_passage_from_text(question, text)
            text = question['question_text']  # ì—…ë°ì´íŠ¸ëœ í…ìŠ¤íŠ¸ ì‚¬ìš©
        
        # 3. ì„ íƒì§€ ë¶„ë¦¬ ë° ì¤‘ë³µ ë°©ì§€ (ì„ íƒì§€ê°€ ë¶€ì¡±í•˜ê±°ë‚˜ í…ìŠ¤íŠ¸ì— ì„ì—¬ìˆì„ ë•Œ)
        if len(existing_options) < 2 or len(choice_markers_in_text) > len(existing_options):
            question = self._extract_options_from_text(question, text)
        
        # 4. ê°•í™”ëœ ì¤‘ë³µ ë‚´ìš© ì œê±°
        question = self._clean_duplicate_content(question)
        
        # 5. ìµœì¢… ê²€ì¦ ë° í’ˆì§ˆ í™•ì¸
        question = self._verify_content_separation_quality(question)
        
        return question
    
    def _extract_passage_from_text(self, question: Dict, text: str) -> Dict:
        """í…ìŠ¤íŠ¸ì—ì„œ ë³´ê¸°/ì§€ë¬¸ ì¶”ì¶œ (í–¥ìƒëœ íŒ¨í„´ ë§¤ì¹­)"""
        
        # í™•ì¥ëœ ë³´ê¸° íŒ¨í„´ ê²€ìƒ‰
        passage_patterns = [
            # ëª…ì‹œì  ë³´ê¸° ë§ˆí¬
            r'<ë³´ê¸°>\s*(.*?)(?=\n\s*[â‘ â‘¡â‘¢â‘£â‘¤]|\n\s*[1-5]\)|ë‹¤ìŒ|ë¬¸ì œ|$)',
            r'\[ë³´ê¸°\]\s*(.*?)(?=\n\s*[â‘ â‘¡â‘¢â‘£â‘¤]|\n\s*[1-5]\)|ë‹¤ìŒ|ë¬¸ì œ|$)',
            r'ë³´ê¸°\s*:?\s*(.*?)(?=\n\s*[â‘ â‘¡â‘¢â‘£â‘¤]|\n\s*[1-5]\)|ë‹¤ìŒ|ë¬¸ì œ|$)',
            
            # í‘œ íŒ¨í„´ (ê°œì„ )
            r'\|[^|\n]*\|[^\n]*\n\|[^|\n]*\|[^\n]*(?:\n\|[^|\n]*\|[^\n]*)*',
            
            # ì½”ë“œ íŒ¨í„´ (í™•ì¥)
            r'(?:public|private|class|function|def|int|void|for|while|if|import|include)\s+[^\n]*(?:\n\s*[^\nâ‘ â‘¡â‘¢â‘£â‘¤]*)*(?=\n\s*[â‘ â‘¡â‘¢â‘£â‘¤]|$)',
            
            # ë§ˆí¬ë‹¤ìš´ ì½”ë“œ ë¸”ë¡
            r'```[\s\S]*?```',
            
            # ìˆ˜ì‹ì´ë‚˜ ë‹¤ì´ì–´ê·¸ë¨
            r'[A-Z]\s*â†’\s*[A-Z].*?(?=\n\s*[â‘ â‘¡â‘¢â‘£â‘¤]|$)',
            r'\w+\s*=\s*\w+.*?(?=\n\s*[â‘ â‘¡â‘¢â‘£â‘¤]|$)'
        ]
        
        for pattern in passage_patterns:
            match = re.search(pattern, text, re.DOTALL | re.IGNORECASE | re.MULTILINE)
            if match:
                passage_content = match.group(0).strip()
                # ìœ ì˜ë¯¸í•œ ê¸¸ì´ì˜ ë³´ê¸°ë§Œ ì¶”ì¶œ (ìµœì†Œ 15ì)
                if len(passage_content) > 15 and not re.match(r'^\s*[â‘ â‘¡â‘¢â‘£â‘¤]', passage_content):
                    question['passage'] = passage_content
                    # ë¬¸ì œ í…ìŠ¤íŠ¸ì—ì„œ ë³´ê¸° ë¶€ë¶„ ì •í™•íˆ ì œê±°
                    clean_text = text.replace(passage_content, '').strip()
                    clean_text = re.sub(r'\s+', ' ', clean_text)  # ê³µë°± ì •ë¦¬
                    question['question_text'] = clean_text
                    print(f"ğŸ“– ë¬¸ì œ {question.get('question_number', '?')}ë²ˆ ë³´ê¸° ë¶„ë¦¬: {len(passage_content)}ì")
                    break
        
        return question
    
    def _extract_options_from_text(self, question: Dict, text: str) -> Dict:
        """í…ìŠ¤íŠ¸ì—ì„œ ì„ íƒì§€ ì¶”ì¶œ (ì¤‘ë³µ ë°©ì§€ ë° ìˆ«ì ì„ íƒì§€ ê°•í™”)"""
        
        # ê¸°ì¡´ ì„ íƒì§€ ë³´ì¡´ (ë®ì–´ì“°ì§€ ì•Šê³  ë³´ê°•)
        existing_options = question.get('options', [])
        
        # í™•ì¥ëœ ì„ íƒì§€ íŒ¨í„´ (ìˆœì„œëŒ€ë¡œ ì‹œë„)
        patterns = [
            # 1ìˆœìœ„: ë™ê·¸ë¼ë¯¸ ìˆ«ì (í•œêµ­ì–´ ì‹œí—˜ í‘œì¤€)
            r'([â‘ â‘¡â‘¢â‘£â‘¤])\s*([^\nâ‘ â‘¡â‘¢â‘£â‘¤]*?)(?=\s*[â‘ â‘¡â‘¢â‘£â‘¤]|\n\n|$)',
            # 2ìˆœìœ„: ê´„í˜¸ ìˆ«ì
            r'([1-5]\))\s*([^\n1-5)]*?)(?=\s*[1-5]\)|\n\n|$)',
            # 3ìˆœìœ„: ì•ŒíŒŒë²³
            r'([A-E]\))\s*([^\nA-E)]*?)(?=\s*[A-E]\)|\n\n|$)',
        ]
        
        extracted_options = []
        clean_question_text = text
        
        for pattern in patterns:
            matches = re.findall(pattern, text, re.DOTALL | re.MULTILINE)
            if len(matches) >= 2:  # ìµœì†Œ 2ê°œ ì„ íƒì§€ ë°œê²¬
                
                # ì²« ë²ˆì§¸ ì„ íƒì§€ ìœ„ì¹˜ ì°¾ê¸°
                first_match = re.search(pattern, text, re.DOTALL | re.MULTILINE)
                if first_match:
                    # ì„ íƒì§€ ì´ì „ê¹Œì§€ê°€ ë¬¸ì œ ì§€ë¬¸
                    clean_question_text = text[:first_match.start()].strip()
                    
                    # ì„ íƒì§€ ë°°ì—´ êµ¬ì„±
                    for marker, content in matches:
                        content = content.strip()
                        
                        # ë¹ˆ ë‚´ìš© ì œì™¸
                        if not content or content == marker:
                            continue
                            
                        # ìˆ«ì ì„ íƒì§€ íŠ¹ë³„ ì²˜ë¦¬
                        if self._is_numeric_choice(content):
                            option_text = f"{marker} {content}"
                            print(f"   ğŸ”¢ ìˆ«ì ì„ íƒì§€ ê°ì§€: {option_text}")
                        elif len(content) > 100:  # ë„ˆë¬´ ê¸´ ë‚´ìš©ì€ ë¬¸ì œ ì§€ë¬¸ì´ ì„ì¸ ê²ƒ
                            continue
                        else:
                            option_text = f"{marker} {content}"
                        
                        extracted_options.append(option_text)
                    
                    break  # ì²« ë²ˆì§¸ ì„±ê³µí•œ íŒ¨í„´ ì‚¬ìš©
        
        # ì¶”ì¶œ ê²°ê³¼ ì ìš©
        if len(extracted_options) >= 2:
            # ê¸°ì¡´ ì„ íƒì§€ë³´ë‹¤ ë” ë§ì´ ì¶”ì¶œí–ˆê±°ë‚˜ ê¸°ì¡´ì´ ì—†ë‹¤ë©´ êµì²´
            if len(extracted_options) > len(existing_options):
                question['options'] = extracted_options
                question['question_text'] = clean_question_text
                print(f"ğŸ”§ ë¬¸ì œ {question.get('question_number', '?')}ë²ˆ ì„ íƒì§€ ì¶”ì¶œ: {len(extracted_options)}ê°œ (ê¸°ì¡´: {len(existing_options)}ê°œ)")
            else:
                print(f"   ê¸°ì¡´ ì„ íƒì§€ ìœ ì§€: {len(existing_options)}ê°œ")
        
        return question
    
    def _is_numeric_choice(self, content: str) -> bool:
        """ìˆ«ì ì„ íƒì§€ ì—¬ë¶€ íŒë‹¨"""
        
        # ìˆœìˆ˜ ìˆ«ì
        if re.match(r'^\d+(\.\d+)?$', content):
            return True
        
        # ìˆ«ì + ë‹¨ìœ„
        if re.match(r'^\d+(\.\d+)?\s*[%ê°œëª…ì›ë‹¬ëŸ¬ì´ˆë¶„ì‹œkmÂ²ã¡ã¥kg]', content):
            return True
            
        # ìˆ˜ì‹
        if re.match(r'^[x-z]\s*=\s*\d+', content):
            return True
            
        # ë¶„ìˆ˜/ë¹„ìœ¨
        if re.match(r'^\d+/\d+|\d+:\d+', content):
            return True
        
        return False
    
    def _clean_duplicate_content(self, question: Dict) -> Dict:
        """ì¤‘ë³µ ë‚´ìš© ì •ë¦¬ ë° ê²€ì¦ (ê°•í™”ëœ ì¤‘ë³µ ì œê±°)"""
        
        question_text = question.get('question_text', '')
        options = question.get('options', [])
        passage = question.get('passage', '')
        
        print(f"   ğŸ§¹ ì¤‘ë³µ ë‚´ìš© ì •ë¦¬ ì‹œì‘")
        
        # 1. ë¬¸ì œ í…ìŠ¤íŠ¸ì—ì„œ ì„ íƒì§€ ë§ˆì»¤ ë° ë‚´ìš© ì œê±°
        cleaned_text = question_text
        
        for option in options:
            if option and len(option) > 2:
                # ë§ˆì»¤ ì¶”ì¶œ (â‘ , â‘¡, 1), 2) ë“±)
                marker_match = re.match(r'^([â‘ â‘¡â‘¢â‘£â‘¤]|[1-5]\))', option)
                if marker_match:
                    marker = marker_match.group(1)
                    # ë§ˆì»¤ê°€ ë¬¸ì œ í…ìŠ¤íŠ¸ì— ë‚¨ì•„ìˆìœ¼ë©´ ì œê±°
                    cleaned_text = cleaned_text.replace(marker, '').strip()
                    
                    # ì„ íƒì§€ ë‚´ìš©ì´ ë¬¸ì œ í…ìŠ¤íŠ¸ì— ì¤‘ë³µë˜ì–´ ìˆìœ¼ë©´ ì œê±°
                    option_content = option.replace(marker, '').strip()
                    if option_content and len(option_content) > 5:
                        # ë¶€ë¶„ ì¼ì¹˜ ì œê±° (ì„ íƒì§€ ë‚´ìš©ì´ ë¬¸ì œì— í¬í•¨ëœ ê²½ìš°)
                        if option_content in cleaned_text:
                            cleaned_text = cleaned_text.replace(option_content, '').strip()
        
        # 2. ì—°ì†ëœ ê³µë°± ì •ë¦¬
        cleaned_text = re.sub(r'\s+', ' ', cleaned_text).strip()
        
        # 3. ë¶ˆì™„ì „í•œ ë¬¸ì¥ ì •ë¦¬ (ì„ íƒì§€ ì œê±° í›„ ë‚¨ì€ ì¡°ê°ë“¤)
        cleaned_text = re.sub(r'\s*[â‘ â‘¡â‘¢â‘£â‘¤]\s*$', '', cleaned_text)  # ëì— ë‚¨ì€ ë§ˆì»¤
        cleaned_text = re.sub(r'\s*[1-5]\)\s*$', '', cleaned_text)  # ëì— ë‚¨ì€ ê´„í˜¸ ìˆ«ì
        
        # 4. ë¬¸ì œ í…ìŠ¤íŠ¸ê°€ ë„ˆë¬´ ì§§ì•„ì§€ë©´ ê²½ê³ 
        if len(cleaned_text) < 10 and len(question_text) > 10:
            print(f"   âš ï¸ ë¬¸ì œ í…ìŠ¤íŠ¸ê°€ ë„ˆë¬´ ì§§ì•„ì§: '{cleaned_text[:30]}...'")
            # ì›ë³¸ì˜ ì¼ë¶€ë¥¼ ë³´ì¡´
            cleaned_text = question_text[:100] + '...' if len(question_text) > 100 else question_text
        
        question['question_text'] = cleaned_text
        
        # 5. ë¹ˆ í•„ë“œ ì²˜ë¦¬
        if not passage:
            question['passage'] = ''
        
        # 6. ì„ íƒì§€ ì¤‘ë³µ ì œê±°
        unique_options = []
        seen_contents = set()
        
        for option in options:
            option_key = re.sub(r'^[â‘ â‘¡â‘¢â‘£â‘¤]|^[1-5]\)', '', option).strip().lower()
            if option_key not in seen_contents and len(option_key) > 0:
                unique_options.append(option)
                seen_contents.add(option_key)
        
        question['options'] = unique_options
        
        print(f"   âœ… ì •ë¦¬ ì™„ë£Œ - í…ìŠ¤íŠ¸: {len(cleaned_text)}ì, ì„ íƒì§€: {len(unique_options)}ê°œ")
        
        return question
    
    def _verify_content_separation_quality(self, question: Dict) -> Dict:
        """ë‚´ìš© ë¶„ë¦¬ í’ˆì§ˆ ìµœì¢… ê²€ì¦"""
        
        q_num = question.get('question_number', '?')
        question_text = question.get('question_text', '')
        options = question.get('options', [])
        passage = question.get('passage', '')
        
        issues = []
        
        # 1. ì„ íƒì§€ ê°œìˆ˜ ê²€ì¦
        if len(options) < 2:
            issues.append("ì„ íƒì§€ ë¶€ì¡±")
        elif len(options) > 5:
            issues.append("ì„ íƒì§€ ê³¼ë‹¤")
        
        # 2. ë¬¸ì œ í…ìŠ¤íŠ¸ì— ì„ íƒì§€ ë§ˆì»¤ ì”ì¡´ ê²€ì¦
        remaining_markers = re.findall(r'[â‘ â‘¡â‘¢â‘£â‘¤]|[1-5]\)', question_text)
        if len(remaining_markers) > 0:
            issues.append(f"ë§ˆì»¤ ì”ì¡´ {len(remaining_markers)}ê°œ")
        
        # 3. ì„ íƒì§€ í˜•ì‹ ê²€ì¦
        malformed_options = 0
        for option in options:
            if not re.match(r'^[â‘ â‘¡â‘¢â‘£â‘¤]|^[1-5]\)', option):
                malformed_options += 1
        
        if malformed_options > 0:
            issues.append(f"í˜•ì‹ ì˜¤ë¥˜ {malformed_options}ê°œ")
        
        # 4. ê²°ê³¼ ì¶œë ¥
        if issues:
            print(f"   âš ï¸ ë¬¸ì œ {q_num}ë²ˆ í’ˆì§ˆ ì´ìŠˆ: {', '.join(issues)}")
        else:
            print(f"   âœ… ë¬¸ì œ {q_num}ë²ˆ ë¶„ë¦¬ í’ˆì§ˆ ì–‘í˜¸")
        
        return question
    
    def _verify_extraction_against_structure(self, extracted_questions: List[Dict], structure_analysis: Dict) -> Dict:
        """ì¶”ì¶œ ê²°ê³¼ì™€ êµ¬ì¡° ë¶„ì„ ë¹„êµ ê²€ì¦"""
        
        expected_count = structure_analysis.get('analysis_summary', {}).get('total_questions', 0)
        actual_count = len(extracted_questions)
        
        # ë¬¸ì œ ë²ˆí˜¸ ê²€ì¦
        expected_numbers = set(range(1, expected_count + 1))
        actual_numbers = {q.get('question_number', 0) for q in extracted_questions}
        
        missing_numbers = expected_numbers - actual_numbers
        extra_numbers = actual_numbers - expected_numbers
        
        verification = {
            "expected_count": expected_count,
            "actual_count": actual_count,
            "count_match": expected_count == actual_count,
            "missing_questions": sorted(list(missing_numbers)),
            "extra_questions": sorted(list(extra_numbers)),
            "accuracy_rate": len(actual_numbers & expected_numbers) / max(len(expected_numbers), 1) * 100
        }
        
        print(f"ğŸ” ì¶”ì¶œ ê²€ì¦ ê²°ê³¼: {actual_count}/{expected_count}ê°œ ì¶”ì¶œ (ì •í™•ë„: {verification['accuracy_rate']:.1f}%)")
        
        if missing_numbers:
            print(f"   âš ï¸ ëˆ„ë½ëœ ë¬¸ì œ: {sorted(list(missing_numbers))}")
        if extra_numbers:
            print(f"   âš ï¸ ì¶”ê°€ëœ ë¬¸ì œ: {sorted(list(extra_numbers))}")
        
        return verification
    
    async def _create_single_page_images(self, pdf_path: str, page_info: Dict, upload_id: int, 
                                       include_boundary: bool = True) -> List[Dict]:
        """ë‹¨ì¼ í˜ì´ì§€ì˜ ê³ í•´ìƒë„ ì´ë¯¸ì§€ ìƒì„± (í˜ì´ì§€ ê²½ê³„ ê³ ë ¤)"""
        
        doc = fitz.open(pdf_path)
        page_num = page_info['page_number'] - 1  # 0-based index
        total_pages = len(doc)
        
        images = []
        
        # ë©”ì¸ í˜ì´ì§€ ì´ë¯¸ì§€
        page = doc.load_page(page_num)
        has_special = page_info.get('special_elements', [])
        # API ì œí•œì„ ê³ ë ¤í•œ ì ì‘ì  í•´ìƒë„ ì„¤ì •
        scale = 14.0 if has_special else 12.0  # íŠ¹ìˆ˜ ìš”ì†Œ ìˆìœ¼ë©´ 14ë°°, ì¼ë°˜ì€ 12ë°°
        
        mat = fitz.Matrix(scale, scale)
        pix = page.get_pixmap(matrix=mat)
        
        # ì´ë¯¸ì§€ í¬ê¸° ìµœì í™”
        img_data = self._optimize_image_size(pix)
        
        import base64
        img_b64 = base64.b64encode(img_data).decode('utf-8')
        
        images.append({
            'page_number': page_info['page_number'],
            'image_data': img_b64,
            'scale': scale,
            'question_range': page_info.get('questions_on_page', []),
            'special_elements': has_special,
            'is_main': True
        })
        
        # í˜ì´ì§€ ê²½ê³„ ì²˜ë¦¬: ë‹¤ìŒ í˜ì´ì§€ ì¼ë¶€ë„ í¬í•¨ (ì„ íƒì§€ ì—°ì†ì„±ì„ ìœ„í•´)
        if include_boundary and page_num + 1 < total_pages:
            next_page = doc.load_page(page_num + 1)
            
            # ë‹¤ìŒ í˜ì´ì§€ì˜ ìƒë‹¨ ì¼ë¶€ë§Œ ìº¡ì²˜ (ì„ íƒì§€ ì—°ì†ì„± í™•ì¸ìš©)
            page_rect = next_page.rect
            top_portion_rect = fitz.Rect(0, 0, page_rect.width, page_rect.height * 0.3)  # ìƒë‹¨ 30%
            
            mat = fitz.Matrix(scale, scale)
            pix = next_page.get_pixmap(matrix=mat, clip=top_portion_rect)
            img_data = self._optimize_image_size(pix)
            img_b64 = base64.b64encode(img_data).decode('utf-8')
            
            images.append({
                'page_number': page_info['page_number'] + 1,
                'image_data': img_b64,
                'scale': scale,
                'question_range': [],
                'special_elements': [],
                'is_main': False,
                'is_boundary_check': True,
                'boundary_type': 'next_page_top'
            })
            
            print(f"   ğŸ“„ í˜ì´ì§€ ê²½ê³„ í™•ì¸: ë‹¤ìŒ í˜ì´ì§€ ìƒë‹¨ í¬í•¨")
        
        doc.close()
        return images
    
    def _create_simple_page_extraction_prompt(self, page_info: Dict, structure_analysis: Optional[Dict] = None) -> str:
        """í…œí”Œë¦¿ ê¸°ë°˜ ì •ë°€ í˜ì´ì§€ë³„ ì¶”ì¶œ í”„ë¡¬í”„íŠ¸ (ìƒì„¸ êµ¬ì¡° ë¶„ì„ í™œìš©)"""
        
        questions_on_page = page_info.get('questions_on_page', [])
        page_num = page_info['page_number']
        question_count = len(questions_on_page)
        special_elements = page_info.get('special_elements', [])
        
        if questions_on_page:
            question_range = f"{min(questions_on_page)}ë²ˆ ~ {max(questions_on_page)}ë²ˆ"
        else:
            question_range = "ì•Œ ìˆ˜ ì—†ìŒ"
        
        # êµ¬ì¡° ë¶„ì„ì—ì„œ ì´ í˜ì´ì§€ì˜ ìƒì„¸ í…œí”Œë¦¿ ì°¾ê¸°
        detailed_templates = []
        special_questions_on_this_page = []
        if structure_analysis:
            # ìƒˆë¡œìš´ ìƒì„¸ í…œí”Œë¦¿ ì •ë³´ ì¶”ì¶œ
            question_templates = structure_analysis.get('question_analysis', {}).get('detailed_question_templates', [])
            for template in question_templates:
                if template.get('question_number') in questions_on_page:
                    detailed_templates.append(template)
            
            # ê¸°ì¡´ íŠ¹ìˆ˜ ë¬¸ì œ ì •ë³´ë„ ìœ ì§€
            detailed_questions = structure_analysis.get('question_analysis', {}).get('detailed_questions', [])
            for special_q in detailed_questions:
                if special_q.get('page_location') == page_num:
                    special_questions_on_this_page.append(special_q)
        
        # í…œí”Œë¦¿ ê¸°ë°˜ í”„ë¡¬í”„íŠ¸
        prompt = f"""ğŸ“„ í˜ì´ì§€ {page_num} í…œí”Œë¦¿ ê¸°ë°˜ ì •ë°€ ë¬¸ì œ ì¶”ì¶œ

ğŸ¯ **ì¶”ì¶œ ëª©í‘œ**:
- ì´ í˜ì´ì§€ì— ìˆëŠ” ë¬¸ì œ {question_range} ì¶”ì¶œ
- ì´ {question_count}ê°œ ë¬¸ì œ ì˜ˆìƒ
- êµ¬ì¡° ë¶„ì„ì—ì„œ ì œê³µëœ ì •í™•í•œ í…œí”Œë¦¿ ì‚¬ìš©"""
        
        # ìƒì„¸ í…œí”Œë¦¿ ì •ë³´ ì¶”ê°€
        if detailed_templates:
            prompt += f"""

ğŸ¯ **ë¬¸ì œë³„ ì •í™•í•œ ì¶”ì¶œ í…œí”Œë¦¿**:"""
            
            for template in detailed_templates:
                q_num = template.get('question_number')
                number_format = template.get('number_format', f'{q_num}.')
                text_start = template.get('text_start_pattern', '')
                text_end = template.get('text_end_pattern', '')
                choice_format = template.get('choice_format', 'â‘ â‘¡â‘¢â‘£')
                choice_count = template.get('choice_count', 4)
                special_elem = template.get('special_elements', {})
                
                prompt += f"""
                
ğŸ“‹ **ë¬¸ì œ {q_num}ë²ˆ ì¶”ì¶œ í…œí”Œë¦¿**:
- ë²ˆí˜¸ í˜•ì‹: "{number_format}"
- ì§€ë¬¸ ì‹œì‘: "{text_start}"
- ì§€ë¬¸ ë: "{text_end}"
- ì„ íƒì§€: {choice_format} ({choice_count}ê°œ)"""
                
                if special_elem.get('has_table'):
                    table_pos = special_elem.get('table_position', 'ì§€ë¬¸ ì•„ë˜')
                    table_struct = special_elem.get('table_structure', 'í‘œ')
                    prompt += f"""
- ğŸ”¸ í‘œ ìœ„ì¹˜: {table_pos}
- ğŸ”¸ í‘œ êµ¬ì¡°: {table_struct}
- ğŸ”¸ ë°˜ë“œì‹œ ë§ˆí¬ë‹¤ìš´ í‘œ í˜•ì‹ìœ¼ë¡œ ì¶”ì¶œ: | í—¤ë”1 | í—¤ë”2 |"""
                
                if special_elem.get('has_code'):
                    prompt += f"""
- ğŸ”¸ ì½”ë“œ ë¸”ë¡ í¬í•¨ë¨ - ë“¤ì—¬ì“°ê¸°ì™€ êµ¬ì¡° ì •í™•íˆ ë³´ì¡´"""
                    
                if special_elem.get('has_diagram'):
                    prompt += f"""
- ğŸ”¸ ë‹¤ì´ì–´ê·¸ë¨ í¬í•¨ë¨ - ëª¨ë“  êµ¬ì„±ìš”ì†Œ ìƒì„¸íˆ ì„¤ëª…"""

        # íŠ¹ìˆ˜ ìš”ì†Œê°€ ìˆëŠ” ê²½ìš° ì„¸ë¶€ ì§€ì‹œ ì¶”ê°€
        if special_elements or special_questions_on_this_page:
            prompt += f"""
            
ğŸ” **íŠ¹ìˆ˜ ìš”ì†Œ ì£¼ì˜ì‚¬í•­**:"""
            
            if special_questions_on_this_page:
                for special_q in special_questions_on_this_page:
                    q_num = special_q.get('question_number')
                    special_type = special_q.get('question_type', '')
                    special_desc = special_q.get('special_elements', '')
                    
                    if 'table' in special_type:
                        prompt += f"""
- ğŸ”¸ ë¬¸ì œ {q_num}ë²ˆ: í‘œ ë°ì´í„°ê°€ í¬í•¨ë¨ ({special_desc})
  â†’ í‘œì˜ ëª¨ë“  í–‰ê³¼ ì—´ì„ ì •í™•íˆ ì¶”ì¶œí•˜ì„¸ìš”
  â†’ ë§ˆí¬ë‹¤ìš´ í‘œ í˜•ì‹ìœ¼ë¡œ ì¶”ì¶œí•˜ì„¸ìš”: | í—¤ë”1 | í—¤ë”2 | í—¤ë”3 |
  â†’ í‘œ êµ¬ë¶„ì„ ë„ í¬í•¨í•˜ì„¸ìš”: |-------|-------|-------|
  â†’ ëª¨ë“  ë°ì´í„° í–‰ì„ ì™„ì „íˆ í¬í•¨í•˜ì„¸ìš”"""
                    
                    elif 'diagram' in special_type:
                        prompt += f"""
- ğŸ”¸ ë¬¸ì œ {q_num}ë²ˆ: ë‹¤ì´ì–´ê·¸ë¨ í¬í•¨ë¨ ({special_desc})
  â†’ ë‹¤ì´ì–´ê·¸ë¨ì˜ êµ¬ì¡°ì™€ ë‚´ìš©ì„ í…ìŠ¤íŠ¸ë¡œ ì„¤ëª…í•˜ì„¸ìš”
  â†’ íŠ¸ë¦¬, ê·¸ë˜í”„, ì°¨íŠ¸ ë“±ì˜ ê´€ê³„ë¥¼ ëª…í™•íˆ ê¸°ìˆ í•˜ì„¸ìš”"""
                    
                    elif 'code' in special_type:
                        prompt += f"""
- ğŸ”¸ ë¬¸ì œ {q_num}ë²ˆ: ì½”ë“œ ë¸”ë¡ í¬í•¨ë¨ ({special_desc})
  â†’ ì½”ë“œì˜ ë“¤ì—¬ì“°ê¸°ì™€ êµ¬ì¡°ë¥¼ ì •í™•íˆ ë³´ì¡´í•˜ì„¸ìš”
  â†’ ëª¨ë“  ë³€ìˆ˜ëª…, í•¨ìˆ˜ëª…ì„ ì •í™•íˆ ì¶”ì¶œí•˜ì„¸ìš”"""
            
            if 'tables' in special_elements:
                prompt += """
- ğŸ“Š ì´ í˜ì´ì§€ì— í‘œê°€ ìˆìŠµë‹ˆë‹¤ - ë§ˆí¬ë‹¤ìš´ í‘œ í˜•ì‹ìœ¼ë¡œ ì™„ì „íˆ ì¶”ì¶œí•˜ì„¸ìš”
  ì˜ˆ: | ì—´1 | ì—´2 | ì—´3 |
      |-----|-----|-----|
      | ê°’1 | ê°’2 | ê°’3 |"""
            
            if 'code_blocks' in special_elements:
                prompt += """
- ğŸ’» ì´ í˜ì´ì§€ì— ì½”ë“œê°€ ìˆìŠµë‹ˆë‹¤ - ì½”ë“œ êµ¬ì¡°ë¥¼ ì •í™•íˆ ë³´ì¡´í•˜ì„¸ìš”"""
            
            if 'diagrams' in special_elements:
                prompt += """
- ğŸ“ˆ ì´ í˜ì´ì§€ì— ë‹¤ì´ì–´ê·¸ë¨ì´ ìˆìŠµë‹ˆë‹¤ - êµ¬ì¡°ë¥¼ í…ìŠ¤íŠ¸ë¡œ ì„¤ëª…í•˜ì„¸ìš”"""

        prompt += f"""

ğŸ“ **ì¶”ì¶œ ë°©ë²•**:
1. ê° ë¬¸ì œì˜ ë²ˆí˜¸ë¥¼ ì •í™•íˆ í™•ì¸
2. ë¬¸ì œ ì§€ë¬¸ì„ ì™„ì „íˆ ì¶”ì¶œ
3. ëª¨ë“  ì„ íƒì§€(â‘ â‘¡â‘¢â‘£â‘¤)ë¥¼ ì™„ì „íˆ ì¶”ì¶œ
4. í˜ì´ì§€ ëì—ì„œ ì˜ë¦° ì„ íƒì§€ê°€ ìˆë‹¤ë©´:
   - ë©”ì¸ ì´ë¯¸ì§€ì—ì„œ ë³´ì´ëŠ” ë¶€ë¶„ê¹Œì§€ ì¶”ì¶œ
   - ì¶”ê°€ ì´ë¯¸ì§€(ë‹¤ìŒ í˜ì´ì§€ ìƒë‹¨)ì—ì„œ ì—°ì† ë¶€ë¶„ í™•ì¸
   - ë‘ ì´ë¯¸ì§€ë¥¼ ì¢…í•©í•´ì„œ ì™„ì „í•œ ì„ íƒì§€ êµ¬ì„±
5. ë¶ˆì™„ì „í•œ ë¬¸ì œëŠ” ì œì™¸í•˜ì§€ ë§ê³  ê°€ëŠ¥í•œ ë¶€ë¶„ê¹Œì§€ ì¶”ì¶œ
6. ì—¬ëŸ¬ ì´ë¯¸ì§€ê°€ ì œê³µë˜ë©´ ëª¨ë‘ í™œìš©í•´ì„œ ì™„ì „í•œ ë¬¸ì œ ì¶”ì¶œ

ğŸ” **í˜ì´ì§€ ê²½ê³„ ì²˜ë¦¬**:
- ì²« ë²ˆì§¸ ì´ë¯¸ì§€: ë©”ì¸ í˜ì´ì§€ (ë¬¸ì œ {question_range})
- ë‘ ë²ˆì§¸ ì´ë¯¸ì§€ (ìˆë‹¤ë©´): ë‹¤ìŒ í˜ì´ì§€ ìƒë‹¨ (ì„ íƒì§€ ì—°ì†ì„± í™•ì¸ìš©)
- ë‘ ì´ë¯¸ì§€ë¥¼ ì¢…í•©í•´ì„œ ì™„ì „í•œ ë¬¸ì œë¥¼ êµ¬ì„±í•˜ì„¸ìš”

ğŸ“Š **ì™„ì „í•œ JSON ì¶œë ¥ í˜•ì‹**:
```json
{{
  "questions": [
    {{
      "question_number": 1,
      "question_text": "ë¬¸ì œ ì§€ë¬¸ë§Œ (ì„ íƒì§€, ë³´ê¸° ì œì™¸)",
      "passage": "ë³´ê¸°/í‘œ/ì½”ë“œ/ë‹¤ì´ì–´ê·¸ë¨ ë‚´ìš© (ì—†ìœ¼ë©´ ë¹ˆ ë¬¸ìì—´)",
      "options": ["â‘  ì²«ë²ˆì§¸ ì„ íƒì§€", "â‘¡ ë‘ë²ˆì§¸ ì„ íƒì§€", "â‘¢ ì„¸ë²ˆì§¸ ì„ íƒì§€", "â‘£ ë„¤ë²ˆì§¸ ì„ íƒì§€"],
      "page_number": {page_num},
      "has_table": true/false,
      "has_code": true/false,
      "has_diagram": true/false
    }}
  ]
}}
```

ğŸš¨ **ì ˆëŒ€ ê¸ˆì§€ì‚¬í•­**:
- question_textì— ì„ íƒì§€(â‘ â‘¡â‘¢â‘£)ë¥¼ í¬í•¨í•˜ì§€ ë§ˆì„¸ìš”
- question_textì— ë³´ê¸° ë‚´ìš©ì„ í¬í•¨í•˜ì§€ ë§ˆì„¸ìš”
- question_textëŠ” ì˜¤ì§ ë¬¸ì œ ì§€ë¬¸ë§Œ í¬í•¨í•˜ì„¸ìš”  
- ì„ íƒì§€ëŠ” ë°˜ë“œì‹œ options ë°°ì—´ì— ë³„ë„ë¡œ ë¶„ë¦¬í•˜ì„¸ìš”
- ë³´ê¸°ëŠ” ë°˜ë“œì‹œ passage í•„ë“œì— ë³„ë„ë¡œ ë¶„ë¦¬í•˜ì„¸ìš”

ğŸ“– **ë³´ê¸°/ì§€ë¬¸ ë¶„ë¦¬ ê°€ì´ë“œ**:
- <ë³´ê¸°>ë¡œ ì‹œì‘í•˜ëŠ” ë‚´ìš©
- í‘œ, ì½”ë“œ, ë‹¤ì´ì–´ê·¸ë¨ ë“±
- ë¬¸ì œ ì•ì— ì œì‹œëœ ì¶”ê°€ ì •ë³´
- ì´ëŸ° ë‚´ìš©ì€ ëª¨ë‘ "passage" í•„ë“œì— ì €ì¥í•˜ì„¸ìš”

âš¡ **ë§¤ìš° ì¤‘ìš”**: 
- í‘œ, ë‹¤ì´ì–´ê·¸ë¨, ì½”ë“œ ë“± íŠ¹ìˆ˜ ìš”ì†Œë¥¼ ì ˆëŒ€ ëˆ„ë½í•˜ì§€ ë§ˆì„¸ìš”
- í‘œëŠ” ë°˜ë“œì‹œ ë§ˆí¬ë‹¤ìš´ í‘œ í˜•ì‹ìœ¼ë¡œ ì¶”ì¶œí•˜ì„¸ìš” (| ë°ì´í„° | í˜•íƒœ)
- ë‹¤ì´ì–´ê·¸ë¨ì€ ëª¨ë“  êµ¬ì„±ìš”ì†Œì™€ ê´€ê³„ë¥¼ ìƒì„¸íˆ ì„¤ëª…í•˜ì„¸ìš”
- ë‹¤ì´ì–´ê·¸ë¨ ì„¤ëª…ì—ëŠ” ë…¸ë“œëª…, ì—°ê²°ê´€ê³„, ì‹œê°ì  ìš”ì†Œë¥¼ ì „ë¶€ í¬í•¨í•˜ì„¸ìš”
- í‘œì˜ ëª¨ë“  í–‰ê³¼ ì—´ì„ ì™„ì „íˆ ë³´ì¡´í•˜ì„¸ìš”
- **íŠ¹ë³„ ì£¼ì˜**: ì„ íƒì§€ì— í¬í•¨ëœ ìˆ«ì+ë‹¨ìœ„(ì˜ˆ: 256KB), ì—°ì‚°ì(ì˜ˆ: !=, >=), SQL ë¬¸(ì˜ˆ: SELECT * FROM), 16ì§„ìˆ˜(ì˜ˆ: 0xFFFF), ì½”ë“œ ë¬¸ë²•(ì˜ˆ: int main())ì„ ë°˜ë“œì‹œ ì •í™•íˆ ì¶”ì¶œí•˜ì„¸ìš”
- í˜ì´ì§€ ê²½ê³„ì—ì„œ ì˜ë¦° ì„ íƒì§€ë„ ë³´ì´ëŠ” ë¶€ë¶„ê¹Œì§€ ì¶”ì¶œí•˜ì„¸ìš”
- ë¬¸ì œ ë²ˆí˜¸ì™€ ë‚´ìš©ì„ ì •í™•íˆ ë§¤ì¹­í•˜ì„¸ìš”"""
        
        return prompt
    
    async def _extract_questions_from_single_page(self, page_images: List[Dict], prompt: str, page_info: Dict) -> List[Dict]:
        """ë‹¨ì¼ í˜ì´ì§€ì—ì„œ ë¬¸ì œ ì¶”ì¶œ"""
        
        try:
            messages = [
                {
                    "role": "system",
                    "content": """ë‹¹ì‹ ì€ ì›ë³¸ í…ìŠ¤íŠ¸ ì™„ì „ ë³´ì¡´ ì „ë¬¸ê°€ì…ë‹ˆë‹¤.

ğŸ”’ **ì ˆëŒ€ ì›ì¹™ - ì›ë³¸ ê·¸ëŒ€ë¡œ**:
- ë³´ì´ëŠ” í…ìŠ¤íŠ¸ë¥¼ **í•œ ê¸€ìë„** ë°”ê¾¸ì§€ ë§ˆì„¸ìš”
- ìˆ«ìë¥¼ **ì ˆëŒ€** ë‹¤ë¥¸ ìˆ«ìë¡œ ë°”ê¾¸ì§€ ë§ˆì„¸ìš” (8.2 â†’ 8 ê¸ˆì§€)
- ì„ íƒì§€ ê°œìˆ˜ë¥¼ **ì ˆëŒ€** ì¡°ì‘í•˜ì§€ ë§ˆì„¸ìš” (4ê°œ â†’ 5ê°œ ê¸ˆì§€) 
- ì†Œìˆ˜ì ì„ **ì ˆëŒ€** ì œê±°í•˜ì§€ ë§ˆì„¸ìš” (9.4 â†’ 9 ê¸ˆì§€)

ğŸš¨ **ì¬í•´ì„/ìƒì„± ì ˆëŒ€ ê¸ˆì§€**:
- ë¬¸ì œë‚˜ ì„ íƒì§€ë¥¼ ì˜ì—­í•˜ì§€ ë§ˆì„¸ìš”
- ì—†ëŠ” ì„ íƒì§€ë¥¼ ë§Œë“¤ì–´ë‚´ì§€ ë§ˆì„¸ìš”
- ì„ íƒì§€ ìˆœì„œë¥¼ ë°”ê¾¸ì§€ ë§ˆì„¸ìš”
- ë‚´ìš©ì„ ìš”ì•½í•˜ê±°ë‚˜ ìˆ˜ì •í•˜ì§€ ë§ˆì„¸ìš”

ğŸ“¸ **ë³µì‚¬ê¸° ëª¨ë“œ**: 
- ì´ ì´ë¯¸ì§€ë¥¼ ë³µì‚¬ê¸°ë¡œ ìƒê°í•˜ì„¸ìš”
- ë³´ì´ëŠ” ê²ƒì„ **ê·¸ëŒ€ë¡œ** ë³µì‚¬í•˜ì„¸ìš”
- **í•´ì„í•˜ì§€ ë§ê³ ** **ë³µì‚¬**í•˜ì„¸ìš”

ğŸ”¢ **ìˆ«ì ì„ íƒì§€ íŠ¹ë³„ ì£¼ì˜**:
- "â‘  7.2" â†’ ì •í™•íˆ "â‘  7.2"ë¡œ ì¶”ì¶œ
- "â‘¡ 8.5%" â†’ ì •í™•íˆ "â‘¡ 8.5%"ë¡œ ì¶”ì¶œ
- "â‘¢ x = 10" â†’ ì •í™•íˆ "â‘¢ x = 10"ìœ¼ë¡œ ì¶”ì¶œ
- "â‘£ 256KB" â†’ ì •í™•íˆ "â‘£ 256KB"ë¡œ ì¶”ì¶œ

ğŸ“– **ë³´ê¸°/ì§€ë¬¸ ì™„ë²½ ë¶„ë¦¬**:
- ë¬¸ì œ ì•ì˜ <ë³´ê¸°>, í‘œ, ì½”ë“œ, ë‹¤ì´ì–´ê·¸ë¨ì€ "passage"ì— ì €ì¥
- ë¬¸ì œ ì§€ë¬¸ì€ "question_text"ì—ë§Œ ì €ì¥
- ì„ íƒì§€ëŠ” "options"ì—ë§Œ ì €ì¥
- ì ˆëŒ€ ë‚´ìš©ì„ ì„ì§€ ë§ˆì„¸ìš”!

âš¡ **ë¶„ë¦¬ ê·œì¹™**:
- question_text: ì§€ë¬¸ë§Œ (ì„ íƒì§€, ë³´ê¸° ì œì™¸)
- passage: ë³´ê¸°/í‘œ/ì½”ë“œ/ë‹¤ì´ì–´ê·¸ë¨ë§Œ
- options: ë³´ì´ëŠ” ì„ íƒì§€ **ì •í™•í•œ ê°œìˆ˜**ë§Œí¼
- íŠ¹ìˆ˜ê¸°í˜¸, ìˆ«ì, ì†Œìˆ˜ì  **ì›ë³¸ ê·¸ëŒ€ë¡œ**"""
                },
                {
                    "role": "user",
                    "content": [{"type": "text", "text": prompt}]
                }
            ]
            
            # í˜ì´ì§€ ì´ë¯¸ì§€ ì¶”ê°€
            for img_data in page_images:
                messages[1]["content"].append({
                    "type": "image_url",
                    "image_url": {
                        "url": f"data:image/png;base64,{img_data['image_data']}",
                        "detail": "high"
                    }
                })
            
            # GPT-4V í˜¸ì¶œ (ë” ë§ì€ í† í° í—ˆìš©)
            response = await self.openai_client.chat.completions.create(
                model="gpt-4o",
                messages=messages,
                max_tokens=6000,  # í† í° ìˆ˜ ì¦ê°€
                temperature=0.1
            )
            
            response_text = response.choices[0].message.content
            
            # ì‘ë‹µ íŒŒì‹±
            questions = self._parse_extraction_response(response_text)
            
            return questions
            
        except Exception as e:
            print(f"âš ï¸ í˜ì´ì§€ {page_info['page_number']} ë¬¸ì œ ì¶”ì¶œ ì‹¤íŒ¨: {e}")
            return []
    
    async def _extract_missing_questions(self, pdf_path: str, extracted_questions: List[Dict], 
                                       structure_analysis: Dict, upload_id: int) -> List[Dict]:
        """ëˆ„ë½ëœ ë¬¸ì œ ì¬ì¶”ì¶œ (êµ¬ì¡° ë¶„ì„ ë°©ì‹ ì°¸ê³ )"""
        
        try:
            # ì¶”ì¶œëœ ë¬¸ì œ ë²ˆí˜¸ í™•ì¸
            extracted_numbers = {q.get('question_number', 0) for q in extracted_questions}
            total_questions = structure_analysis.get('analysis_summary', {}).get('total_questions', 60)
            expected_numbers = set(range(1, total_questions + 1))
            
            missing_numbers = expected_numbers - extracted_numbers
            
            if not missing_numbers:
                print("âœ… ëª¨ë“  ë¬¸ì œê°€ ì¶”ì¶œë˜ì—ˆìŠµë‹ˆë‹¤.")
                return []
            
            print(f"ğŸ” ëˆ„ë½ëœ ë¬¸ì œ ì¬ì¶”ì¶œ ì¤‘: {sorted(list(missing_numbers))}")
            
            # ëˆ„ë½ëœ ë¬¸ì œê°€ ìˆëŠ” í˜ì´ì§€ë“¤ ì°¾ê¸°
            page_analysis = structure_analysis.get('page_analysis', [])
            missing_questions = []
            
            for page_info in page_analysis:
                if page_info.get('page_type') != 'pure_questions':
                    continue
                
                questions_on_page = set(page_info.get('questions_on_page', []))
                missing_on_this_page = missing_numbers & questions_on_page
                
                if missing_on_this_page:
                    print(f"   ğŸ“„ í˜ì´ì§€ {page_info['page_number']}ì—ì„œ {len(missing_on_this_page)}ê°œ ë¬¸ì œ ì¬ì¶”ì¶œ")
                    
                    # ì¬ì¶”ì¶œ ì‹œë„
                    retry_questions = await self._retry_extract_specific_questions(
                        pdf_path, page_info, list(missing_on_this_page), upload_id
                    )
                    
                    if retry_questions:
                        missing_questions.extend(retry_questions)
            
            return missing_questions
            
        except Exception as e:
            print(f"âš ï¸ ëˆ„ë½ ë¬¸ì œ ì¬ì¶”ì¶œ ì‹¤íŒ¨: {e}")
            return []
    
    async def _retry_extract_specific_questions(self, pdf_path: str, page_info: Dict, 
                                              missing_numbers: List[int], upload_id: int) -> List[Dict]:
        """íŠ¹ì • ë¬¸ì œ ë²ˆí˜¸ë“¤ì„ íƒ€ê²Ÿìœ¼ë¡œ ì¬ì¶”ì¶œ"""
        
        try:
            # ì¬ì¶”ì¶œìš© ê³ í•´ìƒë„ ì´ë¯¸ì§€ ìƒì„±
            retry_images = await self._create_single_page_images(pdf_path, page_info, upload_id)
            
            # íŠ¹ì • ë¬¸ì œë¥¼ íƒ€ê²Ÿìœ¼ë¡œ í•œ í”„ë¡¬í”„íŠ¸
            retry_prompt = f"""ğŸ” íŠ¹ì • ë¬¸ì œ ì¬ì¶”ì¶œ

ğŸ¯ **íƒ€ê²Ÿ ë¬¸ì œ**: {', '.join(map(str, missing_numbers))}ë²ˆ
ğŸ“„ **í˜ì´ì§€**: {page_info['page_number']}

ğŸ“ **ì¶”ì¶œ ë°©ë²•**:
ìœ„ ë¬¸ì œ ë²ˆí˜¸ë“¤ë§Œ ì •í™•íˆ ì°¾ì•„ì„œ ì¶”ì¶œí•˜ì„¸ìš”.
- ë¬¸ì œ ë²ˆí˜¸, ì§€ë¬¸, ì„ íƒì§€ë¥¼ ì™„ì „íˆ ì¶”ì¶œ
- ë¶ˆì™„ì „í•œ ê²½ìš° ì œì™¸

ğŸ“Š **JSON ì¶œë ¥**:
```json
{{"questions": [...]}}
```"""
            
            messages = [
                {
                    "role": "system",
                    "content": "íŠ¹ì • ë¬¸ì œ ë²ˆí˜¸ë¥¼ íƒ€ê²Ÿìœ¼ë¡œ ì •í™•í•œ ë¬¸ì œ ì¬ì¶”ì¶œì„ ìˆ˜í–‰í•˜ëŠ” ì „ë¬¸ê°€ì…ë‹ˆë‹¤."
                },
                {
                    "role": "user",
                    "content": [{"type": "text", "text": retry_prompt}]
                }
            ]
            
            # ì´ë¯¸ì§€ ì¶”ê°€
            for img_data in retry_images:
                messages[1]["content"].append({
                    "type": "image_url",
                    "image_url": {
                        "url": f"data:image/png;base64,{img_data['image_data']}",
                        "detail": "high"
                    }
                })
            
            response = await self.openai_client.chat.completions.create(
                model="gpt-4o",
                messages=messages,
                max_tokens=4000,
                temperature=0.1
            )
            
            questions = self._parse_extraction_response(response.choices[0].message.content)
            
            # íƒ€ê²Ÿ ë¬¸ì œ ë²ˆí˜¸ë§Œ í•„í„°ë§
            filtered_questions = [q for q in questions if q.get('question_number', 0) in missing_numbers]
            
            return filtered_questions
            
        except Exception as e:
            print(f"âš ï¸ íŠ¹ì • ë¬¸ì œ ì¬ì¶”ì¶œ ì‹¤íŒ¨: {e}")
            return []
    
    def _clean_and_sort_questions(self, questions: List[Dict]) -> List[Dict]:
        """ë¬¸ì œ ì •ë¦¬ ë° ì •ë ¬"""
        
        # ì¤‘ë³µ ì œê±° (ë¬¸ì œ ë²ˆí˜¸ ê¸°ì¤€)
        seen_numbers = set()
        cleaned_questions = []
        
        for question in questions:
            q_num = question.get('question_number', 0)
            if q_num > 0 and q_num not in seen_numbers:
                seen_numbers.add(q_num)
                cleaned_questions.append(question)
        
        # ë¬¸ì œ ë²ˆí˜¸ìˆœìœ¼ë¡œ ì •ë ¬
        cleaned_questions.sort(key=lambda x: x.get('question_number', 0))
        
        return cleaned_questions
    
    async def _validate_special_elements(self, questions: List[Dict], structure_analysis: Dict, 
                                       pdf_path: str, upload_id: int) -> List[Dict]:
        """íŠ¹ìˆ˜ ìš”ì†Œ ì¶”ì¶œ ê²€ì¦ ë° ë³´ì™„"""
        
        try:
            # êµ¬ì¡° ë¶„ì„ì—ì„œ íŠ¹ìˆ˜ ìš”ì†Œê°€ ìˆëŠ” ë¬¸ì œë“¤ ì°¾ê¸°
            detailed_questions = structure_analysis.get('question_analysis', {}).get('detailed_questions', [])
            
            special_question_numbers = []
            for special_q in detailed_questions:
                special_question_numbers.append(special_q.get('question_number'))
            
            if not special_question_numbers:
                print("   âœ… íŠ¹ìˆ˜ ìš”ì†Œ ë¬¸ì œ ì—†ìŒ")
                return questions
            
            print(f"   ğŸ” íŠ¹ìˆ˜ ìš”ì†Œ ê²€ì¦ ëŒ€ìƒ: {special_question_numbers}ë²ˆ")
            
            validated_questions = []
            missing_special_elements = []
            
            for question in questions:
                q_num = question.get('question_number', 0)
                
                if q_num in special_question_numbers:
                    # íŠ¹ìˆ˜ ìš”ì†Œê°€ ìˆì–´ì•¼ í•˜ëŠ” ë¬¸ì œì¸ì§€ í™•ì¸
                    special_info = None
                    for special_q in detailed_questions:
                        if special_q.get('question_number') == q_num:
                            special_info = special_q
                            break
                    
                    if special_info:
                        has_table = question.get('has_table', False) or question.get('table_data')
                        has_diagram = question.get('has_diagram', False) 
                        has_code = question.get('has_code', False) or question.get('code_content')
                        
                        expected_table = special_info.get('has_table', False)
                        expected_diagram = special_info.get('has_diagram', False)
                        expected_code = special_info.get('has_code', False)
                        
                        # íŠ¹ìˆ˜ ìš”ì†Œ ëˆ„ë½ ê²€ì‚¬
                        missing_elements = []
                        if expected_table and not has_table:
                            missing_elements.append('table')
                        if expected_diagram and not has_diagram:
                            missing_elements.append('diagram')
                        if expected_code and not has_code:
                            missing_elements.append('code')
                        
                        if missing_elements:
                            print(f"   âš ï¸ ë¬¸ì œ {q_num}ë²ˆ: {missing_elements} ëˆ„ë½ ê°ì§€")
                            missing_special_elements.append({
                                'question_number': q_num,
                                'missing_elements': missing_elements,
                                'special_info': special_info,
                                'current_question': question
                            })
                        else:
                            print(f"   âœ… ë¬¸ì œ {q_num}ë²ˆ: íŠ¹ìˆ˜ ìš”ì†Œ í™•ì¸ë¨")
                
                validated_questions.append(question)
            
            # êµ¬ì¡° ë¶„ì„ì—ì„œ ìƒì„¸ í…œí”Œë¦¿ ì°¾ê¸°
            detailed_templates = structure_analysis.get('question_analysis', {}).get('detailed_question_templates', [])
            
            # ëˆ„ë½ëœ íŠ¹ìˆ˜ ìš”ì†Œ í…œí”Œë¦¿ ê¸°ë°˜ ì¬ì¶”ì¶œ
            if missing_special_elements:
                print(f"   ğŸ”„ {len(missing_special_elements)}ê°œ ë¬¸ì œì˜ í…œí”Œë¦¿ ê¸°ë°˜ ì¬ì¶”ì¶œ ì¤‘...")
                
                for missing_item in missing_special_elements:
                    q_num = missing_item['question_number']
                    
                    # í•´ë‹¹ ë¬¸ì œì˜ ìƒì„¸ í…œí”Œë¦¿ ì°¾ê¸°
                    question_template = None
                    for template in detailed_templates:
                        if template.get('question_number') == q_num:
                            question_template = template
                            break
                    
                    if question_template:
                        # í…œí”Œë¦¿ ê¸°ë°˜ ì¬ì¶”ì¶œ ì‹œë„
                        page_location = None
                        for special_q in detailed_questions:
                            if special_q.get('question_number') == q_num:
                                page_location = special_q.get('page_location')
                                break
                        
                        if page_location:
                            enhanced_question = await self._re_extract_with_template(
                                question_template, pdf_path, page_location, upload_id
                            )
                            
                            if enhanced_question:
                                # ê¸°ì¡´ ë¬¸ì œë¥¼ ë³´ì™„ëœ ë¬¸ì œë¡œ êµì²´
                                for i, q in enumerate(validated_questions):
                                    if q.get('question_number') == q_num:
                                        validated_questions[i] = enhanced_question
                                        print(f"   âœ… ë¬¸ì œ {q_num}ë²ˆ í…œí”Œë¦¿ ê¸°ë°˜ ì¬ì¶”ì¶œ ì™„ë£Œ")
                                        break
                    else:
                        # í…œí”Œë¦¿ì´ ì—†ìœ¼ë©´ ê¸°ì¡´ ë°©ì‹ìœ¼ë¡œ ì¬ì¶”ì¶œ
                        enhanced_question = await self._re_extract_special_elements(
                            missing_item, pdf_path, upload_id
                        )
                        
                        if enhanced_question:
                            # ê¸°ì¡´ ë¬¸ì œë¥¼ ë³´ì™„ëœ ë¬¸ì œë¡œ êµì²´
                            for i, q in enumerate(validated_questions):
                                if q.get('question_number') == missing_item['question_number']:
                                    validated_questions[i] = enhanced_question
                                    print(f"   âœ… ë¬¸ì œ {missing_item['question_number']}ë²ˆ íŠ¹ìˆ˜ ìš”ì†Œ ë³´ì™„ ì™„ë£Œ")
                                    break
            
            # 2ë‹¨ê³„ ì •ë°€ ì²˜ë¦¬ ì‹œìŠ¤í…œìœ¼ë¡œ ëŒ€ì²´
            print(f"\nğŸ¯ 2ë‹¨ê³„ ì •ë°€ ì²˜ë¦¬ ì‹œìŠ¤í…œ ì‹œì‘...")
            
            try:
                enhanced_questions = await self._two_stage_precision_processing(
                    validated_questions, pdf_path, structure_analysis, upload_id
                )
                validated_questions = enhanced_questions
                
            except Exception as ve:
                print(f"âš ï¸ 2ë‹¨ê³„ ì •ë°€ ì²˜ë¦¬ ì˜¤ë¥˜: {ve}")
            
            return validated_questions
            
        except Exception as e:
            print(f"   âš ï¸ íŠ¹ìˆ˜ ìš”ì†Œ ê²€ì¦ ì˜¤ë¥˜: {e}")
            return questions
    
    async def _two_stage_precision_processing(
        self, 
        questions: List[Dict], 
        pdf_path: str, 
        structure_analysis: Dict,
        upload_id: int
    ) -> List[Dict]:
        """2ë‹¨ê³„ ì •ë°€ ì²˜ë¦¬ ì‹œìŠ¤í…œ - ì„ íƒì§€ ìˆ˜ì™€ ë³´ê¸° ì¢…ë¥˜ë³„ ì°¨ë³„í™” ì²˜ë¦¬"""
        
        try:
            print("ğŸ¯ 2ë‹¨ê³„ ì •ë°€ ì²˜ë¦¬ ì‹œì‘...")
            
            # 1ë‹¨ê³„: ë¬¸ì œ ë¶„ë¥˜ ë° ì²˜ë¦¬ ì „ëµ ê²°ì •
            categorized_questions = self._categorize_questions_by_complexity(questions, structure_analysis)
            
            # 2ë‹¨ê³„: ë¶„ë¥˜ë³„ ì°¨ë³„í™” ì²˜ë¦¬
            enhanced_questions = []
            
            for category, question_group in categorized_questions.items():
                if not question_group:
                    continue
                
                print(f"ğŸ“Š {category} ì²˜ë¦¬ ì¤‘... ({len(question_group)}ê°œ)")
                
                if category == "simple_text":
                    # ë‹¨ìˆœ í…ìŠ¤íŠ¸ ë¬¸ì œ - ë‚®ì€ í•´ìƒë„ë¡œ ë¹ ë¥¸ ì²˜ë¦¬
                    enhanced = await self._process_simple_text_questions(question_group, pdf_path)
                elif category == "with_tables":
                    # í‘œ í¬í•¨ ë¬¸ì œ - ê³ í•´ìƒë„ + í‘œ íŠ¹í™” ì²˜ë¦¬
                    enhanced = await self._process_table_questions(question_group, pdf_path, structure_analysis)
                elif category == "with_code":
                    # ì½”ë“œ í¬í•¨ ë¬¸ì œ - ì´ˆê³ í•´ìƒë„ + ì½”ë“œ íŠ¹í™” ì²˜ë¦¬
                    enhanced = await self._process_code_questions(question_group, pdf_path, structure_analysis)
                elif category == "with_diagrams":
                    # ë‹¤ì´ì–´ê·¸ë¨ í¬í•¨ ë¬¸ì œ - ìµœê³ í•´ìƒë„ + ì‹œê°ì  ë¶„ì„
                    enhanced = await self._process_diagram_questions(question_group, pdf_path, structure_analysis)
                elif category == "complex_choices":
                    # ë³µì¡í•œ ì„ íƒì§€ - ì„ íƒì§€ ìˆ˜ë³„ ë§ì¶¤ ì²˜ë¦¬
                    enhanced = await self._process_complex_choice_questions(question_group, pdf_path, structure_analysis)
                else:
                    enhanced = question_group
                
                enhanced_questions.extend(enhanced)
                
                # ì§„í–‰ë¥  ì—…ë°ì´íŠ¸ (í–¥í›„ êµ¬í˜„ ì˜ˆì •)
                # await self._update_progress(upload_id, f"2ë‹¨ê³„ ì²˜ë¦¬: {category} ì™„ë£Œ", 
                #                          len(enhanced_questions) / len(questions) * 100)
                print(f"ğŸ“Š 2ë‹¨ê³„ ì²˜ë¦¬: {category} ì™„ë£Œ ({len(enhanced_questions)}/{len(questions)})")
            
            print(f"âœ… 2ë‹¨ê³„ ì •ë°€ ì²˜ë¦¬ ì™„ë£Œ: {len(enhanced_questions)}ê°œ ë¬¸ì œ")
            return enhanced_questions
            
        except Exception as e:
            print(f"âŒ 2ë‹¨ê³„ ì •ë°€ ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
            return questions
    
    def _categorize_questions_by_complexity(self, questions: List[Dict], structure_analysis: Dict) -> Dict[str, List[Dict]]:
        """ë¬¸ì œë¥¼ ë³µì¡ë„ì™€ íŠ¹ìˆ˜ ìš”ì†Œë³„ë¡œ ë¶„ë¥˜"""
        
        categories = {
            "simple_text": [],           # ë‹¨ìˆœ í…ìŠ¤íŠ¸ ë¬¸ì œ (ì„ íƒì§€ 2-4ê°œ)
            "with_tables": [],           # í‘œ í¬í•¨ ë¬¸ì œ
            "with_code": [],             # ì½”ë“œ í¬í•¨ ë¬¸ì œ  
            "with_diagrams": [],         # ë‹¤ì´ì–´ê·¸ë¨/ì´ë¯¸ì§€ í¬í•¨ ë¬¸ì œ
            "complex_choices": []        # ë³µì¡í•œ ì„ íƒì§€ (5ê°œ ì´ìƒ)
        }
        
        # êµ¬ì¡° ë¶„ì„ì—ì„œ íŠ¹ìˆ˜ ìš”ì†Œ ì •ë³´ ì¶”ì¶œ
        special_elements = structure_analysis.get("detailed_analysis", {}).get("special_elements", {})
        
        # í‘œ í¬í•¨ ë¬¸ì œ ë²ˆí˜¸ë“¤
        table_questions = set()
        if "tables" in special_elements:
            for table_info in special_elements["tables"]:
                if "question" in table_info:
                    table_questions.add(table_info["question"])
        
        # ì½”ë“œ í¬í•¨ ë¬¸ì œ ë²ˆí˜¸ë“¤  
        code_questions = set()
        if "code_blocks" in special_elements:
            for code_info in special_elements["code_blocks"]:
                if "question" in code_info:
                    code_questions.add(code_info["question"])
        
        # ë‹¤ì´ì–´ê·¸ë¨ í¬í•¨ ë¬¸ì œ ë²ˆí˜¸ë“¤
        diagram_questions = set()
        visual_elements = special_elements.get("visual_elements_detailed", {})
        for element_type in ["diagram_in_choices", "graph_in_choices", "shape_in_choices", 
                           "diagram_in_passage", "graph_in_passage"]:
            if element_type in visual_elements:
                for visual_info in visual_elements[element_type]:
                    diagram_questions.add(visual_info.get("question", 0))
        
        # ë¬¸ì œë³„ ë¶„ë¥˜
        for question in questions:
            q_num = question.get("question_number", 0)
            options = question.get("options", [])
            option_count = len(options)
            
            if q_num in table_questions:
                categories["with_tables"].append(question)
            elif q_num in code_questions:
                categories["with_code"].append(question)
            elif q_num in diagram_questions:
                categories["with_diagrams"].append(question)
            elif option_count >= 5:
                categories["complex_choices"].append(question)
            else:
                categories["simple_text"].append(question)
        
        # ë¶„ë¥˜ ê²°ê³¼ ì¶œë ¥
        for category, items in categories.items():
            if items:
                q_nums = [q.get("question_number") for q in items]
                print(f"ğŸ“‹ {category}: {len(items)}ê°œ ë¬¸ì œ - {q_nums}")
        
        return categories
    
    async def _process_simple_text_questions(self, questions: List[Dict], pdf_path: str) -> List[Dict]:
        """ë‹¨ìˆœ í…ìŠ¤íŠ¸ ë¬¸ì œ ì²˜ë¦¬ (2x í•´ìƒë„)"""
        return questions  # ê¸°ì¡´ ì¶”ì¶œ ê²°ê³¼ ìœ ì§€
    
    async def _process_table_questions(self, questions: List[Dict], pdf_path: str, structure_analysis: Dict) -> List[Dict]:
        """í‘œ í¬í•¨ ë¬¸ì œ íŠ¹í™” ì²˜ë¦¬ (8x í•´ìƒë„)"""
        
        enhanced_questions = []
        
        for question in questions:
            try:
                q_num = question.get("question_number", 0)
                page_num = question.get("page_number", 1)
                
                print(f"ğŸ“Š ë¬¸ì œ {q_num}ë²ˆ í‘œ íŠ¹í™” ì²˜ë¦¬...")
                
                # 8ë°° ê³ í•´ìƒë„ë¡œ í‘œ ë°ì´í„° ì •ë°€ ì¶”ì¶œ
                doc = fitz.open(pdf_path)
                page = doc[page_num - 1]
                mat = fitz.Matrix(8, 8)  # í‘œ ì²˜ë¦¬ìš© 8ë°° í•´ìƒë„
                pix = page.get_pixmap(matrix=mat)
                img_data = pix.tobytes("png")
                doc.close()
                
                # í‘œ íŠ¹í™” í”„ë¡¬í”„íŠ¸
                table_prompt = f"""ğŸ“Š í‘œ ë°ì´í„° ì •ë°€ ì¶”ì¶œ (ë¬¸ì œ {q_num}ë²ˆ)

ğŸ¯ **í‘œ ì²˜ë¦¬ ì „ìš© ëª¨ë“œ**:
- í‘œì˜ ëª¨ë“  ì…€ ë°ì´í„°ë¥¼ ì •í™•íˆ ì¶”ì¶œ
- ìˆ«ìëŠ” ì†Œìˆ˜ì ê¹Œì§€ ì™„ë²½í•˜ê²Œ ë³´ì¡´
- í—¤ë”ì™€ ë°ì´í„° í–‰ì„ êµ¬ë¶„í•˜ì—¬ ì¶”ì¶œ
- í‘œ êµ¬ì¡°ë¥¼ ì™„ì „íˆ ë³´ì¡´

ğŸ“‹ **ì¶”ì¶œ ë°©ë²•**:
1. í‘œ í—¤ë” ì‹ë³„ ë° ì¶”ì¶œ
2. ê° ë°ì´í„° í–‰ì˜ ëª¨ë“  ì…€ ê°’ ì¶”ì¶œ
3. P1, P2, P3 ë“±ì˜ í”„ë¡œì„¸ìŠ¤ ë°ì´í„° ì™„ì „ ì¶”ì¶œ
4. ê³„ì‚° ê´€ë ¨ ìˆ«ì (ë„ì°©ì‹œê°„, ì‹¤í–‰ì‹œê°„ ë“±) ì •í™• ì¶”ì¶œ

ğŸ”¢ **ìˆ«ì ì •í™•ì„±**:
- 8.2 â†’ 8.2 (ì†Œìˆ˜ì  ë³´ì¡´)
- 9.4 â†’ 9.4 (ì†Œìˆ˜ì  ë³´ì¡´)
- ì •ìˆ˜ëŠ” ì •ìˆ˜ë¡œ, ì†Œìˆ˜ëŠ” ì†Œìˆ˜ë¡œ ì •í™•íˆ

ğŸš¨ **ì„ íƒì§€ ì˜¤ì¸ì‹ ë°©ì§€**:
- â‘ â‘¡â‘¢â‘£â‘¤ ê°™ì€ ë™ê·¸ë¼ë¯¸ ìˆ«ìëŠ” ì„ íƒì§€ì…ë‹ˆë‹¤! ë¬¸ì œ ë²ˆí˜¸ê°€ ì•„ë‹™ë‹ˆë‹¤!
- 1)2)3)4)5) ê°™ì€ ê´„í˜¸ ìˆ«ìë„ ì„ íƒì§€ì…ë‹ˆë‹¤! ë¬¸ì œ ë²ˆí˜¸ê°€ ì•„ë‹™ë‹ˆë‹¤!
- ë¬¸ì œ {q_num}ë²ˆ í•˜ë‚˜ë§Œ ì¶”ì¶œí•˜ì„¸ìš”! ì„ íƒì§€ë¥¼ ë³„ê°œ ë¬¸ì œë¡œ ë§Œë“¤ì§€ ë§ˆì„¸ìš”!

JSON í˜•ì‹ìœ¼ë¡œ ë¬¸ì œì™€ ì„ íƒì§€ë¥¼ ì •í™•íˆ ì¶”ì¶œí•˜ì„¸ìš”."""

                messages = [
                    {"role": "system", "content": "í‘œ ë°ì´í„° ì •ë°€ ì¶”ì¶œ ì „ë¬¸ê°€. ìˆ«ìì™€ í‘œ êµ¬ì¡°ë¥¼ ì™„ë²½í•˜ê²Œ ë³´ì¡´."},
                    {
                        "role": "user",
                        "content": [
                            {"type": "text", "text": table_prompt},
                            {
                                "type": "image_url",
                                "image_url": {
                                    "url": f"data:image/png;base64,{base64.b64encode(img_data).decode()}",
                                    "detail": "high"
                                }
                            }
                        ]
                    }
                ]
                
                response = await self.openai_client.chat.completions.create(
                    model="gpt-4o",
                    messages=messages,
                    max_tokens=4000,
                    temperature=0.0
                )
                
                # ì‘ë‹µ ì²˜ë¦¬ ë° ê¸°ì¡´ ë¬¸ì œ ì •ë³´ ì—…ë°ì´íŠ¸
                enhanced_question = self._enhance_question_with_table_data(
                    question, response.choices[0].message.content
                )
                enhanced_questions.append(enhanced_question)
                
            except Exception as e:
                print(f"âš ï¸ ë¬¸ì œ {q_num}ë²ˆ í‘œ ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
                enhanced_questions.append(question)
        
        return enhanced_questions
    
    async def _process_code_questions(self, questions: List[Dict], pdf_path: str, structure_analysis: Dict) -> List[Dict]:
        """ì½”ë“œ í¬í•¨ ë¬¸ì œ íŠ¹í™” ì²˜ë¦¬ (16x í•´ìƒë„)"""
        
        enhanced_questions = []
        
        for question in questions:
            try:
                q_num = question.get("question_number", 0)
                page_num = question.get("page_number", 1)
                
                print(f"ğŸ’» ë¬¸ì œ {q_num}ë²ˆ ì½”ë“œ íŠ¹í™” ì²˜ë¦¬...")
                
                # 16ë°° ì´ˆê³ í•´ìƒë„ë¡œ ì½”ë“œ ì •ë°€ ì¶”ì¶œ
                doc = fitz.open(pdf_path)
                page = doc[page_num - 1]
                mat = fitz.Matrix(16, 16)  # ì½”ë“œ ì²˜ë¦¬ìš© 16ë°° í•´ìƒë„
                pix = page.get_pixmap(matrix=mat)
                img_data = pix.tobytes("png")
                doc.close()
                
                # ì½”ë“œ íŠ¹í™” í”„ë¡¬í”„íŠ¸
                code_prompt = f"""ğŸ’» ì½”ë“œ ì •ë°€ ì¶”ì¶œ (ë¬¸ì œ {q_num}ë²ˆ)

ğŸ¯ **ì½”ë“œ ì²˜ë¦¬ ì „ìš© ëª¨ë“œ**:
- ì½”ë“œì˜ ëª¨ë“  ë¬¸ë²• ìš”ì†Œë¥¼ ì •í™•íˆ ì¶”ì¶œ
- ë³€ìˆ˜ëª…, í•¨ìˆ˜ëª…ì„ ì •í™•íˆ ë³´ì¡´
- ë“¤ì—¬ì“°ê¸°ì™€ ê³µë°±ì„ ì™„ë²½í•˜ê²Œ ìœ ì§€
- ì„¸ë¯¸ì½œë¡ , ê´„í˜¸, ì—°ì‚°ì ì •í™•íˆ ì¶”ì¶œ

ğŸ“‹ **ì¶”ì¶œ ë°©ë²•**:
1. ì½”ë“œ ë¸”ë¡ ì „ì²´ë¥¼ ì •í™•íˆ ì‹ë³„
2. ê° ë¼ì¸ì˜ ë“¤ì—¬ì“°ê¸° ë³´ì¡´
3. ë³€ìˆ˜ëª…ê³¼ ê°’ì„ ì •í™•íˆ ì¶”ì¶œ
4. ì‹¤í–‰ ê²°ê³¼ë‚˜ ì¶œë ¥ê°’ ì •í™•íˆ ì¶”ì¶œ

ğŸ”¤ **ë¬¸ë²• ì •í™•ì„±**:
- ë³€ìˆ˜ëª…: i, j, sum, result ë“± ì •í™•íˆ
- ì—°ì‚°ì: ++, --, +=, == ë“± ì •í™•íˆ  
- ê°’: ë¬¸ìì—´, ìˆ«ì, ë¶ˆë¦° ë“± ì •í™•íˆ

ğŸš¨ **ì„ íƒì§€ ì˜¤ì¸ì‹ ë°©ì§€**:
- â‘ â‘¡â‘¢â‘£â‘¤ ê°™ì€ ë™ê·¸ë¼ë¯¸ ìˆ«ìëŠ” ì„ íƒì§€ì…ë‹ˆë‹¤! ë¬¸ì œ ë²ˆí˜¸ê°€ ì•„ë‹™ë‹ˆë‹¤!
- 1)2)3)4)5) ê°™ì€ ê´„í˜¸ ìˆ«ìë„ ì„ íƒì§€ì…ë‹ˆë‹¤! ë¬¸ì œ ë²ˆí˜¸ê°€ ì•„ë‹™ë‹ˆë‹¤!
- ë¬¸ì œ {q_num}ë²ˆ í•˜ë‚˜ë§Œ ì¶”ì¶œí•˜ì„¸ìš”! ì„ íƒì§€ë¥¼ ë³„ê°œ ë¬¸ì œë¡œ ë§Œë“¤ì§€ ë§ˆì„¸ìš”!

JSON í˜•ì‹ìœ¼ë¡œ ë¬¸ì œì™€ ì„ íƒì§€ë¥¼ ì •í™•íˆ ì¶”ì¶œí•˜ì„¸ìš”."""

                messages = [
                    {"role": "system", "content": "ì½”ë“œ ì •ë°€ ì¶”ì¶œ ì „ë¬¸ê°€. ë¬¸ë²•ê³¼ êµ¬ì¡°ë¥¼ ì™„ë²½í•˜ê²Œ ë³´ì¡´."},
                    {
                        "role": "user",
                        "content": [
                            {"type": "text", "text": code_prompt},
                            {
                                "type": "image_url",
                                "image_url": {
                                    "url": f"data:image/png;base64,{base64.b64encode(img_data).decode()}",
                                    "detail": "high"
                                }
                            }
                        ]
                    }
                ]
                
                response = await self.openai_client.chat.completions.create(
                    model="gpt-4o",
                    messages=messages,
                    max_tokens=4000,
                    temperature=0.0
                )
                
                # ì‘ë‹µ ì²˜ë¦¬ ë° ê¸°ì¡´ ë¬¸ì œ ì •ë³´ ì—…ë°ì´íŠ¸
                enhanced_question = self._enhance_question_with_code_data(
                    question, response.choices[0].message.content
                )
                enhanced_questions.append(enhanced_question)
                
            except Exception as e:
                print(f"âš ï¸ ë¬¸ì œ {q_num}ë²ˆ ì½”ë“œ ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
                enhanced_questions.append(question)
        
        return enhanced_questions
    
    async def _process_diagram_questions(self, questions: List[Dict], pdf_path: str, structure_analysis: Dict) -> List[Dict]:
        """ë‹¤ì´ì–´ê·¸ë¨ í¬í•¨ ë¬¸ì œ íŠ¹í™” ì²˜ë¦¬ (ìµœê³  í•´ìƒë„)"""
        
        enhanced_questions = []
        
        for question in questions:
            try:
                q_num = question.get("question_number", 0)
                page_num = question.get("page_number", 1)
                
                print(f"ğŸ¨ ë¬¸ì œ {q_num}ë²ˆ ë‹¤ì´ì–´ê·¸ë¨ íŠ¹í™” ì²˜ë¦¬...")
                
                # ìµœê³  í•´ìƒë„ë¡œ ì‹œê°ì  ìš”ì†Œ ì •ë°€ ì¶”ì¶œ
                doc = fitz.open(pdf_path)
                page = doc[page_num - 1]
                mat = fitz.Matrix(20, 20)  # ë‹¤ì´ì–´ê·¸ë¨ ì²˜ë¦¬ìš© ìµœê³  í•´ìƒë„
                pix = page.get_pixmap(matrix=mat)
                img_data = pix.tobytes("png")
                doc.close()
                
                # ë‹¤ì´ì–´ê·¸ë¨ íŠ¹í™” í”„ë¡¬í”„íŠ¸
                diagram_prompt = f"""ğŸ¨ ë‹¤ì´ì–´ê·¸ë¨/ì‹œê°ì  ìš”ì†Œ ì •ë°€ ë¶„ì„ (ë¬¸ì œ {q_num}ë²ˆ)

ğŸ¯ **ì‹œê°ì  ë¶„ì„ ì „ìš© ëª¨ë“œ**:
- ë‹¤ì´ì–´ê·¸ë¨ì˜ ëª¨ë“  êµ¬ì„± ìš”ì†Œ ì‹ë³„
- í™”ì‚´í‘œ, ì—°ê²°ì„ , ë„í˜•ì˜ ê´€ê³„ íŒŒì•…
- í…ìŠ¤íŠ¸ ë¼ë²¨ê³¼ ìˆ«ì ì •í™•íˆ ì¶”ì¶œ
- ê·¸ë˜í”„ì˜ ì¶•, ë²”ë¡€, ë°ì´í„° í¬ì¸íŠ¸ ì¶”ì¶œ

ğŸ“‹ **ë¶„ì„ ë°©ë²•**:
1. ì‹œê°ì  ìš”ì†Œì˜ ì „ì²´ êµ¬ì¡° íŒŒì•…
2. ê° ìš”ì†Œê°„ì˜ ì—°ê²° ê´€ê³„ ë¶„ì„
3. í…ìŠ¤íŠ¸ì™€ ìˆ«ì ë¼ë²¨ ì •í™• ì¶”ì¶œ
4. ì„ íƒì§€ì— í¬í•¨ëœ ì‹œê°ì  ì°¨ì´ì  ì‹ë³„

ğŸ” **ì •í™•ì„± í¬ì¸íŠ¸**:
- í™”ì‚´í‘œ ë°©í–¥ê³¼ ì—°ê²°ì 
- ë„í˜•ì˜ ëª¨ì–‘ê³¼ í¬ê¸° ê´€ê³„
- ê·¸ë˜í”„ ë°ì´í„°ì˜ ìˆ˜ì¹˜ê°’
- ë„¤íŠ¸ì›Œí¬ êµ¬ì¡°ì˜ ì—°ê²° ìƒíƒœ

ğŸš¨ **ì„ íƒì§€ ì˜¤ì¸ì‹ ë°©ì§€**:
- â‘ â‘¡â‘¢â‘£â‘¤ ê°™ì€ ë™ê·¸ë¼ë¯¸ ìˆ«ìëŠ” ì„ íƒì§€ì…ë‹ˆë‹¤! ë¬¸ì œ ë²ˆí˜¸ê°€ ì•„ë‹™ë‹ˆë‹¤!
- 1)2)3)4)5) ê°™ì€ ê´„í˜¸ ìˆ«ìë„ ì„ íƒì§€ì…ë‹ˆë‹¤! ë¬¸ì œ ë²ˆí˜¸ê°€ ì•„ë‹™ë‹ˆë‹¤!
- ë¬¸ì œ {q_num}ë²ˆ í•˜ë‚˜ë§Œ ì¶”ì¶œí•˜ì„¸ìš”! ì„ íƒì§€ë¥¼ ë³„ê°œ ë¬¸ì œë¡œ ë§Œë“¤ì§€ ë§ˆì„¸ìš”!

JSON í˜•ì‹ìœ¼ë¡œ ë¬¸ì œì™€ ì„ íƒì§€ë¥¼ ì •í™•íˆ ì¶”ì¶œí•˜ì„¸ìš”."""

                messages = [
                    {"role": "system", "content": "ì‹œê°ì  ìš”ì†Œ ë¶„ì„ ì „ë¬¸ê°€. ë‹¤ì´ì–´ê·¸ë¨ê³¼ ê·¸ë˜í”„ë¥¼ ì •í™•íˆ í•´ì„."},
                    {
                        "role": "user",
                        "content": [
                            {"type": "text", "text": diagram_prompt},
                            {
                                "type": "image_url",
                                "image_url": {
                                    "url": f"data:image/png;base64,{base64.b64encode(img_data).decode()}",
                                    "detail": "high"
                                }
                            }
                        ]
                    }
                ]
                
                response = await self.openai_client.chat.completions.create(
                    model="gpt-4o",
                    messages=messages,
                    max_tokens=4000,
                    temperature=0.0
                )
                
                # ì‘ë‹µ ì²˜ë¦¬ ë° ê¸°ì¡´ ë¬¸ì œ ì •ë³´ ì—…ë°ì´íŠ¸
                enhanced_question = self._enhance_question_with_diagram_data(
                    question, response.choices[0].message.content
                )
                enhanced_questions.append(enhanced_question)
                
            except Exception as e:
                print(f"âš ï¸ ë¬¸ì œ {q_num}ë²ˆ ë‹¤ì´ì–´ê·¸ë¨ ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
                enhanced_questions.append(question)
        
        return enhanced_questions
    
    async def _process_complex_choice_questions(self, questions: List[Dict], pdf_path: str, structure_analysis: Dict) -> List[Dict]:
        """ë³µì¡í•œ ì„ íƒì§€ ë¬¸ì œ ì²˜ë¦¬ (ì„ íƒì§€ 5ê°œ ì´ìƒ)"""
        
        enhanced_questions = []
        
        for question in questions:
            try:
                q_num = question.get("question_number", 0)
                page_num = question.get("page_number", 1)
                option_count = len(question.get("options", []))
                
                print(f"ğŸ¯ ë¬¸ì œ {q_num}ë²ˆ ë³µì¡ ì„ íƒì§€ ì²˜ë¦¬ ({option_count}ê°œ)...")
                
                # ì„ íƒì§€ ìˆ˜ì— ë”°ë¥¸ í•´ìƒë„ ì¡°ì •
                resolution_scale = min(12, 8 + option_count)  # ìµœëŒ€ 12ë°°
                
                doc = fitz.open(pdf_path)
                page = doc[page_num - 1]
                mat = fitz.Matrix(resolution_scale, resolution_scale)
                pix = page.get_pixmap(matrix=mat)
                img_data = pix.tobytes("png")
                doc.close()
                
                # 2ë‹¨ê³„ ì •ë°€ ì²˜ë¦¬ íŠ¹í™” í”„ë¡¬í”„íŠ¸ (ê°•í™”ë¨)
                complex_prompt = f"""ğŸ¯ 2ë‹¨ê³„ ì •ë°€ ì„ íƒì§€ ì¶”ì¶œ (ë¬¸ì œ {q_num}ë²ˆ)

ğŸš¨ **1ì°¨ ì²˜ë¦¬ì—ì„œ ë†“ì¹œ ë¬¸ì œ ì •í™• ë³´ì™„**:
- ì„ íƒì§€ ê°œìˆ˜: {option_count}ê°œ (ì˜ˆìƒ)
- í•´ìƒë„: {resolution_scale}ë°° (ì´ˆê³ í•´ìƒë„)
- ëª©í‘œ: 100% ì •í™•í•œ ì„ íƒì§€ ì¶”ì¶œ

ğŸ” **ì •ë°€ ì¶”ì¶œ ë‹¨ê³„**:
1ï¸âƒ£ ë¨¼ì € ë¬¸ì œ {q_num}ë²ˆì„ ì°¾ìœ¼ì„¸ìš”
2ï¸âƒ£ ë¬¸ì œ ì§€ë¬¸ê³¼ ì„ íƒì§€ë¥¼ ëª…í™•íˆ êµ¬ë¶„í•˜ì„¸ìš”
3ï¸âƒ£ ëª¨ë“  ì„ íƒì§€ë¥¼ ë¹ ëœ¨ë¦¬ì§€ ë§ê³  ì¶”ì¶œí•˜ì„¸ìš”
4ï¸âƒ£ ë³´ê¸°/í‘œ/ì½”ë“œê°€ ìˆë‹¤ë©´ ë”°ë¡œ ì¶”ì¶œí•˜ì„¸ìš”

ğŸ“‹ **ì„ íƒì§€ ë§ˆì»¤ ì™„ì „ ê°€ì´ë“œ**:
âœ… **í•œêµ­ì–´ ì‹œí—˜ í‘œì¤€**: â‘ , â‘¡, â‘¢, â‘£, â‘¤
âœ… **ì¼ë°˜ í˜•ì‹**: 1), 2), 3), 4), 5)
âœ… **ì˜ì–´ í˜•ì‹**: A), B), C), D), E)
âš ï¸ **ì£¼ì˜**: ì (.) ëŒ€ì‹  ê´„í˜¸()) ì‚¬ìš©í•˜ëŠ” ê²½ìš°ë„ ìˆìŒ

ğŸ”¢ **ìˆ«ì ì„ íƒì§€ ì´ˆì •ë°€ ì²˜ë¦¬** (ë§¤ìš° ì¤‘ìš”!):
ì˜ˆì‹œ 1: â‘  7.2    â†’ "â‘  7.2" (ì†Œìˆ˜ì  ë³´ì¡´)
ì˜ˆì‹œ 2: â‘¡ 15ê°œ   â†’ "â‘¡ 15ê°œ" (ë‹¨ìœ„ í¬í•¨)
ì˜ˆì‹œ 3: â‘¢ x=10   â†’ "â‘¢ x=10" (ìˆ˜ì‹ ì™„ì „ ë³´ì¡´)
ì˜ˆì‹œ 4: â‘£ 80%    â†’ "â‘£ 80%" (í¼ì„¼íŠ¸ ë³´ì¡´)
ğŸš¨ **ì ˆëŒ€ ê¸ˆì§€**: 7.2 â†’ 7, 15ê°œ â†’ 15, 80% â†’ 80 ê°™ì€ ë³€ê²½

ğŸ“– **ë³´ê¸°/ì§€ë¬¸/í‘œ ì •ë°€ ì¶”ì¶œ**:
ğŸ” **ì°¾ì„ ê²ƒë“¤**:
- <ë³´ê¸°> ë˜ëŠ” [ë³´ê¸°] ë§ˆí¬
- í‘œ (| ê¸°í˜¸ë¡œ êµ¬ì„±ëœ í‘œ)
- ì½”ë“œ ë¸”ë¡ (public, class, for, while ë“±)
- ë‹¤ì´ì–´ê·¸ë¨/ê·¸ë¦¼ ì„¤ëª…
- ìˆ˜ì‹ ì§‘í•©
â­ **ë³´ê¸° ìœ„ì¹˜**: ë³´í†µ ë¬¸ì œ ì§€ë¬¸ ì• ë˜ëŠ” ì¤‘ê°„ì— ìˆìŒ
â­ **ë³´ê¸° ê¸¸ì´**: ë³´í†µ 2ì¤„ ì´ìƒì˜ ê¸´ ë‚´ìš©

ğŸ’¡ **ì„ íƒì§€ vs ë¬¸ì œì§€ë¬¸ êµ¬ë¶„ë²•**:
- ë¬¸ì œ ì§€ë¬¸: ê¸¸ê³  ì„¤ëª…ì , ì§ˆë¬¸ìœ¼ë¡œ ëë‚¨
- ì„ íƒì§€: ì§§ê³  ë‹µë³€ í˜•íƒœ, â‘ â‘¡â‘¢â‘£ ë§ˆì»¤ë¡œ ì‹œì‘
- ë³´ê¸°: í‘œ/ì½”ë“œ/ë°ì´í„°, ë¬¸ì œì™€ ì„ íƒì§€ ì‚¬ì´

ğŸš¨ **2ë‹¨ê³„ ì²˜ë¦¬ íŠ¹ë³„ ì§€ì¹¨**:
âœ… 1ì°¨ì—ì„œ ë†“ì¹œ ì„ íƒì§€ ëª¨ë‘ ì°¾ê¸°
âœ… ìˆ«ì ì„ íƒì§€ ì›ë³¸ ê·¸ëŒ€ë¡œ ë³´ì¡´
âœ… ë³´ê¸° ë‚´ìš© ì™„ì „ ì¶”ì¶œ
âŒ ì„ íƒì§€ë¥¼ ë¬¸ì œ í…ìŠ¤íŠ¸ì— í˜¼í•© ê¸ˆì§€
âŒ ë³´ê¸°ë¥¼ ì„ íƒì§€ë¡œ ì°©ê° ê¸ˆì§€
âŒ ìˆ«ì/ê¸°í˜¸ ì„ì˜ ë³€ê²½ ê¸ˆì§€

ğŸ“Š **ì¶œë ¥ JSON í˜•ì‹** (ë°˜ë“œì‹œ ì´ êµ¬ì¡°ë¡œ):
```json
{{
  "question_number": {q_num},
  "question_text": "ìˆœìˆ˜í•œ ë¬¸ì œ ì§€ë¬¸ë§Œ (ì„ íƒì§€/ë³´ê¸° ì œì™¸)",
  "passage": "ë³´ê¸°/í‘œ/ì½”ë“œ/ë‹¤ì´ì–´ê·¸ë¨ (ì—†ìœ¼ë©´ \"\")",
  "options": ["â‘  ì™„ì „í•œì„ íƒì§€1", "â‘¡ ì™„ì „í•œì„ íƒì§€2", "â‘¢ ì™„ì „í•œì„ íƒì§€3", "â‘£ ì™„ì „í•œì„ íƒì§€4"],
  "page_number": {page_num},
  "extraction_confidence": "high",
  "processing_notes": "2ë‹¨ê³„ ì •ë°€ ì¶”ì¶œ ì™„ë£Œ"
}}
```

âš¡ **ì„±ê³µ ê¸°ì¤€**:
- ëª¨ë“  ì„ íƒì§€ ë§ˆì»¤ ì •í™• ì¸ì‹
- ìˆ«ì ì„ íƒì§€ ì›ë³¸ ì™„ì „ ë³´ì¡´
- ë³´ê¸°ì™€ ì„ íƒì§€ ì™„ë²½ ë¶„ë¦¬
- ë¬¸ì œ í…ìŠ¤íŠ¸ì—ì„œ ì„ íƒì§€ ì™„ì „ ì œê±°"""

                messages = [
                    {"role": "system", "content": "ì„ íƒì§€ ì •ë°€ ì¶”ì¶œ ì „ë¬¸ê°€. ì‹¤ì œ ì„ íƒì§€ ê°œìˆ˜ë¥¼ ì •í™•íˆ íŒŒì•…í•˜ê³  ìˆ«ì/ë³´ê¸° ë‚´ìš©ì„ ì™„ë²½ ë³´ì¡´."},
                    {
                        "role": "user",
                        "content": [
                            {"type": "text", "text": complex_prompt},
                            {
                                "type": "image_url",
                                "image_url": {
                                    "url": f"data:image/png;base64,{base64.b64encode(img_data).decode()}",
                                    "detail": "high"
                                }
                            }
                        ]
                    }
                ]
                
                response = await self.openai_client.chat.completions.create(
                    model="gpt-4o",
                    messages=messages,
                    max_tokens=5000,
                    temperature=0.0
                )
                
                # ì‘ë‹µ ì²˜ë¦¬ ë° ê¸°ì¡´ ë¬¸ì œ ì •ë³´ ì—…ë°ì´íŠ¸
                enhanced_question = self._enhance_question_with_complex_choices(
                    question, response.choices[0].message.content, option_count
                )
                enhanced_questions.append(enhanced_question)
                
            except Exception as e:
                print(f"âš ï¸ ë¬¸ì œ {q_num}ë²ˆ ë³µì¡ ì„ íƒì§€ ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
                enhanced_questions.append(question)
        
        return enhanced_questions

    async def _re_extract_with_template(self, question_template: Dict, pdf_path: str, page_num: int, upload_id: int) -> Optional[Dict]:
        """í…œí”Œë¦¿ ê¸°ë°˜ ì •ë°€ ì¬ì¶”ì¶œ"""
        
        try:
            q_num = question_template.get('question_number')
            extraction_template = question_template.get('extraction_template', {})
            
            print(f"   ğŸ¯ ë¬¸ì œ {q_num}ë²ˆ í…œí”Œë¦¿ ê¸°ë°˜ ì¬ì¶”ì¶œ")
            
            # í•´ë‹¹ í˜ì´ì§€ ê³ í•´ìƒë„ ì´ë¯¸ì§€ ìƒì„±
            doc = fitz.open(pdf_path)
            page = doc[page_num - 1]
            mat = fitz.Matrix(14, 14)  # 14ë°° í•´ìƒë„
            pix = page.get_pixmap(matrix=mat)
            img_data = self._optimize_image_size(pix)
            doc.close()
            
            # í…œí”Œë¦¿ ê¸°ë°˜ ì¶”ì¶œ í”„ë¡¬í”„íŠ¸
            template_prompt = f"""ğŸ¯ ë¬¸ì œ {q_num}ë²ˆ í…œí”Œë¦¿ ê¸°ë°˜ ì •ë°€ ì¶”ì¶œ

ğŸ“‹ **ì •í™•í•œ ì¶”ì¶œ í…œí”Œë¦¿**:
- ë¬¸ì œ ì‹œì‘ ë§ˆì»¤: "{extraction_template.get('question_start_marker', '')}"
- ë¬¸ì œ ë ë§ˆì»¤: "{extraction_template.get('question_end_marker', '')}"  
- ì„ íƒì§€ ì‹œì‘: "{extraction_template.get('choice_start_marker', 'â‘ ')}"
- ì„ íƒì§€ ê°œìˆ˜: {question_template.get('choice_count', 4)}ê°œ

ğŸ¯ **íŠ¹ìˆ˜ ìš”ì†Œ ì¶”ì¶œ**:"""
            
            if extraction_template.get('table_extraction_needed'):
                table_format = extraction_template.get('table_format', 'markdown')
                template_prompt += f"""
- ğŸ”¸ í‘œ ì¶”ì¶œ í•„ìˆ˜: {table_format} í˜•ì‹ìœ¼ë¡œ
- ğŸ”¸ ëª¨ë“  í–‰ê³¼ ì—´ ì™„ì „íˆ ì¶”ì¶œ"""
            
            if question_template.get('special_elements', {}).get('has_code'):
                template_prompt += f"""
- ğŸ”¸ ì½”ë“œ ë¸”ë¡ ì™„ì „íˆ ì¶”ì¶œ
- ğŸ”¸ ë“¤ì—¬ì“°ê¸°ì™€ êµ¬ë¬¸ ì •í™•íˆ ë³´ì¡´"""
            
            if question_template.get('special_elements', {}).get('has_diagram'):
                template_prompt += f"""
- ğŸ”¸ ë‹¤ì´ì–´ê·¸ë¨ ëª¨ë“  êµ¬ì„±ìš”ì†Œ ì„¤ëª…
- ğŸ”¸ ì—°ê²°ê´€ê³„ì™€ êµ¬ì¡° ìƒì„¸íˆ ê¸°ìˆ """
            
            template_prompt += f"""

ğŸ“Š **JSON ì¶œë ¥**:
```json
{{
  "question_number": {q_num},
  "question_text": "ì§€ë¬¸ë§Œ (ì„ íƒì§€ ì œì™¸)",
  "options": ["â‘  ì„ íƒì§€1", "â‘¡ ì„ íƒì§€2", "â‘¢ ì„ íƒì§€3", "â‘£ ì„ íƒì§€4"],
  "has_table": true/false,
  "table_data": "ë§ˆí¬ë‹¤ìš´ í‘œ í˜•ì‹",
  "has_code": true/false,
  "code_content": "ì½”ë“œ ë‚´ìš©",
  "has_diagram": true/false,
  "diagram_description": "ë‹¤ì´ì–´ê·¸ë¨ ìƒì„¸ ì„¤ëª…"
}}
```"""
            
            messages = [
                {
                    "role": "system",
                    "content": "í…œí”Œë¦¿ ê¸°ë°˜ ì •ë°€ ë¬¸ì œ ì¶”ì¶œ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ì œê³µëœ í…œí”Œë¦¿ì„ ì •í™•íˆ ë”°ë¼ ì¶”ì¶œí•˜ì„¸ìš”."
                },
                {
                    "role": "user",
                    "content": [
                        {"type": "text", "text": template_prompt},
                        {
                            "type": "image_url",
                            "image_url": {
                                "url": f"data:image/png;base64,{base64.b64encode(img_data).decode()}",
                                "detail": "high"
                            }
                        }
                    ]
                }
            ]
            
            response = await self.openai_client.chat.completions.create(
                model="gpt-4o",
                messages=messages,
                max_tokens=4000,
                temperature=0.1
            )
            
            response_text = response.choices[0].message.content
            
            # JSON íŒŒì‹±
            try:
                json_match = re.search(r'```json\s*(\{.*?\})\s*```', response_text, re.DOTALL)
                if json_match:
                    result = json.loads(json_match.group(1))
                    # ì„ íƒì§€ ë³µêµ¬ ì ìš©
                    result = self._fix_embedded_options(result)
                    return result
            except:
                pass
            
            return None
            
        except Exception as e:
            print(f"   âš ï¸ í…œí”Œë¦¿ ê¸°ë°˜ ì¬ì¶”ì¶œ ì‹¤íŒ¨: {e}")
            return None

    async def _re_extract_special_elements(self, missing_item: Dict, pdf_path: str, upload_id: int) -> Optional[Dict]:
        """ğŸ”§ **ê°•í™”ëœ íŠ¹ìˆ˜ ìš”ì†Œ ì¬ì¶”ì¶œ** - ì›ë³¸ ë³´ì¡´ ìš°ì„ """
        
        try:
            question_number = missing_item['question_number']
            special_info = missing_item['special_info']
            current_question = missing_item['current_question']
            missing_elements = missing_item['missing_elements']
            
            print(f"   ğŸ¯ ë¬¸ì œ {question_number}ë²ˆ íŠ¹ìˆ˜ ìš”ì†Œ ì§‘ì¤‘ ì¬ì¶”ì¶œ: {missing_elements}")
            
            # í•´ë‹¹ ë¬¸ì œê°€ ìˆëŠ” í˜ì´ì§€ íŠ¹ì •
            page_number = special_info.get('page_location', current_question.get('page_number', 1))
            missing_elements = missing_item['missing_elements']
            page_location = special_info.get('page_location', 1)
            
            # í•´ë‹¹ í˜ì´ì§€ ê³ í•´ìƒë„ ì´ë¯¸ì§€ ìƒì„±
            doc = fitz.open(pdf_path)
            page = doc.load_page(page_location - 1)  # 0-based index
            
            # ë†’ì€ í•´ìƒë„ë¡œ íŠ¹ìˆ˜ ìš”ì†Œ ì§‘ì¤‘ ë¶„ì„ (API ì œí•œ ê³ ë ¤)
            mat = fitz.Matrix(14.0, 14.0)  # 14ë°° í•´ìƒë„
            pix = page.get_pixmap(matrix=mat)
            img_data = self._optimize_image_size(pix)
            
            import base64
            img_b64 = base64.b64encode(img_data).decode('utf-8')
            doc.close()
            
            # íŠ¹ìˆ˜ ìš”ì†Œ ì§‘ì¤‘ ì¶”ì¶œ í”„ë¡¬í”„íŠ¸
            special_prompt = f"""ğŸ” ë¬¸ì œ {question_number}ë²ˆ íŠ¹ìˆ˜ ìš”ì†Œ ì§‘ì¤‘ ì¶”ì¶œ

ğŸ¯ **ëª©í‘œ**: ë¬¸ì œ {question_number}ë²ˆì—ì„œ ëˆ„ë½ëœ íŠ¹ìˆ˜ ìš”ì†Œ ì¶”ì¶œ
âš ï¸ **ëˆ„ë½ëœ ìš”ì†Œ**: {', '.join(missing_elements)}

ğŸ“‹ **í˜„ì¬ ë¬¸ì œ**: {current_question.get('question_text', '')}

ğŸ” **ì§‘ì¤‘ ì¶”ì¶œ ìš”ì²­**:"""
            
            if 'table' in missing_elements:
                special_prompt += """
- ğŸ“Š **í‘œ ë°ì´í„°**: ëª¨ë“  í–‰ê³¼ ì—´, í—¤ë”, ë°ì´í„°ë¥¼ ì™„ì „íˆ ì¶”ì¶œ
  â†’ ë°˜ë“œì‹œ ë§ˆí¬ë‹¤ìš´ í‘œ í˜•ì‹ìœ¼ë¡œ ì¶”ì¶œí•˜ì„¸ìš”
  â†’ ì˜ˆì‹œ: | í—¤ë”1 | í—¤ë”2 | í—¤ë”3 |
          |------|------|------|
          | ë°ì´í„° 1 | ë°ì´í„° 2 | ë°ì´í„° 3 |
  â†’ ëª¨ë“  í–‰ê³¼ ì—´ì„ ì™„ì „íˆ ë³´ì¡´í•˜ì„¸ìš”"""
            
            if 'diagram' in missing_elements:
                special_prompt += """
- ğŸ“ˆ **ë‹¤ì´ì–´ê·¸ë¨**: êµ¬ì¡°, ì—°ê²° ê´€ê³„, ë…¸ë“œ, ë¼ë²¨ì„ ìƒì„¸íˆ í…ìŠ¤íŠ¸ë¡œ ì„¤ëª…
  â†’ ëª¨ë“  ë…¸ë“œì™€ ê·¸ ë¼ë²¨ì„ ë‚˜ì—´í•˜ì„¸ìš”
  â†’ ë…¸ë“œ ê°„ ì—°ê²° ë°©í–¥ê³¼ ê´€ê³„(í™”ì‚´í‘œ, ì„ )ë¥¼ ëª…ì‹œí•˜ì„¸ìš”
  â†’ ë‹¤ì´ì–´ê·¸ë¨ì˜ ì „ì²´ì ì¸ ë°°ì¹˜ì™€ ê³„ì¸µ êµ¬ì¡°ë¥¼ ì„¤ëª…í•˜ì„¸ìš”
  â†’ ìƒ‰ìƒ, ëª¨ì–‘, í¬ê¸° ë“± ì‹œê°ì  íŠ¹ì§•ë„ í¬í•¨í•˜ì„¸ìš”
  â†’ í…ìŠ¤íŠ¸ë¡œ ì„¤ëª…í•˜ë˜ ì‹œê°ì ìœ¼ë¡œ ì´í•´ ê°€ëŠ¥í•˜ë„ë¡ ìƒì„¸íˆ ê¸°ë¡í•˜ì„¸ìš”"""
            
            if 'code' in missing_elements:
                special_prompt += """
- ğŸ’» **ì½”ë“œ ë¸”ë¡**: ëª¨ë“  ì½”ë“œ, ë“¤ì—¬ì“°ê¸°, ë³€ìˆ˜ëª…, í•¨ìˆ˜ëª… ì¶”ì¶œ
  â†’ ì½”ë“œì˜ ì •í™•í•œ êµ¬ì¡°ì™€ ë‚´ìš©ì„ ë³´ì¡´
  â†’ SQL ë¬¸(ì˜ˆ: SELECT, INSERT, WHERE), í”„ë¡œê·¸ë˜ë° ì½”ë“œ ì •í™•íˆ ì¶”ì¶œ
  â†’ ì„ íƒì§€ì— í¬í•¨ëœ ì½”ë“œë„ ëˆ„ë½ì—†ì´ ì¶”ì¶œí•˜ì„¸ìš”"""
            
            special_prompt += f"""

ğŸ“Š **JSON ì¶œë ¥**:
```json
{{
  "question_number": {question_number},
  "enhanced_question_text": "ë³´ì™„ëœ ë¬¸ì œ ì§€ë¬¸ (íŠ¹ìˆ˜ ìš”ì†Œ í¬í•¨)",
  "table_data": "í‘œê°€ ìˆë‹¤ë©´ ë§ˆí¬ë‹¤ìš´ í‘œ í˜•ì‹\n| í—¤ë” | í—¤ë”1 | í—¤ë”2 |\n|------|-----|-----|\n| ë°ì´í„° | ê°’ | ê°’ |",
  "diagram_description": "ë‹¤ì´ì–´ê·¸ë¨ì´ ìˆë‹¤ë©´ ë…¸ë“œ, ì—°ê²°ê´€ê³„, ë¼ë²¨, ë°°ì¹˜ ë“± ì „ì²´ êµ¬ì¡°ë¥¼ ìƒì„¸í•˜ê²Œ ì„¤ëª…",
  "code_content": "ì½”ë“œê°€ ìˆë‹¤ë©´ ì™„ì „í•œ ì½”ë“œ",
  "has_table": true/false,
  "has_diagram": true/false,
  "has_code": true/false
}}
```"""
            
            # GPT-4Vë¡œ íŠ¹ìˆ˜ ìš”ì†Œ ì¬ì¶”ì¶œ
            messages = [
                {
                    "role": "system",
                    "content": "íŠ¹ìˆ˜ ìš”ì†Œ(í‘œ, ë‹¤ì´ì–´ê·¸ë¨, ì½”ë“œ) ì§‘ì¤‘ ì¶”ì¶œ ì „ë¬¸ê°€ì…ë‹ˆë‹¤."
                },
                {
                    "role": "user",
                    "content": [
                        {"type": "text", "text": special_prompt},
                        {
                            "type": "image_url",
                            "image_url": {
                                "url": f"data:image/png;base64,{img_b64}",
                                "detail": "high"
                            }
                        }
                    ]
                }
            ]
            
            response = await self.openai_client.chat.completions.create(
                model="gpt-4o",
                messages=messages,
                max_tokens=3000,
                temperature=0.1
            )
            
            # ì‘ë‹µ íŒŒì‹± ë° ê¸°ì¡´ ë¬¸ì œ ë³´ì™„ (íŠ¹ìˆ˜ ìš”ì†Œ ì „ìš©)
            special_result = self._parse_special_element_response(response.choices[0].message.content)
            
            if special_result and len(special_result) > 0:
                enhanced_data = special_result[0]
                
                # ê¸°ì¡´ ë¬¸ì œì— íŠ¹ìˆ˜ ìš”ì†Œ ì •ë³´ ì¶”ê°€
                enhanced_question = current_question.copy()
                enhanced_question['question_text'] = enhanced_data.get('enhanced_question_text', current_question.get('question_text'))
                enhanced_question['table_data'] = enhanced_data.get('table_data')
                enhanced_question['code_content'] = enhanced_data.get('code_content')
                enhanced_question['has_table'] = enhanced_data.get('has_table', False)
                enhanced_question['has_diagram'] = enhanced_data.get('has_diagram', False)
                enhanced_question['has_code'] = enhanced_data.get('has_code', False)
                
                if enhanced_data.get('diagram_description'):
                    enhanced_question['question_text'] += f"\n\n[ë‹¤ì´ì–´ê·¸ë¨ ì„¤ëª…: {enhanced_data['diagram_description']}]"
                    enhanced_question['diagram_description'] = enhanced_data['diagram_description']
                
                return enhanced_question
            
            return None
            
        except Exception as e:
            print(f"   âš ï¸ ë¬¸ì œ {question_number}ë²ˆ íŠ¹ìˆ˜ ìš”ì†Œ ì¬ì¶”ì¶œ ì‹¤íŒ¨: {e}")
            return None

    def _validate_and_fix_question_counts(self, structure: Dict):
        """ë¬¸ì œ ìˆ˜ ë¶ˆì¼ì¹˜ ê²€ì¦ ë° ìˆ˜ì •"""
        
        try:
            # í˜ì´ì§€ë³„ ë¬¸ì œ ìˆ˜ ì§‘ê³„
            page_analysis = structure.get('page_analysis', [])
            total_questions_in_pages = 0
            all_questions_found = []
            
            for page_info in page_analysis:
                questions_on_page = page_info.get('questions_on_page', [])
                if questions_on_page:
                    all_questions_found.extend(questions_on_page)
                else:
                    # questions_on_pageê°€ ì—†ìœ¼ë©´ question_density ì‚¬ìš©
                    question_density = page_info.get('question_density', 0)
                    total_questions_in_pages += question_density
            
            # ì‹¤ì œ ë°œê²¬ëœ ë¬¸ì œ ë²ˆí˜¸ ê¸°ì¤€ìœ¼ë¡œ ê³„ì‚°
            if all_questions_found:
                actual_max_question = max(all_questions_found)
                actual_question_count = len(set(all_questions_found))  # ì¤‘ë³µ ì œê±°
                
                # ì´ ë¬¸ì œ ìˆ˜ì™€ ë¹„êµ
                declared_total = structure.get('analysis_summary', {}).get('total_questions', 0)
                
                # âš ï¸ ìë™ ìˆ˜ì • ì¡°ê±´ì„ ì—„ê²©í•˜ê²Œ ì œí•œ
                if declared_total != actual_max_question:
                    # ê°œê´„ ë¶„ì„ ê²°ê³¼ì™€ ë¹„êµ
                    overview_total = structure.get('global_overview', {}).get('actual_total_questions', 0)
                    
                    if overview_total > 0 and overview_total != actual_max_question:
                        print(f"âš ï¸ êµ¬ì¡° ë¶„ì„ ë¶ˆì¼ì¹˜ ë°œê²¬:")
                        print(f"   ê°œê´„ ë¶„ì„: {overview_total}ê°œ")
                        print(f"   ìƒì„¸ ë¶„ì„: {declared_total}ê°œ") 
                        print(f"   ì‹¤ì œ ë°œê²¬: {actual_max_question}ê°œ")
                        
                        # ê°œê´„ ë¶„ì„ ê²°ê³¼ë¥¼ ë” ì‹ ë¢°
                        if abs(overview_total - actual_max_question) > abs(declared_total - actual_max_question):
                            print(f"ğŸ”§ ê°œê´„ ë¶„ì„ ìš°ì„ : {declared_total} â†’ {overview_total}")
                            structure['analysis_summary']['total_questions'] = overview_total
                        else:
                            print(f"ğŸ”§ ì‹¤ì œ ë°œê²¬ ë°˜ì˜: {declared_total} â†’ {actual_max_question}")
                            structure['analysis_summary']['total_questions'] = actual_max_question
                    else:
                        print(f"ğŸ”§ ë¬¸ì œ ìˆ˜ ìë™ ìˆ˜ì •: {declared_total} â†’ {actual_max_question}")
                        structure['analysis_summary']['total_questions'] = actual_max_question
                    
                    # ê²€ì¦ ê²½ê³  ì¶”ê°€
                    if 'validation_warnings' not in structure:
                        structure['validation_warnings'] = []
                    structure['validation_warnings'].append(
                        f"ë¬¸ì œ ìˆ˜ ë¶ˆì¼ì¹˜: ì„ ì–¸ {declared_total}ê°œ vs ë°œê²¬ {actual_max_question}ê°œ vs ê°œê´„ {overview_total}ê°œ"
                    )
                
                # ëˆ„ë½ëœ ë¬¸ì œ ë²ˆí˜¸ í™•ì¸
                expected_questions = set(range(1, actual_max_question + 1))
                found_questions = set(all_questions_found)
                missing_questions = expected_questions - found_questions
                
                if missing_questions:
                    structure.setdefault('validation_warnings', []).append(
                        f"ëˆ„ë½ëœ ë¬¸ì œ ë²ˆí˜¸: {sorted(list(missing_questions))}"
                    )
            
        except Exception as e:
            print(f"âš ï¸ ë¬¸ì œ ìˆ˜ ê²€ì¦ ì‹¤íŒ¨: {e}")
            if 'validation_warnings' not in structure:
                structure['validation_warnings'] = []
            structure['validation_warnings'].append(f"ë¬¸ì œ ìˆ˜ ê²€ì¦ ì‹¤íŒ¨: {str(e)}")
    
    def _perform_quality_checks(self, structure: Dict) -> Dict[str, Any]:
        """ë¶„ì„ í’ˆì§ˆ ê²€ì‚¬"""
        
        checks = {
            "completeness_score": 0.0,
            "consistency_score": 0.0,
            "reliability_score": 0.0,
            "issues_found": []
        }
        
        try:
            # ì™„ì„±ë„ ê²€ì‚¬
            required_fields = ['analysis_summary', 'page_analysis', 'question_analysis']
            present_fields = sum(1 for field in required_fields if structure.get(field))
            checks['completeness_score'] = present_fields / len(required_fields)
            
            # ì¼ê´€ì„± ê²€ì‚¬
            total_questions_summary = structure.get('analysis_summary', {}).get('total_questions', 0)
            detailed_questions = structure.get('question_analysis', {}).get('detailed_questions', [])
            if total_questions_summary != len(detailed_questions):
                checks['issues_found'].append("ë¬¸ì œ ê°œìˆ˜ ë¶ˆì¼ì¹˜")
                checks['consistency_score'] = 0.7
            else:
                checks['consistency_score'] = 1.0
                
            # ì‹ ë¢°ë„ ì ìˆ˜ ê³„ì‚°
            confidence = structure.get('analysis_summary', {}).get('confidence_score', 0.0)
            checks['reliability_score'] = confidence
            
        except Exception as e:
            checks['issues_found'].append(f"í’ˆì§ˆ ê²€ì‚¬ ì˜¤ë¥˜: {str(e)}")
            
        return checks
    
    def _enhance_processing_strategy(self, structure: Dict) -> Dict[str, Any]:
        """ì²˜ë¦¬ ì „ëµ ë³´ì™„"""
        
        try:
            existing_strategy = structure.get('processing_strategy', {})
            
            # ê¸°ë³¸ ì „ëµ ë³´ì™„
            enhanced_strategy = {
                **existing_strategy,
                "ultra_precise_mode": True,
                "structure_based": True,
                "estimated_processing_time": "5-10ë¶„",
                "resource_requirements": "ê³ ì‚¬ì–‘ API í˜¸ì¶œ",
                "fallback_options": ["ë ˆê±°ì‹œ ì‹œìŠ¤í…œ", "ë‹¨ìˆœ ì²­í¬ ì²˜ë¦¬"]
            }
            
            return enhanced_strategy
            
        except Exception as e:
            print(f"âš ï¸ ì²˜ë¦¬ ì „ëµ ë³´ì™„ ì‹¤íŒ¨: {e}")
            return structure.get('processing_strategy', {})
    
    def _print_detailed_analysis_result(self, structure: Dict):
        """ìƒì„¸ ë¶„ì„ ê²°ê³¼ ì½˜ì†” ì¶œë ¥"""
        
        try:
            print("\n" + "="*80)
            print("ğŸ”¬ ì´ˆì •ë°€ PDF êµ¬ì¡° ë¶„ì„ ê²°ê³¼")
            print("="*80)
            
            # ê¸°ë³¸ ìš”ì•½
            summary = structure.get('analysis_summary', {})
            total_questions = summary.get('total_questions', 0)
            print(f"ğŸ“„ ë¬¸ì„œ íƒ€ì…: {summary.get('document_type', 'ë¯¸ìƒ')}")
            print(f"ğŸ“Š ì´ í˜ì´ì§€: {summary.get('total_pages', 0)}í˜ì´ì§€") 
            print(f"ğŸ“ ì´ ë¬¸ì œ ìˆ˜: {total_questions}ë¬¸ì œ")
            print(f"ğŸ¯ ë¶„ì„ ì‹ ë¢°ë„: {summary.get('confidence_score', 0.0)*100:.1f}%")
            
            # í˜ì´ì§€ë³„ ë¶„ì„ + ë¬¸ì œ ìˆ˜ ê²€ì¦
            page_analysis = structure.get('page_analysis', [])
            if page_analysis:
                print(f"\nğŸ“„ í˜ì´ì§€ë³„ ë¶„ì„:")
                total_questions_in_pages = 0
                all_questions_found = []
                
                for page_info in page_analysis:
                    page_num = page_info.get('page_number', 0)
                    page_type = page_info.get('page_type', 'ë¯¸ìƒ')
                    question_count = page_info.get('question_density', 0)
                    questions_on_page = page_info.get('questions_on_page', [])
                    
                    print(f"   í˜ì´ì§€ {page_num}: {page_type} ({question_count}ë¬¸ì œ)")
                    
                    # ì‹¤ì œ ë¬¸ì œ ë²ˆí˜¸ ëª©ë¡ì´ ìˆì„ ë•Œë§Œ ì¹´ìš´íŠ¸
                    if questions_on_page:
                        total_questions_in_pages += len(questions_on_page)
                        all_questions_found.extend(questions_on_page)
                        if len(questions_on_page) > 0:
                            min_q = min(questions_on_page)
                            max_q = max(questions_on_page)
                            print(f"      â†’ ë¬¸ì œ {min_q}~{max_q}ë²ˆ ({len(questions_on_page)}ê°œ)")
                    else:
                        # questions_on_pageê°€ ì—†ìœ¼ë©´ question_density ì‚¬ìš©
                        total_questions_in_pages += question_count
                
                # ë¬¸ì œ ìˆ˜ ì¼ì¹˜ ì—¬ë¶€ í™•ì¸
                print(f"\nğŸ” ë¬¸ì œ ìˆ˜ ê²€ì¦:")
                print(f"   ì´ ì„ ì–¸ëœ ë¬¸ì œ ìˆ˜: {total_questions}ê°œ")
                print(f"   í˜ì´ì§€ë³„ ë¬¸ì œ ìˆ˜ í•©ê³„: {total_questions_in_pages}ê°œ")
                
                if total_questions != total_questions_in_pages:
                    print(f"   âš ï¸ ë¶ˆì¼ì¹˜ ê°ì§€: {abs(total_questions - total_questions_in_pages)}ê°œ ì°¨ì´")
                    
                    # ëˆ„ë½ëœ ë¬¸ì œ ë²ˆí˜¸ ì°¾ê¸°
                    if all_questions_found:
                        expected_questions = set(range(1, total_questions + 1))
                        found_questions = set(all_questions_found)
                        missing_questions = expected_questions - found_questions
                        extra_questions = found_questions - expected_questions
                        
                        if missing_questions:
                            print(f"   ğŸ“ ëˆ„ë½ëœ ë¬¸ì œ: {sorted(list(missing_questions))}")
                        if extra_questions:
                            print(f"   â• ì¶”ê°€ ë°œê²¬ ë¬¸ì œ: {sorted(list(extra_questions))}")
                        
                        # ì‹¤ì œ ë°œê²¬ëœ ìµœëŒ€ ë¬¸ì œ ë²ˆí˜¸ë¡œ ì´ ë¬¸ì œ ìˆ˜ ìˆ˜ì •
                        if found_questions:
                            actual_max_question = max(found_questions)
                            print(f"   ğŸ¯ ì‹¤ì œ ìµœëŒ€ ë¬¸ì œ ë²ˆí˜¸: {actual_max_question}ë²ˆ")
                            print(f"   ğŸ’¡ ê¶Œì¥: ì´ ë¬¸ì œ ìˆ˜ë¥¼ {actual_max_question}ê°œë¡œ ìˆ˜ì •")
                else:
                    print(f"   âœ… ë¬¸ì œ ìˆ˜ ì¼ì¹˜ í™•ì¸")
            
            # ì„ íƒì§€ ìˆ˜ í†µê³„ ì¶œë ¥ (ìƒˆë¡œ ì¶”ê°€)
            self._display_choice_count_statistics(structure)
            
            # í–¥ìƒëœ í˜ì´ì§€ ê²½ê³„ ë¶„ì„ ê²°ê³¼ ì¶œë ¥
            self._display_cross_page_analysis_results(structure)
            
            # ì²˜ë¦¬ ì „ëµ
            strategy = structure.get('processing_strategy', {})
            if strategy:
                print(f"\nâš™ï¸ ê¶Œì¥ ì²˜ë¦¬ ì „ëµ:")
                print(f"   ì ‘ê·¼ë²•: {strategy.get('recommended_approach', 'ë¯¸ì •')}")
                print(f"   ì²­í¬ í¬ê¸°: {strategy.get('chunk_size_recommendation', 'ë¯¸ì •')}")
                
            # í’ˆì§ˆ ê²€ì‚¬ ê²°ê³¼
            quality = structure.get('quality_checks', {})
            if quality:
                print(f"\nâœ… ë¶„ì„ í’ˆì§ˆ:")
                print(f"   ì™„ì„±ë„: {quality.get('completeness_score', 0)*100:.1f}%")
                print(f"   ì¼ê´€ì„±: {quality.get('consistency_score', 0)*100:.1f}%")
                
            print("="*80 + "\n")
            
        except Exception as e:
            print(f"âš ï¸ ë¶„ì„ ê²°ê³¼ ì¶œë ¥ ì‹¤íŒ¨: {e}")
    
    def _display_cross_page_analysis_results(self, structure: Dict):
        """í–¥ìƒëœ í˜ì´ì§€ ê²½ê³„ ë¶„ì„ ê²°ê³¼ ì¶œë ¥"""
        
        try:
            # í…ìŠ¤íŠ¸ ê¸°ë°˜ ì‚¬ì „ ë¶„ì„ ê²°ê³¼ í‘œì‹œ
            cross_page_issues = structure.get('cross_page_issues')
            if cross_page_issues and not cross_page_issues.get('error'):
                print(f"\nğŸ” í–¥ìƒëœ í˜ì´ì§€ ê²½ê³„ ë¶„ì„ ê²°ê³¼:")
                print("=" * 50)
                
                # ë¶„í• ëœ ë¬¸ì œë“¤
                split_questions = cross_page_issues.get('split_questions', [])
                if split_questions:
                    print(f"âš ï¸ í˜ì´ì§€ ê°„ ë¶„í•  ë¬¸ì œ: {len(split_questions)}ê°œ")
                    for issue in split_questions[:3]:
                        q_num = issue.get('question_number', '?')
                        print(f"   ë¬¸ì œ {q_num}ë²ˆ: {issue.get('issue', '')}")
                
                # ë¶ˆì™„ì „í•œ ì„ íƒì§€ë“¤
                incomplete_choices = cross_page_issues.get('incomplete_choices', [])
                if incomplete_choices:
                    print(f"ğŸ“ ì„ íƒì§€ ë¶„í•  ë¬¸ì œ: {len(incomplete_choices)}ê°œ")
                    for issue in incomplete_choices[:3]:
                        print(f"   {issue.get('issue', '')}")
                        if issue.get('is_sequential'):
                            print(f"      ì—°ì†ì„± í™•ì¸: {issue.get('current_max')} â†’ {issue.get('next_min')}")
                
                # ìƒˆë¡œìš´ ê¸°ëŠ¥: í…ìŠ¤íŠ¸/ì½”ë“œ ìš”ì†Œ (ì‹¤ì œ ë°ì´í„° ê²€ì¦)
                text_code_elements = cross_page_issues.get('text_code_elements', [])
                if text_code_elements:
                    print(f"ğŸ” ì„ íƒì§€ ë‚´ íŠ¹ìˆ˜ ìš”ì†Œ: {len(text_code_elements)}ê°œ (ì‹¤ì œ ë¶„ì„ë¨)")
                    
                    # ì‹¤ì œ ë°ì´í„°ì¸ì§€ ê²€ì¦í•˜ê¸° ìœ„í•´ ì²« ë²ˆì§¸ ìš”ì†Œ ë””ë²„ê¹…
                    if len(text_code_elements) > 0:
                        first_element = text_code_elements[0]
                        print(f"   ğŸ” ì²« ë²ˆì§¸ ìš”ì†Œ ê²€ì¦: í˜ì´ì§€ {first_element.get('page')}, "
                              f"ì„ íƒì§€ {first_element.get('choice')}, "
                              f"ìš”ì†Œ {first_element.get('elements', [])}")
                        print(f"   ğŸ“ ë‚´ìš© ë¯¸ë¦¬ë³´ê¸°: {first_element.get('content_preview', 'N/A')}")
                    
                    choice_summary = {}
                    actual_count = 0
                    for element in text_code_elements:
                        page = element.get('page')
                        choice = element.get('choice')
                        elements = element.get('elements', [])
                        
                        if elements:  # ì‹¤ì œ ìš”ì†Œê°€ ìˆëŠ” ê²½ìš°ë§Œ ì¹´ìš´íŠ¸
                            actual_count += 1
                            key = f"í˜ì´ì§€ {page}"
                            if key not in choice_summary:
                                choice_summary[key] = []
                            choice_summary[key].append(f"ì„ íƒì§€ {choice}: {', '.join(elements)}")
                    
                    print(f"   ğŸ“Š ì‹¤ì œ íŠ¹ìˆ˜ ìš”ì†Œ í¬í•¨ ì„ íƒì§€: {actual_count}ê°œ")
                    
                    for page_key, elements in list(choice_summary.items())[:3]:  # ìµœëŒ€ 3í˜ì´ì§€ë§Œ í‘œì‹œ
                        print(f"   {page_key}:")
                        for elem in elements[:2]:  # í˜ì´ì§€ë‹¹ ìµœëŒ€ 2ê°œì”©
                            print(f"      â€¢ {elem}")
                
                # í˜ì´ì§€ ê°„ í‘œ ë¶„í• 
                cross_page_tables = cross_page_issues.get('cross_page_tables', [])
                if cross_page_tables:
                    print(f"ğŸ“Š í˜ì´ì§€ ê°„ í‘œ ë¶„í• : {len(cross_page_tables)}ê°œ")
                    for table in cross_page_tables[:2]:
                        print(f"   {table.get('issue', '')}")
                
                # ì¢…í•© ë¶„ì„ ê²°ê³¼
                detailed_issues = cross_page_issues.get('detailed_issues', {})
                if detailed_issues:
                    total_problems = detailed_issues.get('total_cross_page_problems', 0)
                    severity = detailed_issues.get('severity_level', 'low')
                    print(f"ğŸ“Š ì¢…í•© ë¶„ì„: {total_problems}ê°œ ì´ìŠˆ ({severity} ì‹¬ê°ë„)")
                    
                    recommendations = detailed_issues.get('processing_recommendations', [])
                    if recommendations:
                        print(f"ğŸ’¡ ì²˜ë¦¬ ê¶Œì¥ì‚¬í•­:")
                        for rec in recommendations[:3]:
                            print(f"   â€¢ {rec}")
                
                if not any([split_questions, incomplete_choices, text_code_elements, cross_page_tables]):
                    print("âœ… í˜ì´ì§€ ê²½ê³„ ë¬¸ì œ ì—†ìŒ")
                
                print("=" * 50)
                
        except Exception as e:
            print(f"âš ï¸ í˜ì´ì§€ ê²½ê³„ ë¶„ì„ ê²°ê³¼ ì¶œë ¥ ì‹¤íŒ¨: {e}")
    
    def _display_choice_count_statistics(self, structure: Dict):
        """ì„ íƒì§€ ìˆ˜ í†µê³„ ì¶œë ¥"""
        
        try:
            print("\nğŸ“Š ë¬¸ì œë³„ ì„ íƒì§€ ìˆ˜ í†µê³„:")
            print("=" * 50)
            
            # detailed_analysisì—ì„œ ì„ íƒì§€ ë¶„ì„ ì •ë³´ ì¶”ì¶œ
            detailed_analysis = structure.get('detailed_analysis', {})
            special_elements = detailed_analysis.get('special_elements', {})
            
            # ì„ íƒì§€ ìˆ˜ë³„ ìƒì„¸ ë¶„ì„ì´ ìˆëŠ”ì§€ í™•ì¸
            choice_analysis = special_elements.get('choice_analysis_detailed', {})
            
            if choice_analysis:
                # ê¸°ì¡´ ë°ì´í„°ê°€ ìˆëŠ” ê²½ìš°
                total_questions = 0
                for choice_count, questions in choice_analysis.items():
                    if isinstance(questions, list) and questions:
                        count = len(questions)
                        total_questions += count
                        choice_num = choice_count.replace('questions_with_', '').replace('_choices', '')
                        print(f"ğŸ“ {choice_num}ê°œ ì„ íƒì§€: {count}ê°œ ë¬¸ì œ - {questions}")
                
                print(f"\nğŸ“Š ì´ {total_questions}ê°œ ë¬¸ì œ ë¶„ì„ ì™„ë£Œ")
                
                # í‰ê·  ì„ íƒì§€ ìˆ˜ ê³„ì‚°
                if total_questions > 0:
                    weighted_sum = 0
                    for choice_count, questions in choice_analysis.items():
                        if isinstance(questions, list) and questions:
                            choice_num = int(choice_count.replace('questions_with_', '').replace('_choices', ''))
                            weighted_sum += choice_num * len(questions)
                    
                    average_choices = weighted_sum / total_questions
                    print(f"ğŸ“ˆ í‰ê·  ì„ íƒì§€ ìˆ˜: {average_choices:.1f}ê°œ")
                    
            else:
                # ê¸°ë³¸ ì¶”ì •ì¹˜ ì œê³µ
                total_questions = structure.get('analysis_summary', {}).get('total_questions', 0)
                print(f"ğŸ“ ë¶„ì„ ëŒ€ê¸° ì¤‘... (ì´ {total_questions}ê°œ ë¬¸ì œ)")
                print("   ìƒì„¸ ì„ íƒì§€ ë¶„ì„ì€ ë¬¸ì œ ì¶”ì¶œ ê³¼ì •ì—ì„œ ìˆ˜í–‰ë©ë‹ˆë‹¤")
            
            # íŠ¹ìˆ˜ ìš”ì†Œê°€ ìˆëŠ” ë¬¸ì œë“¤ì˜ ì„ íƒì§€ íŠ¹ì„±
            special_question_types = ['tables', 'code_blocks', 'visual_elements_detailed']
            for element_type in special_question_types:
                if element_type in special_elements:
                    elements = special_elements[element_type]
                    if elements:
                        if element_type == 'tables':
                            table_questions = [elem.get('question') for elem in elements if elem.get('question')]
                            if table_questions:
                                print(f"ğŸ“Š í‘œ í¬í•¨ ë¬¸ì œ({len(table_questions)}ê°œ): ì„ íƒì§€ ì •ë°€ ì¶”ì¶œ í•„ìš” - {table_questions}")
                        elif element_type == 'code_blocks':
                            code_questions = [elem.get('question') for elem in elements if elem.get('question')]
                            if code_questions:
                                print(f"ğŸ’» ì½”ë“œ í¬í•¨ ë¬¸ì œ({len(code_questions)}ê°œ): ë¬¸ë²• ì •í™•ì„± ì¤‘ìš” - {code_questions}")
                        elif element_type == 'visual_elements_detailed':
                            visual_questions = []
                            for visual_type in ['diagram_in_choices', 'graph_in_choices', 'shape_in_choices']:
                                if visual_type in elements:
                                    visual_questions.extend([elem.get('question') for elem in elements[visual_type] if elem.get('question')])
                            if visual_questions:
                                visual_questions = list(set(visual_questions))  # ì¤‘ë³µ ì œê±°
                                print(f"ğŸ¨ ì‹œê°ì  ì„ íƒì§€ ë¬¸ì œ({len(visual_questions)}ê°œ): ê³ í•´ìƒë„ ì²˜ë¦¬ í•„ìš” - {visual_questions}")
            
            print("=" * 50)
            
        except Exception as e:
            print(f"âš ï¸ ì„ íƒì§€ ìˆ˜ í†µê³„ ì¶œë ¥ ì‹¤íŒ¨: {e}")
    
    def _enhance_question_with_table_data(self, original_question: Dict, api_response: str) -> Dict:
        """í‘œ ë°ì´í„°ë¡œ ë¬¸ì œ ì •ë³´ í–¥ìƒ"""
        
        try:
            # JSON ì‘ë‹µ íŒŒì‹±
            import json
            import re
            
            # JSON ë¸”ë¡ ì¶”ì¶œ
            json_match = re.search(r'```json\s*(.*?)\s*```', api_response, re.DOTALL)
            if json_match:
                json_str = json_match.group(1)
            else:
                json_str = api_response
            
            parsed_response = json.loads(json_str)
            
            # ê¸°ì¡´ ë¬¸ì œ ì •ë³´ë¥¼ ë³µì‚¬í•˜ê³  í–¥ìƒëœ ë°ì´í„°ë¡œ ì—…ë°ì´íŠ¸
            enhanced_question = original_question.copy()
            
            if isinstance(parsed_response, dict):
                # ë‹¨ì¼ ë¬¸ì œ ì‘ë‹µ
                if "question_text" in parsed_response:
                    enhanced_question["question_text"] = parsed_response["question_text"]
                if "options" in parsed_response:
                    enhanced_question["options"] = parsed_response["options"]
                if "table_data" in parsed_response:
                    enhanced_question["table_data"] = parsed_response["table_data"]
                    
            elif isinstance(parsed_response, list) and len(parsed_response) > 0:
                # ë‹¤ì¤‘ ë¬¸ì œ ì‘ë‹µ ì¤‘ ì²« ë²ˆì§¸
                first_question = parsed_response[0]
                if "question_text" in first_question:
                    enhanced_question["question_text"] = first_question["question_text"]
                if "options" in first_question:
                    enhanced_question["options"] = first_question["options"]
                if "table_data" in first_question:
                    enhanced_question["table_data"] = first_question["table_data"]
            
            # ì²˜ë¦¬ ë©”íƒ€ë°ì´í„° ì¶”ê°€
            enhanced_question["processing_enhanced"] = "table_specialized"
            enhanced_question["enhancement_timestamp"] = str(int(time.time()))
            
            return enhanced_question
            
        except Exception as e:
            print(f"âš ï¸ í‘œ ë°ì´í„° í–¥ìƒ ì‹¤íŒ¨: {e}")
            return original_question
    
    def _enhance_question_with_code_data(self, original_question: Dict, api_response: str) -> Dict:
        """ì½”ë“œ ë°ì´í„°ë¡œ ë¬¸ì œ ì •ë³´ í–¥ìƒ"""
        
        try:
            import json
            import re
            
            # JSON ì‘ë‹µ íŒŒì‹±
            json_match = re.search(r'```json\s*(.*?)\s*```', api_response, re.DOTALL)
            if json_match:
                json_str = json_match.group(1)
            else:
                json_str = api_response
            
            parsed_response = json.loads(json_str)
            
            # ê¸°ì¡´ ë¬¸ì œ ì •ë³´ë¥¼ ë³µì‚¬í•˜ê³  í–¥ìƒëœ ë°ì´í„°ë¡œ ì—…ë°ì´íŠ¸
            enhanced_question = original_question.copy()
            
            if isinstance(parsed_response, dict):
                # ë‹¨ì¼ ë¬¸ì œ ì‘ë‹µ
                if "question_text" in parsed_response:
                    enhanced_question["question_text"] = parsed_response["question_text"]
                if "options" in parsed_response:
                    enhanced_question["options"] = parsed_response["options"]
                if "code_block" in parsed_response:
                    enhanced_question["code_block"] = parsed_response["code_block"]
                    
            elif isinstance(parsed_response, list) and len(parsed_response) > 0:
                # ë‹¤ì¤‘ ë¬¸ì œ ì‘ë‹µ ì¤‘ ì²« ë²ˆì§¸
                first_question = parsed_response[0]
                if "question_text" in first_question:
                    enhanced_question["question_text"] = first_question["question_text"]
                if "options" in first_question:
                    enhanced_question["options"] = first_question["options"]
                if "code_block" in first_question:
                    enhanced_question["code_block"] = first_question["code_block"]
            
            # ì²˜ë¦¬ ë©”íƒ€ë°ì´í„° ì¶”ê°€
            enhanced_question["processing_enhanced"] = "code_specialized"
            enhanced_question["enhancement_timestamp"] = str(int(time.time()))
            
            return enhanced_question
            
        except Exception as e:
            print(f"âš ï¸ ì½”ë“œ ë°ì´í„° í–¥ìƒ ì‹¤íŒ¨: {e}")
            return original_question
    
    def _enhance_question_with_diagram_data(self, original_question: Dict, api_response: str) -> Dict:
        """ë‹¤ì´ì–´ê·¸ë¨ ë°ì´í„°ë¡œ ë¬¸ì œ ì •ë³´ í–¥ìƒ"""
        
        try:
            import json
            import re
            
            # JSON ì‘ë‹µ íŒŒì‹±
            json_match = re.search(r'```json\s*(.*?)\s*```', api_response, re.DOTALL)
            if json_match:
                json_str = json_match.group(1)
            else:
                json_str = api_response
            
            parsed_response = json.loads(json_str)
            
            # ê¸°ì¡´ ë¬¸ì œ ì •ë³´ë¥¼ ë³µì‚¬í•˜ê³  í–¥ìƒëœ ë°ì´í„°ë¡œ ì—…ë°ì´íŠ¸
            enhanced_question = original_question.copy()
            
            if isinstance(parsed_response, dict):
                # ë‹¨ì¼ ë¬¸ì œ ì‘ë‹µ
                if "question_text" in parsed_response:
                    enhanced_question["question_text"] = parsed_response["question_text"]
                if "options" in parsed_response:
                    enhanced_question["options"] = parsed_response["options"]
                if "diagram_description" in parsed_response:
                    enhanced_question["diagram_description"] = parsed_response["diagram_description"]
                    
            elif isinstance(parsed_response, list) and len(parsed_response) > 0:
                # ë‹¤ì¤‘ ë¬¸ì œ ì‘ë‹µ ì¤‘ ì²« ë²ˆì§¸
                first_question = parsed_response[0]
                if "question_text" in first_question:
                    enhanced_question["question_text"] = first_question["question_text"]
                if "options" in first_question:
                    enhanced_question["options"] = first_question["options"]
                if "diagram_description" in first_question:
                    enhanced_question["diagram_description"] = first_question["diagram_description"]
            
            # ì²˜ë¦¬ ë©”íƒ€ë°ì´í„° ì¶”ê°€
            enhanced_question["processing_enhanced"] = "diagram_specialized"
            enhanced_question["enhancement_timestamp"] = str(int(time.time()))
            
            return enhanced_question
            
        except Exception as e:
            print(f"âš ï¸ ë‹¤ì´ì–´ê·¸ë¨ ë°ì´í„° í–¥ìƒ ì‹¤íŒ¨: {e}")
            return original_question
    
    def _enhance_question_with_complex_choices(self, original_question: Dict, api_response: str, expected_choice_count: int) -> Dict:
        """ë³µì¡í•œ ì„ íƒì§€ë¡œ ë¬¸ì œ ì •ë³´ í–¥ìƒ"""
        
        try:
            import json
            import re
            
            # JSON ì‘ë‹µ íŒŒì‹±
            json_match = re.search(r'```json\s*(.*?)\s*```', api_response, re.DOTALL)
            if json_match:
                json_str = json_match.group(1)
            else:
                json_str = api_response
            
            parsed_response = json.loads(json_str)
            
            # ê¸°ì¡´ ë¬¸ì œ ì •ë³´ë¥¼ ë³µì‚¬í•˜ê³  í–¥ìƒëœ ë°ì´í„°ë¡œ ì—…ë°ì´íŠ¸
            enhanced_question = original_question.copy()
            
            if isinstance(parsed_response, dict):
                # ë‹¨ì¼ ë¬¸ì œ ì‘ë‹µ
                if "question_text" in parsed_response:
                    enhanced_question["question_text"] = parsed_response["question_text"]
                if "options" in parsed_response:
                    new_options = parsed_response["options"]
                    # ì„ íƒì§€ ê°œìˆ˜ ê²€ì¦
                    if len(new_options) == expected_choice_count:
                        enhanced_question["options"] = new_options
                    else:
                        print(f"âš ï¸ ì„ íƒì§€ ê°œìˆ˜ ë¶ˆì¼ì¹˜: ì˜ˆìƒ {expected_choice_count}ê°œ vs ì¶”ì¶œ {len(new_options)}ê°œ")
                        # ê¸°ì¡´ ì„ íƒì§€ ìœ ì§€
                    
            elif isinstance(parsed_response, list) and len(parsed_response) > 0:
                # ë‹¤ì¤‘ ë¬¸ì œ ì‘ë‹µ ì¤‘ ì²« ë²ˆì§¸
                first_question = parsed_response[0]
                if "question_text" in first_question:
                    enhanced_question["question_text"] = first_question["question_text"]
                if "options" in first_question:
                    new_options = first_question["options"]
                    # ì„ íƒì§€ ê°œìˆ˜ ê²€ì¦
                    if len(new_options) == expected_choice_count:
                        enhanced_question["options"] = new_options
                    else:
                        print(f"âš ï¸ ì„ íƒì§€ ê°œìˆ˜ ë¶ˆì¼ì¹˜: ì˜ˆìƒ {expected_choice_count}ê°œ vs ì¶”ì¶œ {len(new_options)}ê°œ")
            
            # ì²˜ë¦¬ ë©”íƒ€ë°ì´í„° ì¶”ê°€
            enhanced_question["processing_enhanced"] = "complex_choices_specialized"
            enhanced_question["enhancement_timestamp"] = str(int(time.time()))
            enhanced_question["expected_choice_count"] = expected_choice_count
            
            return enhanced_question
            
        except Exception as e:
            print(f"âš ï¸ ë³µì¡ ì„ íƒì§€ í–¥ìƒ ì‹¤íŒ¨: {e}")
            return original_question