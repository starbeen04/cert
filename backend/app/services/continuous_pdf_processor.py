"""
Continuous PDF Processor - ê³ í•´ìƒë„ ì—°ì† ì´ë¯¸ì§€ ì²˜ë¦¬ ì‹œìŠ¤í…œ
í˜ì´ì§€ ê²½ê³„ ë¬¸ì œ í•´ê²°ì„ ìœ„í•œ í•˜ë‚˜ì˜ ê¸´ ì´ë¯¸ì§€ ì ‘ê·¼ë²•
"""

import asyncio
import json
import os
import base64
import io
from typing import Dict, List, Any, Optional, Tuple
from pathlib import Path
import cv2
import numpy as np
from PIL import Image
import fitz  # PyMuPDF
from sqlalchemy.orm import Session
from sqlalchemy import text
import openai
import anthropic
from datetime import datetime
import logging

logger = logging.getLogger(__name__)

class ContinuousPDFProcessor:
    """ì—°ì† ì´ë¯¸ì§€ ì²˜ë¦¬ ê¸°ë°˜ PDF ì²˜ë¦¬ ì‹œìŠ¤í…œ"""
    
    def __init__(self):
        self.openai_client = None
        self.claude_client = None
        self.temp_dir = Path("temp_processing")
        self.temp_dir.mkdir(exist_ok=True)
        
    def set_openai_key(self, api_key: str):
        """OpenAI API í‚¤ ì„¤ì •"""
        self.openai_client = openai.AsyncOpenAI(api_key=api_key)
        
    def set_claude_key(self, api_key: str):
        """Claude API í‚¤ ì„¤ì •"""
        self.claude_client = anthropic.AsyncAnthropic(api_key=api_key)
    
    async def process_pdf_continuous_pipeline(self, upload_id: int, pdf_path: str, db: Session) -> Dict[str, Any]:
        """ì—°ì† ì´ë¯¸ì§€ ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰"""
        try:
            print(f"[ì‹œì‘] Continuous Image Processing Pipeline - Upload {upload_id}")
            
            # 1ë‹¨ê³„: PDFë¥¼ í•˜ë‚˜ì˜ ê¸´ ì—°ì† ì´ë¯¸ì§€ë¡œ ë³€í™˜
            continuous_result = await self._convert_pdf_to_continuous_image(upload_id, pdf_path, db)
            if not continuous_result['success']:
                return continuous_result
                
            # 2ë‹¨ê³„: GPT Visionìœ¼ë¡œ ì—°ì† ì´ë¯¸ì§€ë¥¼ ì²­í¬ë³„ë¡œ Markdown ë³€í™˜
            markdown_result = await self._convert_continuous_image_to_markdown(upload_id, continuous_result, db)
            if not markdown_result['success']:
                return markdown_result
                
            # 3ë‹¨ê³„: Ultra Enhanced Processorë¡œ ë¬¸ì œ êµ¬ì¡°í™”
            final_results = await self._structure_continuous_content(upload_id, markdown_result, db)
            
            return {
                'success': True,
                'upload_id': upload_id,
                'processing_summary': final_results,
                'continuous_image_info': continuous_result.get('image_info', {}),
                'questions_extracted': len(final_results.get('questions', [])),
                'materials_generated': len(final_results.get('materials', []))
            }
            
        except Exception as e:
            logger.error(f"Continuous PDF ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ ì˜¤ë¥˜: {str(e)}")
            await self._log_processing_step(upload_id, "continuous_pipeline_error", "failed", 
                                          {"error": str(e)}, db)
            return {'success': False, 'error': str(e)}
    
    async def _convert_pdf_to_continuous_image(self, upload_id: int, pdf_path: str, db: Session) -> Dict[str, Any]:
        """1ë‹¨ê³„: PDFë¥¼ í•˜ë‚˜ì˜ ê¸´ ì—°ì† ì´ë¯¸ì§€ë¡œ ë³€í™˜"""
        try:
            await self._log_processing_step(upload_id, "pdf_to_continuous_image", "processing", {}, db)
            
            doc = fitz.open(pdf_path)
            total_pages = len(doc)
            
            print(f"Converting {total_pages} pages to one continuous high-resolution image...")
            
            # ê³ í•´ìƒë„ ì„¤ì • (4ë°° í™•ëŒ€ë¡œ ìµœê³  í’ˆì§ˆ)
            scale_factor = 4.0
            mat = fitz.Matrix(scale_factor, scale_factor)
            
            # ëª¨ë“  í˜ì´ì§€ì˜ ì´ë¯¸ì§€ ìˆ˜ì§‘
            page_images = []
            total_width = 0
            max_width = 0
            total_height = 0
            
            for page_num in range(total_pages):
                page = doc[page_num]
                
                # ê³ í•´ìƒë„ë¡œ í˜ì´ì§€ ë Œë”ë§
                pix = page.get_pixmap(matrix=mat, alpha=False)
                
                # PIL Imageë¡œ ë³€í™˜
                img_data = pix.tobytes("png")
                pil_image = Image.open(io.BytesIO(img_data))
                
                page_images.append({
                    'image': pil_image,
                    'page_number': page_num + 1,
                    'width': pil_image.width,
                    'height': pil_image.height
                })
                
                total_height += pil_image.height
                max_width = max(max_width, pil_image.width)
                
                print(f"  Page {page_num + 1}: {pil_image.width}x{pil_image.height}")
            
            doc.close()
            
            # ì—°ì† ì´ë¯¸ì§€ ìƒì„± (ëª¨ë“  í˜ì´ì§€ë¥¼ ì„¸ë¡œë¡œ ì—°ê²°)
            print(f"Creating continuous image: {max_width}x{total_height}")
            
            # í°ìƒ‰ ë°°ê²½ìœ¼ë¡œ í° ì´ë¯¸ì§€ ìƒì„±
            continuous_image = Image.new('RGB', (max_width, total_height), 'white')
            
            # ê° í˜ì´ì§€ë¥¼ ìˆœì„œëŒ€ë¡œ ë°°ì¹˜
            current_y = 0
            page_boundaries = []  # í˜ì´ì§€ ê²½ê³„ ì •ë³´ ì €ì¥
            
            for page_info in page_images:
                page_img = page_info['image']
                
                # ì¤‘ì•™ ì •ë ¬ë¡œ í˜ì´ì§€ ë°°ì¹˜
                x_offset = (max_width - page_img.width) // 2
                continuous_image.paste(page_img, (x_offset, current_y))
                
                # í˜ì´ì§€ ê²½ê³„ ì •ë³´ ì €ì¥
                page_boundaries.append({
                    'page_number': page_info['page_number'],
                    'start_y': current_y,
                    'end_y': current_y + page_img.height,
                    'x_offset': x_offset,
                    'width': page_img.width,
                    'height': page_img.height
                })
                
                current_y += page_img.height
                
                print(f"  Placed page {page_info['page_number']} at y={current_y - page_img.height}")
            
            # ì—°ì† ì´ë¯¸ì§€ ì €ì¥
            continuous_image_path = self.temp_dir / f"continuous_{upload_id}.png"
            continuous_image.save(continuous_image_path, quality=95, optimize=True)
            
            image_info = {
                'path': str(continuous_image_path),
                'width': max_width,
                'height': total_height,
                'total_pages': total_pages,
                'file_size': os.path.getsize(continuous_image_path),
                'page_boundaries': page_boundaries,
                'scale_factor': scale_factor
            }
            
            print(f"Continuous image created:")
            print(f"  - Size: {max_width}x{total_height}")
            print(f"  - File size: {image_info['file_size'] / (1024*1024):.1f} MB")
            print(f"  - Pages: {total_pages}")
            
            await self._log_processing_step(upload_id, "pdf_to_continuous_image", "completed", {
                'image_dimensions': f"{max_width}x{total_height}",
                'total_pages': total_pages,
                'file_size_mb': round(image_info['file_size'] / (1024*1024), 2)
            }, db)
            
            return {
                'success': True,
                'image_info': image_info
            }
            
        except Exception as e:
            logger.error(f"PDF to continuous image conversion error: {str(e)}")
            await self._log_processing_step(upload_id, "pdf_to_continuous_image", "failed", 
                                          {"error": str(e)}, db)
            return {'success': False, 'error': str(e)}
    
    async def _convert_continuous_image_to_markdown(self, upload_id: int, continuous_result: Dict, db: Session) -> Dict[str, Any]:
        """2ë‹¨ê³„: ì—°ì† ì´ë¯¸ì§€ë¥¼ ê²¹ì¹˜ëŠ” ì²­í¬ë¡œ ë¶„í• í•˜ì—¬ Markdown ë³€í™˜"""
        try:
            await self._log_processing_step(upload_id, "continuous_to_markdown", "processing", {}, db)
            
            if not self.openai_client:
                print("ERROR: OpenAI API key not set")
                return {'success': False, 'error': 'OpenAI API key not set'}
            
            image_info = continuous_result['image_info']
            image_path = image_info['path']
            total_height = image_info['height']
            width = image_info['width']
            
            print(f"Processing continuous image with overlapping chunks...")
            
            # ì²­í¬ ì„¤ì • (í˜ì´ì§€ ê²½ê³„ ë¬¸ì œ í•´ê²°ì„ ìœ„í•´ ê²¹ì¹˜ëŠ” êµ¬ê°„ ì„¤ì •)
            chunk_height = 3000  # ê° ì²­í¬ ë†’ì´ (ì•½ 1-2í˜ì´ì§€ ë¶„ëŸ‰)
            overlap_height = 800  # ê²¹ì¹˜ëŠ” êµ¬ê°„ (ë¬¸ì œ ë¶„í•  ë°©ì§€)
            
            chunks_info = []
            current_y = 0
            chunk_index = 0
            
            while current_y < total_height:
                # ì²­í¬ ê²½ê³„ ê³„ì‚°
                chunk_end_y = min(current_y + chunk_height, total_height)
                
                chunks_info.append({
                    'chunk_index': chunk_index,
                    'start_y': current_y,
                    'end_y': chunk_end_y,
                    'height': chunk_end_y - current_y,
                    'width': width
                })
                
                print(f"  Chunk {chunk_index}: y={current_y}-{chunk_end_y} (height={chunk_end_y - current_y})")
                
                # ë‹¤ìŒ ì²­í¬ ì‹œì‘ì  (ê²¹ì¹˜ëŠ” êµ¬ê°„ ê³ ë ¤)
                current_y += chunk_height - overlap_height
                chunk_index += 1
                
                # ë§ˆì§€ë§‰ ì²­í¬ ì²˜ë¦¬
                if chunk_end_y >= total_height:
                    break
            
            print(f"Total chunks created: {len(chunks_info)}")
            
            # ì›ë³¸ ì´ë¯¸ì§€ ë¡œë“œ
            original_image = Image.open(image_path)
            
            # ê° ì²­í¬ë¥¼ Markdownìœ¼ë¡œ ë³€í™˜
            all_chunks_markdown = []
            total_tokens_used = 0
            
            for chunk_info in chunks_info:
                print(f"Processing chunk {chunk_info['chunk_index'] + 1}/{len(chunks_info)}...")
                
                # ì²­í¬ ì´ë¯¸ì§€ ì¶”ì¶œ
                chunk_image = original_image.crop((
                    0,
                    chunk_info['start_y'],
                    chunk_info['width'],
                    chunk_info['end_y']
                ))
                
                # ì²­í¬ ì´ë¯¸ì§€ë¥¼ base64ë¡œ ì¸ì½”ë”©
                buffer = io.BytesIO()
                chunk_image.save(buffer, format='PNG')
                base64_image = base64.b64encode(buffer.getvalue()).decode('utf-8')
                
                try:
                    # GPT Visionìœ¼ë¡œ ì²­í¬ë¥¼ Markdownìœ¼ë¡œ ë³€í™˜
                    prompt = f"""
ì´ ì´ë¯¸ì§€ëŠ” ì—°ì† PDF ë¬¸ì„œì˜ ì²­í¬ {chunk_info['chunk_index'] + 1}ë²ˆì…ë‹ˆë‹¤.
ì´ ì²­í¬ì—ëŠ” ì—¬ëŸ¬ í˜ì´ì§€ì˜ ë‚´ìš©ì´ ì—°ê²°ë˜ì–´ ìˆì„ ìˆ˜ ìˆìœ¼ë©°, ë¬¸ì œê°€ ì´ì „ ì²­í¬ì—ì„œ ì‹œì‘ë˜ì–´ ì´ ì²­í¬ì—ì„œ ëë‚˜ê±°ë‚˜, ì´ ì²­í¬ì—ì„œ ì‹œì‘ë˜ì–´ ë‹¤ìŒ ì²­í¬ë¡œ ì´ì–´ì§ˆ ìˆ˜ ìˆìŠµë‹ˆë‹¤.

ğŸ¯ **í•µì‹¬ ìš”êµ¬ì‚¬í•­**:
1. **ì™„ì „í•œ ë¬¸ì œ ì‹ë³„**: ì´ ì²­í¬ì—ì„œ ì™„ì „íˆ ë³´ì´ëŠ” ë¬¸ì œë“¤ì„ ëª¨ë‘ ì¶”ì¶œ
2. **ë¶€ë¶„ ë¬¸ì œ í‘œì‹œ**: ì‹œì‘ë§Œ ìˆê±°ë‚˜ ëë§Œ ìˆëŠ” ë¶ˆì™„ì „í•œ ë¬¸ì œë„ ëª¨ë‘ í¬í•¨
3. **ëª¨ë“  ì„ íƒì§€ ì¶”ì¶œ**: â‘ â‘¡â‘¢â‘£, 1)2)3)4), (A)(B)(C)(D) ë“± ëª¨ë“  í˜•íƒœ
4. **í‘œ/ê·¸ë¦¼/ì½”ë“œ ì™„ì „ ë³´ì¡´**: ë³µì¡í•œ ë‚´ìš©ë„ í…ìŠ¤íŠ¸ë¡œ ìƒì„¸íˆ ì„¤ëª…
5. **í…ìŠ¤íŠ¸ í’ˆì§ˆ**: OCR ì˜¤ë¥˜ ìµœì†Œí™”ë¥¼ ìœ„í•´ ì •í™•í•œ í…ìŠ¤íŠ¸ ì¶”ì¶œ

ğŸ“‹ **ì¶œë ¥ í˜•ì‹**:
```
# Chunk {chunk_info['chunk_index'] + 1}

## [ë¬¸ì œë²ˆí˜¸ ë˜ëŠ” PARTIAL_START/PARTIAL_END í‘œì‹œ]
[ë¬¸ì œ ë‚´ìš©ì„ ì™„ì „íˆ ì‘ì„±]
[ì„ íƒì§€ë“¤ì„ ëª¨ë‘ ì‘ì„±]

## [ë‹¤ìŒ ë¬¸ì œ...]
```

âš ï¸ **íŠ¹ë³„ ì²˜ë¦¬**:
- ë¬¸ì œê°€ ì¤‘ê°„ì— ì˜ë¦° ê²½ìš°: "## PARTIAL_START_Q[ë²ˆí˜¸]" ë˜ëŠ” "## PARTIAL_END_Q[ë²ˆí˜¸]"ë¡œ í‘œì‹œ
- í‘œë‚˜ ê·¸ë˜í”„: êµ¬ì¡°ì™€ ë‚´ìš©ì„ ëª¨ë‘ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜
- ë¹ˆ êµ¬ê°„: ê±´ë„ˆë›°ì§€ ë§ê³  "[ë¹ˆ ê³µê°„]"ìœ¼ë¡œ í‘œì‹œ
- ì¶”ê°€ ì„¤ëª… ì—†ì´ ì˜¤ì§ Markdownë§Œ ì¶œë ¥
                    """.strip()
                    
                    response = await self.openai_client.chat.completions.create(
                        model="gpt-4o",
                        messages=[
                            {
                                "role": "user",
                                "content": [
                                    {"type": "text", "text": prompt},
                                    {
                                        "type": "image_url",
                                        "image_url": {
                                            "url": f"data:image/png;base64,{base64_image}",
                                            "detail": "high"
                                        }
                                    }
                                ]
                            }
                        ],
                        max_tokens=8000,  # ë” ë§ì€ í† í°ìœ¼ë¡œ ì™„ì „í•œ ì¶”ì¶œ
                        temperature=0.1   # ì •í™•ì„±ì„ ìœ„í•´ ë‚®ì€ ì˜¨ë„
                    )
                    
                    chunk_markdown = response.choices[0].message.content.strip()
                    
                    all_chunks_markdown.append({
                        'chunk_index': chunk_info['chunk_index'],
                        'markdown_content': chunk_markdown,
                        'start_y': chunk_info['start_y'],
                        'end_y': chunk_info['end_y'],
                        'tokens_used': response.usage.total_tokens
                    })
                    
                    total_tokens_used += response.usage.total_tokens
                    
                    print(f"  Chunk {chunk_info['chunk_index'] + 1} converted - {len(chunk_markdown)} chars, {response.usage.total_tokens} tokens")
                    
                    # API ë ˆì´íŠ¸ ë¦¬ë¯¸íŠ¸ ë°©ì§€
                    await asyncio.sleep(1.0)
                    
                except Exception as chunk_error:
                    print(f"  Failed to convert chunk {chunk_info['chunk_index']}: {chunk_error}")
                    all_chunks_markdown.append({
                        'chunk_index': chunk_info['chunk_index'],
                        'markdown_content': f"# Chunk {chunk_info['chunk_index'] + 1}\n\n[ì²­í¬ ë³€í™˜ ì‹¤íŒ¨: {str(chunk_error)}]",
                        'start_y': chunk_info['start_y'],
                        'end_y': chunk_info['end_y'],
                        'tokens_used': 0
                    })
            
            # ëª¨ë“  ì²­í¬ ê²°í•©
            complete_markdown = "\n\n---\n\n".join([chunk['markdown_content'] for chunk in all_chunks_markdown])
            
            print(f"Continuous image to Markdown conversion completed:")
            print(f"  - Total chunks: {len(all_chunks_markdown)}")
            print(f"  - Total markdown length: {len(complete_markdown)} characters")
            print(f"  - Total tokens used: {total_tokens_used}")
            
            # ìƒ˜í”Œ ì¶œë ¥
            print(f"=== ì—°ì† ì²˜ë¦¬ Markdown ê²°ê³¼ ìƒ˜í”Œ (ì²˜ìŒ 1500ì) ===")
            print(complete_markdown[:1500])
            print("=== ìƒ˜í”Œ ë ===")
            
            await self._log_processing_step(upload_id, "continuous_to_markdown", "completed", {
                'total_chunks': len(all_chunks_markdown),
                'markdown_length': len(complete_markdown),
                'tokens_used': total_tokens_used
            }, db)
            
            return {
                'success': True,
                'markdown_content': complete_markdown,
                'chunks_processed': len(all_chunks_markdown),
                'chunks_info': all_chunks_markdown,
                'tokens_used': total_tokens_used
            }
            
        except Exception as e:
            logger.error(f"Continuous image to Markdown conversion error: {str(e)}")
            await self._log_processing_step(upload_id, "continuous_to_markdown", "failed", 
                                          {"error": str(e)}, db)
            return {'success': False, 'error': str(e)}
    
    async def _structure_continuous_content(self, upload_id: int, markdown_result: Dict, db: Session) -> Dict[str, Any]:
        """3ë‹¨ê³„: Ultra Enhanced Processorë¡œ ì—°ì† ì½˜í…ì¸  êµ¬ì¡°í™”"""
        try:
            await self._log_processing_step(upload_id, "continuous_structuring", "processing", {}, db)
            
            if not self.claude_client:
                print("ERROR: Claude API key not set")
                return {'success': False, 'questions': [], 'materials': [], 'error': 'Claude API key not set'}
            
            # Ultra Enhanced PDF Processor ì‚¬ìš©
            from app.services.ultra_enhanced_pdf_processor import UltraEnhancedPDFProcessor
            
            ultra_processor = UltraEnhancedPDFProcessor(self.claude_client)
            
            # Markdownì„ ì²­í¬ë¡œ ë¶„í•  (Ultra Enhanced Processor í˜¸í™˜)
            markdown_content = markdown_result.get('markdown_content', '')
            chunks_info = markdown_result.get('chunks_info', [])
            
            # ì²­í¬ë³„ Markdown ì¶”ì¶œ
            chunks = [chunk_info['markdown_content'] for chunk_info in chunks_info]
            
            print(f"Processing {len(chunks)} chunks with Ultra Enhanced Pipeline...")
            
            # Ultra Enhanced ì²˜ë¦¬ ì‹¤í–‰
            processing_result = await ultra_processor.process_chunks_ultra(chunks)
            
            # ê²°ê³¼ë¥¼ ë°ì´í„°ë² ì´ìŠ¤ì— ì €ì¥
            questions = processing_result.get('questions', [])
            
            saved_questions = 0
            saved_materials = 0
            
            if questions:
                for question in questions:
                    try:
                        # ë°ì´í„°ë² ì´ìŠ¤ì— ë¬¸ì œ ì €ì¥
                        db.execute(
                            text("""
                                INSERT INTO extracted_questions 
                                (pdf_upload_id, question_number, question_text, passage, options, 
                                 correct_answer, explanation, difficulty_level, category, 
                                 additional_info, created_at) 
                                VALUES 
                                (:pdf_upload_id, :question_number, :question_text, :passage, :options,
                                 :correct_answer, :explanation, :difficulty_level, :category,
                                 :additional_info, :created_at)
                            """),
                            {
                                'pdf_upload_id': upload_id,
                                'question_number': question.get('question_number', 0),
                                'question_text': question.get('question_text', ''),
                                'passage': question.get('passage', ''),
                                'options': json.dumps(question.get('options', []), ensure_ascii=False),
                                'correct_answer': question.get('correct_answer'),
                                'explanation': question.get('explanation'),
                                'difficulty_level': question.get('difficulty_level', 'medium'),
                                'category': question.get('category', 'general'),
                                'additional_info': json.dumps({
                                    'processing_method': 'continuous_image',
                                    'confidence': question.get('confidence', 'medium'),
                                    'has_table': question.get('has_table', False),
                                    'has_figure': question.get('has_figure', False),
                                    'has_code': question.get('has_code', False),
                                    'special_handling': question.get('special_handling', 'none'),
                                    'validation_status': question.get('validation_status', 'unknown')
                                }, ensure_ascii=False),
                                'created_at': datetime.now()
                            }
                        )
                        saved_questions += 1
                        
                    except Exception as save_error:
                        print(f"Failed to save question {question.get('question_number')}: {save_error}")
                
                db.commit()
            
            final_result = {
                'success': True,
                'questions': questions,
                'materials': [],  # ì¶”í›„ êµ¬í˜„
                'chapters': [],  # ì¶”í›„ êµ¬í˜„
                'processing_stats': processing_result.get('stage_stats', {}),
                'saved_questions': saved_questions,
                'saved_materials': saved_materials
            }
            
            print(f"Continuous structuring completed:")
            print(f"  - Questions extracted: {len(questions)}")
            print(f"  - Questions saved: {saved_questions}")
            print(f"  - Processing stats: {processing_result.get('stage_stats', {})}")
            
            await self._log_processing_step(upload_id, "continuous_structuring", "completed", {
                'questions_extracted': len(questions),
                'questions_saved': saved_questions,
                'materials_saved': saved_materials,
                'processing_stats': processing_result.get('stage_stats', {})
            }, db)
            
            return final_result
            
        except Exception as e:
            logger.error(f"Continuous content structuring error: {str(e)}")
            await self._log_processing_step(upload_id, "continuous_structuring", "failed", 
                                          {"error": str(e)}, db)
            return {'success': False, 'questions': [], 'materials': [], 'error': str(e)}
    
    async def _log_processing_step(self, upload_id: int, step_name: str, status: str, 
                                 step_data: Dict[str, Any], db: Session):
        """ì²˜ë¦¬ ë‹¨ê³„ ë¡œê¹…"""
        try:
            db.execute(
                text("""
                    INSERT INTO processing_steps 
                    (pdf_upload_id, step_name, status, step_data, created_at)
                    VALUES (:pdf_upload_id, :step_name, :status, :step_data, :created_at)
                """),
                {
                    'pdf_upload_id': upload_id,
                    'step_name': step_name,
                    'status': status,
                    'step_data': json.dumps(step_data, ensure_ascii=False),
                    'created_at': datetime.now()
                }
            )
            db.commit()
        except Exception as e:
            print(f"Failed to log processing step: {e}")