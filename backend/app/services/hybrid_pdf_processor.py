#!/usr/bin/env python3
"""
ğŸ¯ í•˜ì´ë¸Œë¦¬ë“œ PDF ì²˜ë¦¬ ì‹œìŠ¤í…œ
- ë°ì´í„° íƒ€ì…ë³„ íŠ¹í™” ì²˜ë¦¬ + ê°œë³„ ì²˜ë¦¬ ì •í™•ë„ ìœ ì§€ + í¬ë¡œìŠ¤ í˜ì´ì§€ ì—°ê²°
"""

import re
import json
import base64
from PIL import Image
import io
import os
import asyncio
from pathlib import Path
from typing import Dict, List, Optional, Any, Tuple, Union
import fitz  # PyMuPDF
import openai
from datetime import datetime
import numpy as np
from dataclasses import dataclass
from enum import Enum

class DataType(Enum):
    """ë°ì´í„° íƒ€ì… ë¶„ë¥˜"""
    TEXT_QUESTION = "text_question"      # ìˆœìˆ˜ í…ìŠ¤íŠ¸ ë¬¸ì œ
    TABLE_DATA = "table_data"            # í‘œ ë°ì´í„° (ìŠ¤ì¼€ì¤„ë§, ì„±ëŠ¥ ë“±)
    CODE_BLOCK = "code_block"            # í”„ë¡œê·¸ë˜ë° ì½”ë“œ
    DIAGRAM_IMAGE = "diagram_image"      # ë‹¤ì´ì–´ê·¸ë¨, íŠ¸ë¦¬, ê·¸ë˜í”„
    CHOICE_IMAGE = "choice_image"        # ì´ë¯¸ì§€ê°€ ì„ íƒì§€ì¸ ê²½ìš° (ERD ê¸°í˜¸ ë“±)
    MIXED_CONTENT = "mixed_content"      # í…ìŠ¤íŠ¸ + í‘œ/ì´ë¯¸ì§€ í˜¼í•©
    PASSAGE_TEXT = "passage_text"        # ê¸´ ì§€ë¬¸/ë³´ê¸°
    FORMULA_MATH = "formula_math"        # ìˆ˜ì‹/ê³µì‹

@dataclass
class ProcessingStrategy:
    """ë°ì´í„° íƒ€ì…ë³„ ì²˜ë¦¬ ì „ëµ"""
    data_type: DataType
    extraction_method: str
    accuracy_priority: str
    special_handling: List[str]
    cross_page_sensitive: bool

class HybridPDFProcessor:
    """í•˜ì´ë¸Œë¦¬ë“œ PDF ì²˜ë¦¬ ì‹œìŠ¤í…œ - ë°ì´í„° íƒ€ì…ë³„ íŠ¹í™” + ê°œë³„ ì •í™•ë„ ìœ ì§€"""
    
    def __init__(self, openai_client: openai.AsyncOpenAI):
        self.openai_client = openai_client
        
        # ë°ì´í„° íƒ€ì…ë³„ ì²˜ë¦¬ ì „ëµ ì •ì˜
        self.processing_strategies = {
            DataType.TEXT_QUESTION: ProcessingStrategy(
                data_type=DataType.TEXT_QUESTION,
                extraction_method="vision_ocr_standard",
                accuracy_priority="text_precision",
                special_handling=["korean_text", "choice_markers"],
                cross_page_sensitive=True
            ),
            DataType.TABLE_DATA: ProcessingStrategy(
                data_type=DataType.TABLE_DATA,
                extraction_method="table_structure_analysis",
                accuracy_priority="data_integrity",
                special_handling=["number_precision", "cell_boundary", "header_detection"],
                cross_page_sensitive=True  # í‘œëŠ” í¬ë¡œìŠ¤ í˜ì´ì§€ì— ë§¤ìš° ë¯¼ê°
            ),
            DataType.CODE_BLOCK: ProcessingStrategy(
                data_type=DataType.CODE_BLOCK,
                extraction_method="code_syntax_analysis",
                accuracy_priority="syntax_preservation",
                special_handling=["indentation", "keywords", "symbols", "brackets"],
                cross_page_sensitive=True
            ),
            DataType.DIAGRAM_IMAGE: ProcessingStrategy(
                data_type=DataType.DIAGRAM_IMAGE,
                extraction_method="image_extraction",
                accuracy_priority="visual_fidelity",
                special_handling=["image_save", "description_generation"],
                cross_page_sensitive=False  # ë‹¤ì´ì–´ê·¸ë¨ì€ ë³´í†µ í•œ í˜ì´ì§€ ë‚´
            ),
            DataType.CHOICE_IMAGE: ProcessingStrategy(
                data_type=DataType.CHOICE_IMAGE,
                extraction_method="choice_image_analysis",
                accuracy_priority="choice_distinction",
                special_handling=["image_per_choice", "choice_labeling"],
                cross_page_sensitive=False
            ),
            DataType.MIXED_CONTENT: ProcessingStrategy(
                data_type=DataType.MIXED_CONTENT,
                extraction_method="multi_modal_analysis",
                accuracy_priority="content_separation",
                special_handling=["text_image_boundary", "context_preservation"],
                cross_page_sensitive=True
            ),
            DataType.PASSAGE_TEXT: ProcessingStrategy(
                data_type=DataType.PASSAGE_TEXT,
                extraction_method="long_text_analysis",
                accuracy_priority="context_coherence",
                special_handling=["paragraph_structure", "line_breaks"],
                cross_page_sensitive=True
            ),
            DataType.FORMULA_MATH: ProcessingStrategy(
                data_type=DataType.FORMULA_MATH,
                extraction_method="math_formula_extraction",
                accuracy_priority="symbol_accuracy",
                special_handling=["math_symbols", "subscripts", "superscripts"],
                cross_page_sensitive=False
            )
        }
        
        # PDF ê·œì¹™
        self.PDF_RULES = {
            "total_questions": 60,
            "choice_markers": ["â‘ ", "â‘¡", "â‘¢", "â‘£"],
            "subjects": {
                "1-20": "ì •ë³´ì‹œìŠ¤í…œ ê¸°ë°˜ ê¸°ìˆ ", 
                "21-40": "í”„ë¡œê·¸ë˜ë° ì–¸ì–´ í™œìš©",
                "41-60": "ë°ì´í„°ë² ì´ìŠ¤ í™œìš©"
            },
            # ë°ì´í„° íƒ€ì…ë³„ ì˜ˆìƒ ë¶„í¬
            "data_type_distribution": {
                DataType.TEXT_QUESTION: [1, 2, 3, 4, 5, 7, 8, 9, 10, 11, 12, 14, 16, 17, 18, 19, 20, 21, 22, 23, 25, 26, 27, 28, 29, 30, 31, 32, 34, 35, 36, 37, 38, 39, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 57, 58, 59, 60],
                DataType.TABLE_DATA: [6],  # ìŠ¤ì¼€ì¤„ë§ í‘œ
                DataType.CHOICE_IMAGE: [15],  # ERD ê¸°í˜¸ ì„ íƒì§€
                DataType.CODE_BLOCK: [24, 33, 40],  # Java ì½”ë“œ
                DataType.DIAGRAM_IMAGE: [56],  # íŠ¸ë¦¬ ë‹¤ì´ì–´ê·¸ë¨
                DataType.MIXED_CONTENT: [13],  # í…ìŠ¤íŠ¸ + ì´ë¯¸ì§€
            }
        }
    
    async def process_pdf_hybrid(self, pdf_path: str, upload_id: int) -> Dict[str, Any]:
        """í•˜ì´ë¸Œë¦¬ë“œ ì²˜ë¦¬ ë©”ì¸ í•¨ìˆ˜"""
        
        print("ğŸ¯ í•˜ì´ë¸Œë¦¬ë“œ PDF ì²˜ë¦¬ ì‹œì‘ - ë°ì´í„° íƒ€ì…ë³„ íŠ¹í™” ì²˜ë¦¬")
        print("=" * 70)
        
        try:
            # === 1ë‹¨ê³„: PDF êµ¬ì¡° ë¶„ì„ ë° ë°ì´í„° íƒ€ì… ë¶„ë¥˜ ===
            print("ğŸ“‹ 1ë‹¨ê³„: PDF êµ¬ì¡° ë¶„ì„ ë° ë°ì´í„° íƒ€ì… ë¶„ë¥˜")
            structure_analysis = await self._analyze_pdf_structure_and_classify(pdf_path, upload_id)
            
            # === 2ë‹¨ê³„: í¬ë¡œìŠ¤ í˜ì´ì§€ ì—°ê²° ì§€ë„ ìƒì„± ===
            print("ğŸ”— 2ë‹¨ê³„: í¬ë¡œìŠ¤ í˜ì´ì§€ ì—°ê²° ì§€ë„ ìƒì„±")
            cross_page_map = await self._create_cross_page_connection_map(structure_analysis)
            
            # === 3ë‹¨ê³„: ë°ì´í„° íƒ€ì…ë³„ íŠ¹í™” ì²˜ë¦¬ ===
            print("ğŸ¨ 3ë‹¨ê³„: ë°ì´í„° íƒ€ì…ë³„ íŠ¹í™” ì²˜ë¦¬")
            processed_elements = await self._process_by_data_type(structure_analysis, cross_page_map)
            
            # === 4ë‹¨ê³„: ê°œë³„ ì²˜ë¦¬ ê²°ê³¼ í†µí•© ===
            print("ğŸ”§ 4ë‹¨ê³„: ê°œë³„ ì²˜ë¦¬ ê²°ê³¼ í†µí•©")
            unified_questions = self._unify_processed_elements(processed_elements)
            
            # === 5ë‹¨ê³„: í’ˆì§ˆ ê²€ì¦ ë° ìµœì í™” ===
            print("âœ… 5ë‹¨ê³„: í’ˆì§ˆ ê²€ì¦ ë° ìµœì í™”")
            final_questions = await self._quality_check_and_optimize(unified_questions, structure_analysis)
            
            print(f"âœ… í•˜ì´ë¸Œë¦¬ë“œ ì²˜ë¦¬ ì™„ë£Œ: {len(final_questions)}ê°œ ë¬¸ì œ ì¶”ì¶œ")
            
            return {
                "success": True,
                "questions": final_questions,
                "total_questions": len(final_questions),
                "processing_method": "hybrid_specialized",
                "structure_analysis": structure_analysis,
                "cross_page_map": cross_page_map,
                "processing_stats": self._generate_processing_stats(processed_elements)
            }
            
        except Exception as e:
            print(f"âŒ í•˜ì´ë¸Œë¦¬ë“œ ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
            import traceback
            traceback.print_exc()
            return {"success": False, "error": str(e)}
    
    async def _analyze_pdf_structure_and_classify(self, pdf_path: str, upload_id: int) -> Dict[str, Any]:
        """1ë‹¨ê³„: PDF êµ¬ì¡° ë¶„ì„ ë° ê° ë¬¸ì œì˜ ë°ì´í„° íƒ€ì… ë¶„ë¥˜"""
        
        assets_dir = Path(f"assets/upload_{upload_id}")
        assets_dir.mkdir(parents=True, exist_ok=True)
        
        doc = fitz.open(pdf_path)
        
        # í˜ì´ì§€ë³„ ê¸°ë³¸ ì •ë³´ ìˆ˜ì§‘
        pages_info = []
        for page_num in range(len(doc)):
            page = doc[page_num]
            
            # ê³ í•´ìƒë„ ì´ë¯¸ì§€ ìƒì„± (ê°œë³„ ì²˜ë¦¬ ì •í™•ë„ ìœ ì§€)
            mat = fitz.Matrix(4.0, 4.0)  # 400dpi for maximum accuracy
            pix = page.get_pixmap(matrix=mat)
            img_path = assets_dir / f"page_{page_num + 1}_hq.png"
            pix.save(str(img_path))
            
            # í…ìŠ¤íŠ¸ ë° êµ¬ì¡° ì •ë³´
            text_dict = page.get_text("dict")
            raw_text = page.get_text()
            
            pages_info.append({
                "page_number": page_num + 1,
                "raw_text": raw_text,
                "text_dict": text_dict,
                "image_path": str(img_path),
                "blocks": text_dict["blocks"] if "blocks" in text_dict else []
            })
        
        doc.close()
        
        # ê° í˜ì´ì§€ì—ì„œ ë¬¸ì œ ìš”ì†Œë“¤ ê°ì§€ ë° ë°ì´í„° íƒ€ì… ë¶„ë¥˜
        classified_elements = []
        
        for page_info in pages_info:
            page_elements = await self._classify_page_elements(page_info)
            classified_elements.extend(page_elements)
        
        print(f"   ğŸ“Š ë¶„ë¥˜ëœ ìš”ì†Œ: {len(classified_elements)}ê°œ")
        
        return {
            "pages_info": pages_info,
            "classified_elements": classified_elements,
            "assets_dir": str(assets_dir)
        }
    
    async def _classify_page_elements(self, page_info: Dict) -> List[Dict[str, Any]]:
        """í˜ì´ì§€ ë‚´ ìš”ì†Œë“¤ì„ ë°ì´í„° íƒ€ì…ë³„ë¡œ ë¶„ë¥˜"""
        
        elements = []
        raw_text = page_info["raw_text"]
        page_num = page_info["page_number"]
        
        # ë¬¸ì œ ë²ˆí˜¸ íŒ¨í„´ ì°¾ê¸°
        question_patterns = re.findall(r'(\d+)\.?\s*([^0-9\n]{10,})', raw_text)
        
        for q_num_str, content_preview in question_patterns:
            try:
                q_num = int(q_num_str)
                if 1 <= q_num <= 60:
                    # ë°ì´í„° íƒ€ì… ë¶„ë¥˜
                    data_type = self._classify_question_data_type(q_num, content_preview, raw_text)
                    
                    elements.append({
                        "question_number": q_num,
                        "page_number": page_num,
                        "data_type": data_type,
                        "content_preview": content_preview[:100],
                        "processing_strategy": self.processing_strategies[data_type],
                        "estimated_position": self._estimate_question_position(q_num_str, raw_text)
                    })
            except ValueError:
                continue
        
        return elements
    
    def _classify_question_data_type(self, q_num: int, content: str, full_text: str) -> DataType:
        """ë¬¸ì œ ë²ˆí˜¸ì™€ ë‚´ìš©ì„ ê¸°ë°˜ìœ¼ë¡œ ë°ì´í„° íƒ€ì… ë¶„ë¥˜"""
        
        # ë¯¸ë¦¬ ì•Œë ¤ì§„ íŠ¹ìˆ˜ ë¬¸ì œë“¤
        if q_num in self.PDF_RULES["data_type_distribution"].get(DataType.TABLE_DATA, []):
            return DataType.TABLE_DATA
        elif q_num in self.PDF_RULES["data_type_distribution"].get(DataType.CODE_BLOCK, []):
            return DataType.CODE_BLOCK
        elif q_num in self.PDF_RULES["data_type_distribution"].get(DataType.CHOICE_IMAGE, []):
            return DataType.CHOICE_IMAGE
        elif q_num in self.PDF_RULES["data_type_distribution"].get(DataType.DIAGRAM_IMAGE, []):
            return DataType.DIAGRAM_IMAGE
        
        # ë‚´ìš© ê¸°ë°˜ ìë™ ë¶„ë¥˜
        content_lower = content.lower()
        full_text_lower = full_text.lower()
        
        # í‘œ ê´€ë ¨ í‚¤ì›Œë“œ
        table_keywords = ["í”„ë¡œì„¸ìŠ¤", "ë„ì°©ì‹œê°„", "ì‹¤í–‰ì‹œê°„", "ìŠ¤ì¼€ì¤„ë§", "ëŒ€ê¸°ì‹œê°„", "p1", "p2", "p3"]
        if any(keyword in content_lower for keyword in table_keywords):
            return DataType.TABLE_DATA
        
        # ì½”ë“œ ê´€ë ¨ í‚¤ì›Œë“œ
        code_keywords = ["class", "public", "void", "main", "string", "int", "for", "if", "while", "{", "}", ";"]
        if any(keyword in content_lower for keyword in code_keywords):
            return DataType.CODE_BLOCK
        
        # ìˆ˜ì‹ ê´€ë ¨ í‚¤ì›Œë“œ
        math_keywords = ["âˆ‘", "âˆ", "âˆ«", "âˆš", "Â²", "Â³", "â‰¤", "â‰¥", "â‰ ", "Î±", "Î²", "Î³"]
        if any(keyword in content for keyword in math_keywords):
            return DataType.FORMULA_MATH
        
        # ê¸´ ì§€ë¬¸ì¸ì§€ í™•ì¸
        if len(content) > 200:
            return DataType.PASSAGE_TEXT
        
        # ê¸°ë³¸ê°’: í…ìŠ¤íŠ¸ ë¬¸ì œ
        return DataType.TEXT_QUESTION
    
    def _estimate_question_position(self, q_num_str: str, text: str) -> int:
        """í…ìŠ¤íŠ¸ì—ì„œ ë¬¸ì œì˜ ëŒ€ëµì ì¸ ìœ„ì¹˜ ì¶”ì •"""
        try:
            pattern = f"{q_num_str}\\."
            match = re.search(pattern, text)
            if match:
                # í…ìŠ¤íŠ¸ì—ì„œì˜ ë¬¸ì ìœ„ì¹˜ë¥¼ Y ì¢Œí‘œë¡œ ê·¼ì‚¬ ë³€í™˜
                char_pos = match.start()
                estimated_y = (char_pos / len(text)) * 3000  # ëŒ€ëµì ì¸ ì´ë¯¸ì§€ ë†’ì´
                return int(estimated_y)
        except:
            pass
        return 0
    
    async def _create_cross_page_connection_map(self, structure_analysis: Dict) -> Dict[str, Any]:
        """2ë‹¨ê³„: í¬ë¡œìŠ¤ í˜ì´ì§€ ì—°ê²°ì´ í•„ìš”í•œ ìš”ì†Œë“¤ ì‹ë³„"""
        
        elements = structure_analysis["classified_elements"]
        cross_page_connections = []
        
        # í¬ë¡œìŠ¤ í˜ì´ì§€ì— ë¯¼ê°í•œ ë°ì´í„° íƒ€ì…ë“¤ í™•ì¸
        sensitive_elements = [elem for elem in elements 
                            if elem["processing_strategy"].cross_page_sensitive]
        
        print(f"   ğŸ”— í¬ë¡œìŠ¤ í˜ì´ì§€ ë¯¼ê° ìš”ì†Œ: {len(sensitive_elements)}ê°œ")
        
        # í˜ì´ì§€ ê²½ê³„ ê·¼ì²˜ì˜ ìš”ì†Œë“¤ í™•ì¸
        pages_info = structure_analysis["pages_info"]
        
        for i in range(len(pages_info) - 1):
            current_page = pages_info[i]
            next_page = pages_info[i + 1]
            
            # í˜„ì¬ í˜ì´ì§€ ëë¶€ë¶„ê³¼ ë‹¤ìŒ í˜ì´ì§€ ì‹œì‘ë¶€ë¶„ ë¶„ì„
            connection = await self._analyze_page_boundary_connection(
                current_page, next_page, sensitive_elements
            )
            
            if connection["has_connection"]:
                cross_page_connections.append(connection)
        
        return {
            "connections": cross_page_connections,
            "sensitive_elements": sensitive_elements,
            "total_connections": len(cross_page_connections)
        }
    
    async def _analyze_page_boundary_connection(self, current_page: Dict, next_page: Dict, 
                                             sensitive_elements: List[Dict]) -> Dict[str, Any]:
        """í˜ì´ì§€ ê²½ê³„ì—ì„œ ì—°ê²°ì´ í•„ìš”í•œ ìš”ì†Œ ë¶„ì„"""
        
        current_text = current_page["raw_text"]
        next_text = next_page["raw_text"]
        
        # í˜„ì¬ í˜ì´ì§€ì˜ ë§ˆì§€ë§‰ ë¶€ë¶„ (í•˜ìœ„ 20%)
        current_lines = current_text.strip().split('\n')
        boundary_lines = current_lines[-max(5, len(current_lines)//5):]
        
        # ë‹¤ìŒ í˜ì´ì§€ì˜ ì‹œì‘ ë¶€ë¶„ (ìƒìœ„ 20%)  
        next_lines = next_text.strip().split('\n')
        start_lines = next_lines[:max(5, len(next_lines)//5)]
        
        # ì—°ê²° íŒ¨í„´ ê°ì§€
        connection_patterns = {
            "incomplete_choices": self._detect_incomplete_choices(boundary_lines, start_lines),
            "split_table": self._detect_split_table(boundary_lines, start_lines),
            "continued_code": self._detect_continued_code(boundary_lines, start_lines),
            "split_passage": self._detect_split_passage(boundary_lines, start_lines)
        }
        
        has_connection = any(pattern["detected"] for pattern in connection_patterns.values())
        
        return {
            "has_connection": has_connection,
            "from_page": current_page["page_number"],
            "to_page": next_page["page_number"],
            "connection_type": [k for k, v in connection_patterns.items() if v["detected"]],
            "connection_details": connection_patterns
        }
    
    def _detect_incomplete_choices(self, boundary_lines: List[str], start_lines: List[str]) -> Dict:
        """ë¶ˆì™„ì „í•œ ì„ íƒì§€ ê°ì§€"""
        
        choice_markers = ["â‘ ", "â‘¡", "â‘¢", "â‘£"]
        
        # ê²½ê³„ì—ì„œ ì˜ë¦° ì„ íƒì§€ ì°¾ê¸°
        incomplete_choices = []
        for line in boundary_lines:
            for marker in choice_markers:
                if marker in line and (len(line) < 30 or "..." in line or line.endswith(marker)):
                    incomplete_choices.append({"marker": marker, "partial_text": line})
        
        # ë‹¤ìŒ í˜ì´ì§€ì—ì„œ ì—°ê²°ë˜ëŠ” ë¶€ë¶„ ì°¾ê¸°
        continuation_found = False
        for line in start_lines:
            if any(marker in line for marker in choice_markers):
                continuation_found = True
                break
        
        return {
            "detected": len(incomplete_choices) > 0 and continuation_found,
            "incomplete_choices": incomplete_choices,
            "continuation_found": continuation_found
        }
    
    def _detect_split_table(self, boundary_lines: List[str], start_lines: List[str]) -> Dict:
        """ë¶„í• ëœ í‘œ ê°ì§€"""
        
        table_indicators = ["í”„ë¡œì„¸ìŠ¤", "ë„ì°©ì‹œê°„", "ì‹¤í–‰ì‹œê°„", "|", "P1", "P2", "P3"]
        
        # ê²½ê³„ì— í‘œ í—¤ë”ê°€ ìˆëŠ”ì§€ í™•ì¸
        has_table_header = any(
            any(indicator in line for indicator in table_indicators[:4]) 
            for line in boundary_lines
        )
        
        # ë‹¤ìŒ í˜ì´ì§€ì— í‘œ ë°ì´í„°ê°€ ìˆëŠ”ì§€ í™•ì¸
        has_table_data = any(
            any(indicator in line for indicator in table_indicators[4:]) 
            for line in start_lines
        )
        
        return {
            "detected": has_table_header and has_table_data,
            "header_in_boundary": has_table_header,
            "data_in_start": has_table_data
        }
    
    def _detect_continued_code(self, boundary_lines: List[str], start_lines: List[str]) -> Dict:
        """ì—°ì†ëœ ì½”ë“œ ë¸”ë¡ ê°ì§€"""
        
        code_indicators = ["{", "}", "class", "public", "void", ";", "//"]
        
        # ê²½ê³„ì— ì½”ë“œ ì‹œì‘ì´ ìˆëŠ”ì§€
        has_code_start = any(
            any(indicator in line for indicator in code_indicators)
            for line in boundary_lines
        )
        
        # ë‹¤ìŒ í˜ì´ì§€ì— ì½”ë“œ ì—°ì†ì´ ìˆëŠ”ì§€
        has_code_continuation = any(
            any(indicator in line for indicator in code_indicators)
            for line in start_lines
        )
        
        return {
            "detected": has_code_start and has_code_continuation,
            "code_start": has_code_start,
            "code_continuation": has_code_continuation
        }
    
    def _detect_split_passage(self, boundary_lines: List[str], start_lines: List[str]) -> Dict:
        """ë¶„í• ëœ ì§€ë¬¸ ê°ì§€"""
        
        # ê²½ê³„ì˜ ë§ˆì§€ë§‰ ì¤„ì´ ë¬¸ì¥ ì¤‘ê°„ì—ì„œ ëë‚˜ëŠ”ì§€ í™•ì¸
        last_line = boundary_lines[-1] if boundary_lines else ""
        incomplete_sentence = not any(last_line.strip().endswith(punct) for punct in [".", "?", "!", "ã€‘", ")"]) and len(last_line.strip()) > 0
        
        # ë‹¤ìŒ í˜ì´ì§€ ì²« ì¤„ì´ ì†Œë¬¸ìë‚˜ ì—°ê²°ì–´ë¡œ ì‹œì‘í•˜ëŠ”ì§€ í™•ì¸
        first_line = start_lines[0] if start_lines else ""
        sentence_continuation = any(first_line.strip().startswith(cont) for cont in ["ê·¸", "ì´", "ê·¸ëŸ¬", "í•˜ì§€ë§Œ", "ë˜í•œ"])
        
        return {
            "detected": incomplete_sentence and sentence_continuation,
            "incomplete_sentence": incomplete_sentence,
            "sentence_continuation": sentence_continuation
        }
    
    async def _process_by_data_type(self, structure_analysis: Dict, cross_page_map: Dict) -> Dict[str, Any]:
        """3ë‹¨ê³„: ë°ì´í„° íƒ€ì…ë³„ íŠ¹í™” ì²˜ë¦¬"""
        
        elements = structure_analysis["classified_elements"]
        processed_results = {}
        
        # ë°ì´í„° íƒ€ì…ë³„ë¡œ ê·¸ë£¹í™”
        grouped_by_type = {}
        for element in elements:
            data_type = element["data_type"]
            if data_type not in grouped_by_type:
                grouped_by_type[data_type] = []
            grouped_by_type[data_type].append(element)
        
        print(f"   ğŸ¨ ë°ì´í„° íƒ€ì…ë³„ ë¶„í¬:")
        for data_type, items in grouped_by_type.items():
            print(f"      - {data_type.value}: {len(items)}ê°œ")
        
        # ê° ë°ì´í„° íƒ€ì…ë³„ë¡œ íŠ¹í™” ì²˜ë¦¬
        for data_type, items in grouped_by_type.items():
            print(f"   ğŸ”„ {data_type.value} ì²˜ë¦¬ ì¤‘... ({len(items)}ê°œ)")
            
            if data_type == DataType.TEXT_QUESTION:
                results = await self._process_text_questions(items, structure_analysis, cross_page_map)
            elif data_type == DataType.TABLE_DATA:
                results = await self._process_table_data(items, structure_analysis, cross_page_map)
            elif data_type == DataType.CODE_BLOCK:
                results = await self._process_code_blocks(items, structure_analysis, cross_page_map)
            elif data_type == DataType.DIAGRAM_IMAGE:
                results = await self._process_diagram_images(items, structure_analysis, cross_page_map)
            elif data_type == DataType.CHOICE_IMAGE:
                results = await self._process_choice_images(items, structure_analysis, cross_page_map)
            elif data_type == DataType.MIXED_CONTENT:
                results = await self._process_mixed_content(items, structure_analysis, cross_page_map)
            elif data_type == DataType.PASSAGE_TEXT:
                results = await self._process_passage_text(items, structure_analysis, cross_page_map)
            elif data_type == DataType.FORMULA_MATH:
                results = await self._process_formula_math(items, structure_analysis, cross_page_map)
            else:
                results = await self._process_default(items, structure_analysis, cross_page_map)
            
            processed_results[data_type] = results
            print(f"   âœ… {data_type.value} ì™„ë£Œ: {len(results)}ê°œ ê²°ê³¼")
        
        return processed_results
    
    async def _process_text_questions(self, items: List[Dict], structure_analysis: Dict, cross_page_map: Dict) -> List[Dict]:
        """í…ìŠ¤íŠ¸ ë¬¸ì œ íŠ¹í™” ì²˜ë¦¬ - ë†’ì€ OCR ì •í™•ë„ ìš°ì„ """
        
        results = []
        pages_info = {p["page_number"]: p for p in structure_analysis["pages_info"]}
        
        for item in items:
            page_num = item["page_number"]
            page_info = pages_info[page_num]
            
            # í¬ë¡œìŠ¤ í˜ì´ì§€ ì—°ê²° í™•ì¸
            cross_page_connection = self._find_cross_page_connection(item, cross_page_map)
            
            # ê°œë³„ í˜ì´ì§€ ê³ ì •ë°€ ì²˜ë¦¬
            result = await self._extract_text_question_precise(page_info, item, cross_page_connection)
            if result:
                results.append(result)
        
        return results
    
    async def _extract_text_question_precise(self, page_info: Dict, item: Dict, cross_page_connection: Optional[Dict]) -> Optional[Dict]:
        """í…ìŠ¤íŠ¸ ë¬¸ì œ ì •ë°€ ì¶”ì¶œ"""
        
        with open(page_info["image_path"], "rb") as image_file:
            image_data = base64.b64encode(image_file.read()).decode('utf-8')
        
        # í…ìŠ¤íŠ¸ ë¬¸ì œ íŠ¹í™” í”„ë¡¬í”„íŠ¸
        prompt = f"""í˜ì´ì§€ì—ì„œ {item['question_number']}ë²ˆ ë¬¸ì œë¥¼ ì •ë°€í•˜ê²Œ ì¶”ì¶œí•´ ì£¼ì„¸ìš”.

**í…ìŠ¤íŠ¸ ë¬¸ì œ ì „ìš© ì²˜ë¦¬ ê·œì¹™:**
- í•œê¸€ í…ìŠ¤íŠ¸ ì •í™•ë„ ìµœìš°ì„ 
- ì„ íƒì§€ ë§ˆì»¤ â‘ â‘¡â‘¢â‘£ ì •í™• ì¸ì‹
- ë°›ì¹¨, ëª¨ìŒ êµ¬ë¶„ ì •ë°€ ì²˜ë¦¬
- ë¬¸ì¥ ë¶€í˜¸, ê´„í˜¸ ì •í™• ë³´ì¡´

{self._get_cross_page_instruction(cross_page_connection)}

JSON ì‘ë‹µ:
{{
  "question_number": {item['question_number']},
  "question_text": "ë¬¸ì œ ë‚´ìš©",
  "choices": [
    {{"marker": "â‘ ", "content": "ì„ íƒì§€ 1"}},
    {{"marker": "â‘¡", "content": "ì„ íƒì§€ 2"}},
    {{"marker": "â‘¢", "content": "ì„ íƒì§€ 3"}},
    {{"marker": "â‘£", "content": "ì„ íƒì§€ 4"}}
  ],
  "passage": "ë³´ê¸° ë‚´ìš© (ìˆëŠ” ê²½ìš°)"
}}"""

        try:
            response = await self.openai_client.chat.completions.create(
                model="gpt-4o",
                messages=[
                    {
                        "role": "system",
                        "content": "í…ìŠ¤íŠ¸ ë¬¸ì œ ì „ìš© OCR ì „ë¬¸ê°€. í•œê¸€ í…ìŠ¤íŠ¸ì™€ ì„ íƒì§€ ë§ˆì»¤ì˜ ì •í™•ë„ê°€ ìµœìš°ì„ ì…ë‹ˆë‹¤."
                    },
                    {
                        "role": "user",
                        "content": [
                            {"type": "text", "text": prompt},
                            {
                                "type": "image_url",
                                "image_url": {
                                    "url": f"data:image/png;base64,{image_data}",
                                    "detail": "high"
                                }
                            }
                        ]
                    }
                ],
                max_tokens=4000,
                temperature=0.05  # ë§¤ìš° ë‚®ì€ ì˜¨ë„ë¡œ ì •í™•ë„ ê·¹ëŒ€í™”
            )
            
            response_text = response.choices[0].message.content
            result = self._parse_question_json(response_text)
            
            if result:
                result["data_type"] = DataType.TEXT_QUESTION.value
                result["processing_method"] = "text_specialized"
                return result
                
        except Exception as e:
            print(f"      âŒ í…ìŠ¤íŠ¸ ë¬¸ì œ {item['question_number']}ë²ˆ ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
        
        return None
    
    async def _process_table_data(self, items: List[Dict], structure_analysis: Dict, cross_page_map: Dict) -> List[Dict]:
        """í‘œ ë°ì´í„° íŠ¹í™” ì²˜ë¦¬ - ë°ì´í„° ë¬´ê²°ì„± + ì‹œê°ì  ë³´ì¡´"""
        
        results = []
        pages_info = {p["page_number"]: p for p in structure_analysis["pages_info"]}
        assets_dir = Path(structure_analysis["assets_dir"])
        
        for item in items:
            page_num = item["page_number"]
            page_info = pages_info[page_num]
            
            # í¬ë¡œìŠ¤ í˜ì´ì§€ ì—°ê²° í™•ì¸ (í‘œëŠ” ë§¤ìš° ì¤‘ìš”)
            cross_page_connection = self._find_cross_page_connection(item, cross_page_map)
            
            # í‘œ ì „ìš© ì²˜ë¦¬
            result = await self._extract_table_data_specialized(page_info, item, cross_page_connection, assets_dir)
            if result:
                results.append(result)
        
        return results
    
    async def _extract_table_data_specialized(self, page_info: Dict, item: Dict, cross_page_connection: Optional[Dict], assets_dir: Path) -> Optional[Dict]:
        """í‘œ ë°ì´í„° íŠ¹í™” ì¶”ì¶œ"""
        
        with open(page_info["image_path"], "rb") as image_file:
            image_data = base64.b64encode(image_file.read()).decode('utf-8')
        
        # í‘œ ë°ì´í„° ì „ìš© í”„ë¡¬í”„íŠ¸
        prompt = f"""í˜ì´ì§€ì—ì„œ {item['question_number']}ë²ˆ ë¬¸ì œì˜ í‘œ ë°ì´í„°ë¥¼ ì™„ë²½í•˜ê²Œ ì¶”ì¶œí•´ ì£¼ì„¸ìš”.

**í‘œ ë°ì´í„° ì „ìš© ì²˜ë¦¬ ê·œì¹™:**
- ìˆ«ì ì •í™•ë„ ìµœìš°ì„ : 2â‰ 3, 0â‰ O, 1â‰ l, 5â‰ S, 8â‰ B, 6â‰ 9
- í‘œ êµ¬ì¡° ì™„ì „ ë³´ì¡´: í—¤ë”, ëª¨ë“  ë°ì´í„° í–‰ í¬í•¨
- ì…€ ê²½ê³„ ì •í™• ì¸ì‹
- í”„ë¡œì„¸ìŠ¤ëª… (P1, P2, P3) ì •í™• ì¶”ì¶œ
- ì‹œê°„ ë°ì´í„° (ìˆ«ì) ì •í™• ì¶”ì¶œ

{self._get_cross_page_instruction(cross_page_connection)}

**ì¤‘ìš”**: í‘œëŠ” ë§ˆí¬ë‹¤ìš´ í˜•ì‹ + ì›ë³¸ ì´ë¯¸ì§€ ë‘˜ ë‹¤ ì œê³µ

JSON ì‘ë‹µ:
{{
  "question_number": {item['question_number']},
  "question_text": "ë¬¸ì œ ë‚´ìš©",
  "choices": [
    {{"marker": "â‘ ", "content": "ì„ íƒì§€ 1"}},
    {{"marker": "â‘¡", "content": "ì„ íƒì§€ 2"}},
    {{"marker": "â‘¢", "content": "ì„ íƒì§€ 3"}},
    {{"marker": "â‘£", "content": "ì„ íƒì§€ 4"}}
  ],
  "table_markdown": "| í”„ë¡œì„¸ìŠ¤ | ë„ì°©ì‹œê°„ | ì‹¤í–‰ì‹œê°„ |\\n|---------|--------|--------|\\n| P1 | 0 | 3 |\\n| P2 | 1 | 4 |",
  "table_data": [
    {{"process": "P1", "arrival_time": 0, "execution_time": 3}},
    {{"process": "P2", "arrival_time": 1, "execution_time": 4}}
  ],
  "passage": "í‘œ ì„¤ëª… í…ìŠ¤íŠ¸",
  "requires_image_extraction": true
}}"""

        try:
            response = await self.openai_client.chat.completions.create(
                model="gpt-4o",
                messages=[
                    {
                        "role": "system",
                        "content": "í‘œ ë°ì´í„° ì „ìš© ì¶”ì¶œ ì „ë¬¸ê°€. ìˆ«ìì™€ í‘œ êµ¬ì¡°ì˜ ì •í™•ë„ê°€ ìµœìš°ì„ ì´ë©°, ì ˆëŒ€ë¡œ ë°ì´í„°ë¥¼ ì¶”ì¸¡í•˜ê±°ë‚˜ ìƒì„±í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤."
                    },
                    {
                        "role": "user",
                        "content": [
                            {"type": "text", "text": prompt},
                            {
                                "type": "image_url",
                                "image_url": {
                                    "url": f"data:image/png;base64,{image_data}",
                                    "detail": "high"
                                }
                            }
                        ]
                    }
                ],
                max_tokens=4000,
                temperature=0.01  # í‘œ ë°ì´í„°ëŠ” ê·¹ë„ë¡œ ë‚®ì€ ì˜¨ë„
            )
            
            response_text = response.choices[0].message.content
            result = self._parse_question_json(response_text)
            
            if result:
                result["data_type"] = DataType.TABLE_DATA.value
                result["processing_method"] = "table_specialized"
                
                # í‘œ ì´ë¯¸ì§€ ë³„ë„ ì¶”ì¶œ
                if result.get("requires_image_extraction"):
                    table_image_path = await self._extract_table_image(page_info["image_path"], item, assets_dir)
                    if table_image_path:
                        result["table_image_path"] = table_image_path
                
                return result
                
        except Exception as e:
            print(f"      âŒ í‘œ ë°ì´í„° {item['question_number']}ë²ˆ ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
        
        return None
    
    async def _extract_table_image(self, page_image_path: str, item: Dict, assets_dir: Path) -> Optional[str]:
        """í‘œ ì˜ì—­ë§Œ ë³„ë„ ì´ë¯¸ì§€ë¡œ ì¶”ì¶œ"""
        
        try:
            # í˜ì´ì§€ ì´ë¯¸ì§€ì—ì„œ í‘œ ì˜ì—­ ì¶”ì •í•˜ì—¬ crop
            page_image = Image.open(page_image_path)
            
            # ë¬¸ì œ ìœ„ì¹˜ ê¸°ë°˜ìœ¼ë¡œ í‘œ ì˜ì—­ ì¶”ì •
            estimated_y = item.get("estimated_position", 0)
            if estimated_y > 0:
                # í‘œ ì˜ì—­ ì¶”ì • (ë¬¸ì œ ìœ„ì¹˜ì—ì„œ +200~+600í”½ì…€ ì •ë„)
                crop_area = (0, max(0, estimated_y), page_image.width, min(page_image.height, estimated_y + 400))
                table_image = page_image.crop(crop_area)
                
                # ì €ì¥
                table_path = assets_dir / f"q{item['question_number']}_table.png"
                table_image.save(table_path)
                
                return str(table_path)
        except Exception as e:
            print(f"      âš ï¸ í‘œ ì´ë¯¸ì§€ ì¶”ì¶œ ì‹¤íŒ¨: {e}")
        
        return None
    
    async def _process_code_blocks(self, items: List[Dict], structure_analysis: Dict, cross_page_map: Dict) -> List[Dict]:
        """ì½”ë“œ ë¸”ë¡ íŠ¹í™” ì²˜ë¦¬ - ë¬¸ë²• ë³´ì¡´ + ë“¤ì—¬ì“°ê¸° ì •í™•ë„"""
        
        results = []
        pages_info = {p["page_number"]: p for p in structure_analysis["pages_info"]}
        assets_dir = Path(structure_analysis["assets_dir"])
        
        for item in items:
            page_num = item["page_number"]
            page_info = pages_info[page_num]
            
            cross_page_connection = self._find_cross_page_connection(item, cross_page_map)
            result = await self._extract_code_block_specialized(page_info, item, cross_page_connection, assets_dir)
            if result:
                results.append(result)
        
        return results
    
    async def _extract_code_block_specialized(self, page_info: Dict, item: Dict, cross_page_connection: Optional[Dict], assets_dir: Path) -> Optional[Dict]:
        """ì½”ë“œ ë¸”ë¡ íŠ¹í™” ì¶”ì¶œ"""
        
        with open(page_info["image_path"], "rb") as image_file:
            image_data = base64.b64encode(image_file.read()).decode('utf-8')
        
        # ì½”ë“œ ë¸”ë¡ ì „ìš© í”„ë¡¬í”„íŠ¸
        cross_page_instruction = self._get_cross_page_instruction(cross_page_connection)
        
        prompt = f"""í˜ì´ì§€ì—ì„œ {item['question_number']}ë²ˆ ë¬¸ì œì˜ ì½”ë“œ ë¸”ë¡ì„ ì™„ë²½í•˜ê²Œ ì¶”ì¶œí•´ ì£¼ì„¸ìš”.

**ì½”ë“œ ë¸”ë¡ ì „ìš© ì²˜ë¦¬ ê·œì¹™:**
- ë¬¸ë²• ì •í™•ë„ ìµœìš°ì„ 
- ë“¤ì—¬ì“°ê¸° ì™„ì „ ë³´ì¡´ (ê³µë°±, íƒ­ êµ¬ë¶„)
- í‚¤ì›Œë“œ ì •í™• ì¸ì‹: class, public, void, main, String, int, for, if, while
- ê¸°í˜¸ ì •í™• ì¸ì‹: {{}}, (), [], ;, =, !=, <=, >=
- ë³€ìˆ˜ëª…/í•¨ìˆ˜ëª… ëŒ€ì†Œë¬¸ì ì •í™• ë³´ì¡´
- ì£¼ì„ (// /* */) ì •í™• ì¸ì‹
- ë¬¸ìì—´ ë”°ì˜´í‘œ ("", '', ``) ì •í™• ì¸ì‹

{cross_page_instruction}

JSON ì‘ë‹µ:
{{
  "question_number": {item['question_number']},
  "question_text": "ë¬¸ì œ ë‚´ìš©",
  "choices": [
    {{"marker": "â‘ ", "content": "ì„ íƒì§€ 1"}},
    {{"marker": "â‘¡", "content": "ì„ íƒì§€ 2"}},
    {{"marker": "â‘¢", "content": "ì„ íƒì§€ 3"}},
    {{"marker": "â‘£", "content": "ì„ íƒì§€ 4"}}
  ],
  "code_block": {{
    "language": "java",
    "code": "public class Example {{\\n    public static void main(String[] args) {{\\n        int result = 0;\\n        System.out.println(result);\\n    }}\\n}}",
    "line_numbers": true
  }},
  "passage": "ì½”ë“œ ì„¤ëª… í…ìŠ¤íŠ¸",
  "requires_image_extraction": true
}}"""

        try:
            response = await self.openai_client.chat.completions.create(
                model="gpt-4o",
                messages=[
                    {
                        "role": "system",
                        "content": "ì½”ë“œ ë¸”ë¡ ì „ìš© ì¶”ì¶œ ì „ë¬¸ê°€. í”„ë¡œê·¸ë˜ë° ë¬¸ë²•ê³¼ ë“¤ì—¬ì“°ê¸°ì˜ ì •í™•ë„ê°€ ìµœìš°ì„ ì´ë©°, ì½”ë“œë¥¼ ìˆ˜ì •í•˜ê±°ë‚˜ ì¶”ì¸¡í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤."
                    },
                    {
                        "role": "user",
                        "content": [
                            {"type": "text", "text": prompt},
                            {
                                "type": "image_url",
                                "image_url": {
                                    "url": f"data:image/png;base64,{image_data}",
                                    "detail": "high"
                                }
                            }
                        ]
                    }
                ],
                max_tokens=4000,
                temperature=0.02  # ì½”ë“œëŠ” ë§¤ìš° ë‚®ì€ ì˜¨ë„
            )
            
            response_text = response.choices[0].message.content
            result = self._parse_question_json(response_text)
            
            if result:
                result["data_type"] = DataType.CODE_BLOCK.value
                result["processing_method"] = "code_specialized"
                
                # ì½”ë“œ ì´ë¯¸ì§€ ë³„ë„ ì¶”ì¶œ
                if result.get("requires_image_extraction"):
                    code_image_path = await self._extract_code_image(page_info["image_path"], item, assets_dir)
                    if code_image_path:
                        result["code_image_path"] = code_image_path
                
                return result
                
        except Exception as e:
            print(f"      âŒ ì½”ë“œ ë¸”ë¡ {item['question_number']}ë²ˆ ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
        
        return None
    
    async def _extract_code_image(self, page_image_path: str, item: Dict, assets_dir: Path) -> Optional[str]:
        """ì½”ë“œ ì˜ì—­ë§Œ ë³„ë„ ì´ë¯¸ì§€ë¡œ ì¶”ì¶œ"""
        
        try:
            page_image = Image.open(page_image_path)
            estimated_y = item.get("estimated_position", 0)
            
            if estimated_y > 0:
                # ì½”ë“œ ë¸”ë¡ ì˜ì—­ ì¶”ì •
                crop_area = (0, max(0, estimated_y), page_image.width, min(page_image.height, estimated_y + 500))
                code_image = page_image.crop(crop_area)
                
                code_path = assets_dir / f"q{item['question_number']}_code.png"
                code_image.save(code_path)
                
                return str(code_path)
        except Exception as e:
            print(f"      âš ï¸ ì½”ë“œ ì´ë¯¸ì§€ ì¶”ì¶œ ì‹¤íŒ¨: {e}")
        
        return None
    
    async def _process_diagram_images(self, items: List[Dict], structure_analysis: Dict, cross_page_map: Dict) -> List[Dict]:
        """ë‹¤ì´ì–´ê·¸ë¨ ì´ë¯¸ì§€ íŠ¹í™” ì²˜ë¦¬ - ì‹œê°ì  ì •ë³´ ë³´ì¡´"""
        # ë‹¤ì´ì–´ê·¸ë¨ì€ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜í•˜ì§€ ì•Šê³  ì´ë¯¸ì§€ ê·¸ëŒ€ë¡œ ë³´ì¡´
        return await self._process_image_based_elements(items, structure_analysis, "diagram")
    
    async def _process_choice_images(self, items: List[Dict], structure_analysis: Dict, cross_page_map: Dict) -> List[Dict]:
        """ì„ íƒì§€ ì´ë¯¸ì§€ íŠ¹í™” ì²˜ë¦¬ - ê° ì„ íƒì§€ë³„ ì´ë¯¸ì§€ ë¶„ë¦¬"""
        return await self._process_image_based_elements(items, structure_analysis, "choice_image")
    
    async def _process_image_based_elements(self, items: List[Dict], structure_analysis: Dict, element_type: str) -> List[Dict]:
        """ì´ë¯¸ì§€ ê¸°ë°˜ ìš”ì†Œ ì²˜ë¦¬"""
        
        results = []
        pages_info = {p["page_number"]: p for p in structure_analysis["pages_info"]}
        assets_dir = Path(structure_analysis["assets_dir"])
        
        for item in items:
            page_info = pages_info[item["page_number"]]
            
            # ì´ë¯¸ì§€ ì¶”ì¶œ ë° ì €ì¥
            image_path = await self._extract_element_image(page_info["image_path"], item, assets_dir, element_type)
            
            # ê¸°ë³¸ ë¬¸ì œ ì •ë³´ ì¶”ì¶œ
            basic_result = await self._extract_basic_question_info(page_info, item)
            
            if basic_result and image_path:
                basic_result["data_type"] = element_type
                basic_result["image_path"] = image_path
                basic_result["processing_method"] = f"{element_type}_specialized"
                results.append(basic_result)
        
        return results
    
    async def _extract_element_image(self, page_image_path: str, item: Dict, assets_dir: Path, element_type: str) -> Optional[str]:
        """ìš”ì†Œë³„ ì´ë¯¸ì§€ ì¶”ì¶œ"""
        
        try:
            page_image = Image.open(page_image_path)
            estimated_y = item.get("estimated_position", 0)
            
            if estimated_y > 0:
                crop_area = (0, max(0, estimated_y - 100), page_image.width, min(page_image.height, estimated_y + 600))
                element_image = page_image.crop(crop_area)
                
                element_path = assets_dir / f"q{item['question_number']}_{element_type}.png"
                element_image.save(element_path)
                
                return str(element_path)
        except Exception as e:
            print(f"      âš ï¸ {element_type} ì´ë¯¸ì§€ ì¶”ì¶œ ì‹¤íŒ¨: {e}")
        
        return None
    
    async def _extract_basic_question_info(self, page_info: Dict, item: Dict) -> Optional[Dict]:
        """ê¸°ë³¸ ë¬¸ì œ ì •ë³´ ì¶”ì¶œ (ì´ë¯¸ì§€ ê¸°ë°˜ ìš”ì†Œìš©)"""
        
        with open(page_info["image_path"], "rb") as image_file:
            image_data = base64.b64encode(image_file.read()).decode('utf-8')
        
        prompt = f"""í˜ì´ì§€ì—ì„œ {item['question_number']}ë²ˆ ë¬¸ì œì˜ ê¸°ë³¸ ì •ë³´ë¥¼ ì¶”ì¶œí•´ ì£¼ì„¸ìš”. 
ì´ë¯¸ì§€/ë‹¤ì´ì–´ê·¸ë¨ ìš”ì†ŒëŠ” ë³„ë„ ì²˜ë¦¬ë˜ë¯€ë¡œ í…ìŠ¤íŠ¸ ë¶€ë¶„ë§Œ ì •í™•íˆ ì¶”ì¶œí•˜ì„¸ìš”.

JSON ì‘ë‹µ:
{{
  "question_number": {item['question_number']},
  "question_text": "ë¬¸ì œ í…ìŠ¤íŠ¸ ë¶€ë¶„ë§Œ",
  "choices": [
    {{"marker": "â‘ ", "content": "ì„ íƒì§€ 1 (ì´ë¯¸ì§€ì¸ ê²½ìš° 'ì´ë¯¸ì§€ ì„ íƒì§€'ë¡œ í‘œì‹œ)"}},
    {{"marker": "â‘¡", "content": "ì„ íƒì§€ 2"}},
    {{"marker": "â‘¢", "content": "ì„ íƒì§€ 3"}},
    {{"marker": "â‘£", "content": "ì„ íƒì§€ 4"}}
  ]
}}"""

        try:
            response = await self.openai_client.chat.completions.create(
                model="gpt-4o",
                messages=[
                    {"role": "system", "content": "ì´ë¯¸ì§€ ê¸°ë°˜ ë¬¸ì œì˜ í…ìŠ¤íŠ¸ ë¶€ë¶„ë§Œ ì¶”ì¶œí•˜ëŠ” ì „ë¬¸ê°€ì…ë‹ˆë‹¤."},
                    {
                        "role": "user",
                        "content": [
                            {"type": "text", "text": prompt},
                            {"type": "image_url", "image_url": {"url": f"data:image/png;base64,{image_data}", "detail": "high"}}
                        ]
                    }
                ],
                max_tokens=2000,
                temperature=0.1
            )
            
            return self._parse_question_json(response.choices[0].message.content)
            
        except Exception as e:
            print(f"      âŒ ê¸°ë³¸ ì •ë³´ ì¶”ì¶œ ì‹¤íŒ¨: {e}")
        
        return None
    
    async def _process_mixed_content(self, items: List[Dict], structure_analysis: Dict, cross_page_map: Dict) -> List[Dict]:
        """í˜¼í•© ì½˜í…ì¸  ì²˜ë¦¬"""
        # í…ìŠ¤íŠ¸ì™€ ì´ë¯¸ì§€ê°€ ì„ì¸ ê²½ìš°ì˜ íŠ¹ë³„ ì²˜ë¦¬
        return await self._process_default(items, structure_analysis, cross_page_map)
    
    async def _process_passage_text(self, items: List[Dict], structure_analysis: Dict, cross_page_map: Dict) -> List[Dict]:
        """ê¸´ ì§€ë¬¸ ì²˜ë¦¬"""
        # ê¸´ í…ìŠ¤íŠ¸ì˜ ë§¥ë½ê³¼ êµ¬ì¡° ë³´ì¡´ì— íŠ¹í™”
        return await self._process_default(items, structure_analysis, cross_page_map)
    
    async def _process_formula_math(self, items: List[Dict], structure_analysis: Dict, cross_page_map: Dict) -> List[Dict]:
        """ìˆ˜ì‹ ì²˜ë¦¬"""
        # ìˆ˜í•™ ê¸°í˜¸ì™€ ìˆ˜ì‹ êµ¬ì¡° ì •í™•ë„ì— íŠ¹í™”
        return await self._process_default(items, structure_analysis, cross_page_map)
    
    async def _process_default(self, items: List[Dict], structure_analysis: Dict, cross_page_map: Dict) -> List[Dict]:
        """ê¸°ë³¸ ì²˜ë¦¬ (íŠ¹í™”ë˜ì§€ ì•Šì€ íƒ€ì…ë“¤)"""
        
        results = []
        pages_info = {p["page_number"]: p for p in structure_analysis["pages_info"]}
        
        for item in items:
            page_info = pages_info[item["page_number"]]
            result = await self._extract_basic_question_info(page_info, item)
            if result:
                result["data_type"] = item["data_type"].value
                result["processing_method"] = "default"
                results.append(result)
        
        return results
    
    def _find_cross_page_connection(self, item: Dict, cross_page_map: Dict) -> Optional[Dict]:
        """íŠ¹ì • ì•„ì´í…œì˜ í¬ë¡œìŠ¤ í˜ì´ì§€ ì—°ê²° ì°¾ê¸°"""
        
        connections = cross_page_map.get("connections", [])
        item_page = item["page_number"]
        
        for connection in connections:
            if connection["from_page"] == item_page or connection["to_page"] == item_page:
                return connection
        
        return None
    
    def _get_cross_page_instruction(self, connection: Optional[Dict]) -> str:
        """í¬ë¡œìŠ¤ í˜ì´ì§€ ì—°ê²°ì— ëŒ€í•œ ì¶”ê°€ ì§€ì‹œì‚¬í•­"""
        
        if not connection:
            return ""
        
        instructions = ["**í¬ë¡œìŠ¤ í˜ì´ì§€ ì—°ê²° ê°ì§€ë¨:**"]
        
        if "incomplete_choices" in connection.get("connection_type", []):
            instructions.append("- ì˜ë¦° ì„ íƒì§€ê°€ ë‹¤ìŒ í˜ì´ì§€ì— ì—°ê²°ë  ìˆ˜ ìˆìŒ")
        if "split_table" in connection.get("connection_type", []):
            instructions.append("- í‘œ ë°ì´í„°ê°€ ë‹¤ìŒ í˜ì´ì§€ì— ê³„ì†ë  ìˆ˜ ìˆìŒ")
        if "continued_code" in connection.get("connection_type", []):
            instructions.append("- ì½”ë“œ ë¸”ë¡ì´ ë‹¤ìŒ í˜ì´ì§€ì— ê³„ì†ë  ìˆ˜ ìˆìŒ")
        
        return "\n".join(instructions)
    
    def _parse_question_json(self, response_text: str) -> Optional[Dict]:
        """JSON ì‘ë‹µ íŒŒì‹±"""
        try:
            # ```json ... ``` ë¸”ë¡ì—ì„œ JSON ì¶”ì¶œ
            json_match = re.search(r'```json\s*(.*?)\s*```', response_text, re.DOTALL)
            if json_match:
                json_str = json_match.group(1)
            else:
                # ì§ì ‘ JSON ì°¾ê¸°
                json_match = re.search(r'\{.*\}', response_text, re.DOTALL)
                if json_match:
                    json_str = json_match.group(0)
                else:
                    json_str = response_text
            
            return json.loads(json_str)
        except json.JSONDecodeError as e:
            print(f"JSON íŒŒì‹± ì˜¤ë¥˜: {e}")
            return None
    
    def _unify_processed_elements(self, processed_results: Dict[str, Any]) -> List[Dict]:
        """4ë‹¨ê³„: ê° ë°ì´í„° íƒ€ì…ë³„ ì²˜ë¦¬ ê²°ê³¼ë¥¼ í†µí•©"""
        
        all_questions = []
        
        for data_type, results in processed_results.items():
            for result in results:
                all_questions.append(result)
        
        # ë¬¸ì œ ë²ˆí˜¸ìˆœìœ¼ë¡œ ì •ë ¬
        all_questions.sort(key=lambda q: q.get("question_number", 999))
        
        print(f"   ğŸ”§ í†µí•©ëœ ë¬¸ì œ: {len(all_questions)}ê°œ")
        
        return all_questions
    
    async def _quality_check_and_optimize(self, questions: List[Dict], structure_analysis: Dict) -> List[Dict]:
        """5ë‹¨ê³„: í’ˆì§ˆ ê²€ì¦ ë° ìµœì í™”"""
        
        print("   âœ… í’ˆì§ˆ ê²€ì¦ ì‹œì‘...")
        
        # ì¤‘ë³µ ì œê±°
        unique_questions = []
        seen_numbers = set()
        
        for q in questions:
            q_num = q.get("question_number")
            if q_num and q_num not in seen_numbers:
                seen_numbers.add(q_num)
                unique_questions.append(q)
        
        # ëˆ„ë½ í™•ì¸
        expected_numbers = set(range(1, 61))
        found_numbers = {q.get("question_number") for q in unique_questions}
        missing = expected_numbers - found_numbers
        
        if missing:
            print(f"   âš ï¸ ëˆ„ë½ëœ ë¬¸ì œ: {sorted(missing)}")
        
        print(f"   ğŸ“Š ìµœì¢… í’ˆì§ˆ ê²€ì¦ ì™„ë£Œ: {len(unique_questions)}ê°œ ë¬¸ì œ")
        
        return unique_questions
    
    def _generate_processing_stats(self, processed_results: Dict[str, Any]) -> Dict[str, Any]:
        """ì²˜ë¦¬ í†µê³„ ìƒì„±"""
        
        stats = {
            "data_type_counts": {},
            "total_processed": 0,
            "processing_methods": {}
        }
        
        for data_type, results in processed_results.items():
            type_name = data_type.value if hasattr(data_type, 'value') else str(data_type)
            count = len(results)
            
            stats["data_type_counts"][type_name] = count
            stats["total_processed"] += count
            
            # ì²˜ë¦¬ ë°©ë²•ë³„ í†µê³„
            for result in results:
                method = result.get("processing_method", "unknown")
                if method not in stats["processing_methods"]:
                    stats["processing_methods"][method] = 0
                stats["processing_methods"][method] += 1
        
        return stats