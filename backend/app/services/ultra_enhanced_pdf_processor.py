"""
Ultra Enhanced PDF Processor - ê·¼ë³¸ì  ê°œì„  ë²„ì „
GPT ì œì•ˆì‚¬í•­ì„ ë°˜ì˜í•œ Multi-Stage Processing Pipeline
"""

import json
import asyncio
import re
from typing import List, Dict, Any, Optional, Tuple
import hashlib

class UltraEnhancedPDFProcessor:
    def __init__(self, claude_client):
        self.claude_client = claude_client
        
    async def process_chunks_ultra(self, chunks: List[str]) -> Dict[str, Any]:
        """
        ë©€í‹°ìŠ¤í…Œì´ì§€ ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸:
        1. Layout Analysis (í˜ì´ì§€ ìœ í˜• ë¶„ì„)
        2. Content Extraction (ë‚´ìš© ì¶”ì¶œ)
        3. Cross-page Linking (í˜ì´ì§€ê°„ ì—°ê²° ì²˜ë¦¬)
        4. Quality Validation (í’ˆì§ˆ ê²€ì¦)
        5. Deduplication (ì¤‘ë³µ ì œê±°)
        """
        
        all_questions = []
        processing_metadata = []
        
        print(f"\nğŸ”¥ Ultra Enhanced Processing Pipeline ì‹œì‘")
        print(f"ğŸ“Š ì´ {len(chunks)}ê°œ ì²­í¬ ì²˜ë¦¬ ì˜ˆì •")
        
        # Stage 1: Layout Analysis for all chunks
        print(f"\n=== Stage 1: Layout Analysis ===")
        layout_results = await self._analyze_layouts(chunks)
        
        # Stage 2: Content Extraction (ì§ˆ ë†’ì€ í˜ì´ì§€ë§Œ)
        print(f"\n=== Stage 2: Content Extraction ===")
        for chunk_idx, (chunk, layout_result) in enumerate(zip(chunks, layout_results)):
            if layout_result.get("page_type") not in ["questions_only", "mixed"]:
                print(f"âš ï¸ Skipping chunk {chunk_idx+1}: {layout_result.get('page_type')}")
                continue
            
            questions = await self._extract_questions_from_chunk(chunk_idx, chunk, layout_result)
            all_questions.extend(questions)
            processing_metadata.append({
                "chunk_idx": chunk_idx,
                "layout": layout_result,
                "questions_extracted": len(questions)
            })
        
        print(f"\n=== Stage 3: Cross-page Linking ===")
        # Stage 3: Handle cross-page questions
        linked_questions = await self._handle_cross_page_questions(all_questions, chunks)
        
        print(f"\n=== Stage 4: Quality Validation ===")
        # Stage 4: Comprehensive validation
        validated_questions = await self._comprehensive_validation(linked_questions)
        
        print(f"\n=== Stage 5: Deduplication ===")
        # Stage 5: Advanced deduplication
        final_questions = await self._advanced_deduplication(validated_questions)
        
        # Final report
        print(f"\nğŸ“Š Ultra Enhanced Processing ì™„ë£Œ:")
        print(f"  - ì›ë³¸ ì²­í¬: {len(chunks)}ê°œ")
        print(f"  - ì²˜ë¦¬ëœ ì²­í¬: {len([r for r in layout_results if r.get('page_type') in ['questions_only', 'mixed']])}ê°œ")
        print(f"  - ì¶”ì¶œëœ ë¬¸ì œ: {len(all_questions)}ê°œ")
        print(f"  - ì—°ê²° ì²˜ë¦¬ í›„: {len(linked_questions)}ê°œ")
        print(f"  - ê²€ì¦ í†µê³¼: {len(validated_questions)}ê°œ")
        print(f"  - ìµœì¢… ë¬¸ì œ: {len(final_questions)}ê°œ")
        
        return {
            "questions": final_questions,
            "processing_metadata": processing_metadata,
            "stage_stats": {
                "raw_extraction": len(all_questions),
                "cross_page_linking": len(linked_questions),
                "validation_passed": len(validated_questions),
                "final_count": len(final_questions)
            }
        }
    
    async def _analyze_layouts(self, chunks: List[str]) -> List[Dict[str, Any]]:
        """Stage 1: ëª¨ë“  ì²­í¬ì˜ ë ˆì´ì•„ì›ƒ ë¶„ì„"""
        layout_results = []
        
        for idx, chunk in enumerate(chunks):
            print(f"ğŸ” Layout analysis {idx+1}/{len(chunks)}")
            
            layout_prompt = f"""ë‹¹ì‹ ì€ PDF í˜ì´ì§€ ë ˆì´ì•„ì›ƒ ë¶„ì„ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ë‹¤ìŒ ë§ˆí¬ë‹¤ìš´ í…ìŠ¤íŠ¸ë¥¼ ë¶„ì„í•˜ì—¬ í˜ì´ì§€ì˜ êµ¬ì¡°ì™€ íŠ¹ì„±ì„ íŒŒì•…í•´ì£¼ì„¸ìš”:

```markdown
{chunk[:800] if len(chunk) > 800 else chunk}
```

**ë¶„ì„ ê¸°ì¤€**:
1. **page_type íŒë³„**:
   - questions_only: ë¬¸ì œ ë²ˆí˜¸(1, 2, 3...)ì™€ ì„ íƒì§€(â‘ â‘¡â‘¢â‘£)ê°€ ëª…í™•í•œ ì‹œí—˜ ë¬¸ì œ í˜ì´ì§€
   - answers_only: "í•´ì„¤", "ì •ë‹µ", "í’€ì´", "ë‹µì•ˆ"ì´ ì£¼ëœ ë‚´ìš©ì¸ í•´ì„¤ í˜ì´ì§€
   - mixed: ë¬¸ì œì™€ í•´ì„¤ì´ ì„ì¸ í˜ì´ì§€
   - empty: ì˜ë¯¸ìˆëŠ” ë‚´ìš©ì´ ê±°ì˜ ì—†ëŠ” ë¹ˆ í˜ì´ì§€
   - invalid: OCR ì˜¤ë¥˜ë¡œ ê¹¨ì§„ í…ìŠ¤íŠ¸ê°€ ë§ì€ í˜ì´ì§€

2. **íŠ¹ìˆ˜ ì½˜í…ì¸  ê°ì§€**:
   - í‘œ(table): "|" ë¬¸ìë‚˜ ì •ë ¬ëœ ë°ì´í„°ê°€ ìˆëŠ”ì§€
   - ê·¸ë¦¼(figure): "ê·¸ë¦¼", "ë„í‘œ", "Figure" ë“±ì˜ ì°¸ì¡°ê°€ ìˆëŠ”ì§€
   - ì½”ë“œ(code): í”„ë¡œê·¸ë˜ë° ì½”ë“œ ë¸”ë¡ì´ ìˆëŠ”ì§€

3. **í’ˆì§ˆ í‰ê°€**:
   - high: ì™„ì „í•œ ë¬¸ì œë“¤ì´ ê¹”ë”í•˜ê²Œ ì •ë¦¬ë¨
   - medium: ì¼ë¶€ ë¶ˆì™„ì „í•˜ê±°ë‚˜ ê¹¨ì§„ ë¶€ë¶„ ìˆìŒ
   - low: ëŒ€ë¶€ë¶„ ê¹¨ì§€ê±°ë‚˜ ì˜ë¯¸ë¥¼ íŒŒì•…í•˜ê¸° ì–´ë ¤ì›€

ë‹¤ìŒ JSON í˜•íƒœë¡œë§Œ ë‹µë³€í•˜ì„¸ìš”:
{{
  "page_type": "questions_only|answers_only|mixed|empty|invalid",
  "layout_style": "single_column|two_column|table_heavy|mixed",
  "question_count_estimate": 0,
  "has_tables": false,
  "has_figures": false,
  "has_code": false,
  "content_quality": "high|medium|low",
  "special_notes": "íŠ¹ì´ì‚¬í•­ ê°„ë‹¨íˆ"
}}"""

            try:
                response = await self.claude_client.messages.create(
                    model="claude-3-5-sonnet-20241022",
                    max_tokens=800,
                    temperature=0,
                    messages=[{"role": "user", "content": layout_prompt}]
                )
                
                result_text = response.content[0].text.strip()
                if result_text.startswith('```json'):
                    result_text = result_text[7:-3].strip()
                elif result_text.startswith('```'):
                    result_text = result_text[3:-3].strip()
                
                layout_result = json.loads(result_text)
                layout_results.append(layout_result)
                
                print(f"  âœ… Chunk {idx+1}: {layout_result.get('page_type')} - {layout_result.get('question_count_estimate')} questions")
                
            except Exception as e:
                print(f"  âŒ Layout analysis failed for chunk {idx+1}: {e}")
                layout_results.append({
                    "page_type": "invalid",
                    "content_quality": "low",
                    "question_count_estimate": 0
                })
            
            # API ê³¼ë¶€í•˜ ë°©ì§€
            await asyncio.sleep(0.5)
        
        return layout_results
    
    async def _extract_questions_from_chunk(self, chunk_idx: int, chunk: str, layout_result: Dict[str, Any]) -> List[Dict[str, Any]]:
        """Stage 2: ê°œë³„ ì²­í¬ì—ì„œ ë¬¸ì œ ì¶”ì¶œ (ë ˆì´ì•„ì›ƒ ë¶„ì„ ê²°ê³¼ ê¸°ë°˜)"""
        
        extraction_prompt = f"""ë‹¹ì‹ ì€ ì‹œí—˜ ë¬¸ì œ ì „ë¬¸ ì¶”ì¶œê¸°ì…ë‹ˆë‹¤. ë ˆì´ì•„ì›ƒ ë¶„ì„ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì´ í˜ì´ì§€ì—ì„œ ì™„ì „í•œ ë¬¸ì œë§Œì„ ì •í™•íˆ ì¶”ì¶œí•´ì£¼ì„¸ìš”.

**ë ˆì´ì•„ì›ƒ ë¶„ì„ ê²°ê³¼**: 
- í˜ì´ì§€ ìœ í˜•: {layout_result.get('page_type')}
- ì˜ˆìƒ ë¬¸ì œ ìˆ˜: {layout_result.get('question_count_estimate')}
- íŠ¹ìˆ˜ ì½˜í…ì¸ : í‘œ={layout_result.get('has_tables')}, ê·¸ë¦¼={layout_result.get('has_figures')}, ì½”ë“œ={layout_result.get('has_code')}

**ì²˜ë¦¬í•  ë§ˆí¬ë‹¤ìš´**:
```markdown
{chunk}
```

**ğŸ¯ ì •ë°€ ì¶”ì¶œ ê·œì¹™**:

1. **ì™„ì „í•œ ë¬¸ì œë§Œ ì¶”ì¶œ**:
   - ë¬¸ì œ ë²ˆí˜¸ê°€ ëª…í™• (1, 2, 3... ë˜ëŠ” 1), 2), 3)...)
   - ì™„ì „í•œ ì§ˆë¬¸ ë¬¸ì¥ ("~ì€?", "~ëŠ”?", "~ì¸ê°€?")
   - ìµœì†Œ 2ê°œ ì´ìƒì˜ ì™„ì „í•œ ì„ íƒì§€ (â‘ â‘¡â‘¢â‘£)

2. **í‘œ/ì½”ë“œ/ê·¸ë¦¼ íŠ¹ë³„ ì²˜ë¦¬**:
   - í‘œ: ì „ì²´ í‘œ êµ¬ì¡°ë¥¼ passageì— ë§ˆí¬ë‹¤ìš´ í…Œì´ë¸”ë¡œ í¬í•¨
   - ì½”ë“œ: ì›ë˜ ë“¤ì—¬ì“°ê¸°ì™€ í˜•ì‹ ë³´ì¡´
   - ê·¸ë¦¼ ì°¸ì¡°: "ë‹¤ìŒ ê·¸ë¦¼ì„ ë³´ê³ ..." ê°™ì€ ë‚´ìš©ë„ passageì— í¬í•¨

3. **ë¶ˆì™„ì „í•œ ë¬¸ì œ ê°ì§€**:
   - ì„ íƒì§€ê°€ ì¤‘ê°„ì— ëŠì–´ì§„ ê²½ìš°: special_handling = "incomplete_options"
   - ë‹¤ìŒ í˜ì´ì§€ë¡œ ì´ì–´ì§€ëŠ” ê²½ìš°: special_handling = "continues_next_page"
   - í‘œê°€ ë³µì¡í•´ ì œëŒ€ë¡œ íŒŒì‹± ì•ˆëœ ê²½ìš°: special_handling = "complex_table"

4. **ì ˆëŒ€ ì œì™¸ ëŒ€ìƒ**:
   - "í•´ì„¤:", "ì •ë‹µ:", "í’€ì´:" ë“±ì´ í¬í•¨ëœ í•´ì„¤ ë‚´ìš©
   - ì„ íƒì§€ê°€ ì—†ê±°ë‚˜ 1ê°œë¿ì¸ ê²ƒ
   - ë¬¸ì œ í…ìŠ¤íŠ¸ê°€ ë„ˆë¬´ ì§§ì€ ê²ƒ (10ì ë¯¸ë§Œ)
   - ê¹¨ì§„ OCR í…ìŠ¤íŠ¸

**JSON ì¶œë ¥ í˜•ì‹**:
{{
  "questions": [
    {{
      "question_id": "Q{chunk_idx+1:02d}_{idx:03d}",
      "question_number": 1,
      "question_text": "ë¬¸ì œ ë³¸ë¬¸ (ì§€ë¬¸ ì œì™¸)",
      "passage": "ì§€ë¬¸, í‘œ, ê·¸ë¦¼ ì°¸ì¡°, ì½”ë“œ ë“±",
      "options": ["â‘  ì„ íƒì§€1", "â‘¡ ì„ íƒì§€2", "â‘¢ ì„ íƒì§€3", "â‘£ ì„ íƒì§€4"],
      "has_table": true,
      "has_code": false,
      "has_figure": false,
      "special_handling": "none|incomplete_options|continues_next_page|complex_table",
      "confidence": "high|medium|low",
      "extraction_notes": "íŠ¹ì´ì‚¬í•­"
    }}
  ]
}}

**ì¤‘ìš”**: JSONë§Œ ì¶œë ¥í•˜ì„¸ìš”. í™•ì‹ ì´ ì—†ìœ¼ë©´ í•´ë‹¹ ë¬¸ì œëŠ” ì œì™¸í•˜ì„¸ìš”."""

        try:
            response = await self.claude_client.messages.create(
                model="claude-3-5-sonnet-20241022",
                max_tokens=4000,
                temperature=0,
                messages=[{"role": "user", "content": extraction_prompt}]
            )
            
            result_text = response.content[0].text.strip()
            if result_text.startswith('```json'):
                result_text = result_text[7:-3].strip()
            elif result_text.startswith('```'):
                result_text = result_text[3:-3].strip()
            
            result = json.loads(result_text)
            questions = result.get('questions', [])
            
            print(f"  âœ… Chunk {chunk_idx+1}: {len(questions)} questions extracted")
            for q in questions:
                special = q.get('special_handling', 'none')
                confidence = q.get('confidence', 'unknown')
                print(f"    - Q{q.get('question_number')}: {len(q.get('options', []))} options, {special}, {confidence}")
            
            return questions
            
        except Exception as e:
            print(f"  âŒ Question extraction failed for chunk {chunk_idx+1}: {e}")
            return []
    
    async def _handle_cross_page_questions(self, questions: List[Dict[str, Any]], chunks: List[str]) -> List[Dict[str, Any]]:
        """Stage 3: í˜ì´ì§€ê°„ ë¶„í• ëœ ë¬¸ì œë“¤ì„ ì—°ê²° ì²˜ë¦¬ (ì—°ì† ì´ë¯¸ì§€ ì²˜ë¦¬ ì§€ì›)"""
        
        # ì—°ì† ì²˜ë¦¬ì—ì„œ ì˜¤ëŠ” partial ë¬¸ì œë“¤ì„ ì°¾ê¸°
        partial_start_questions = []  # PARTIAL_STARTë¡œ í‘œì‹œëœ ë¬¸ì œë“¤
        partial_end_questions = []    # PARTIAL_ENDë¡œ í‘œì‹œëœ ë¬¸ì œë“¤
        complete_questions = []
        incomplete_questions = []
        
        for q in questions:
            question_text = q.get('question_text', '')
            question_id = q.get('question_id', '')
            
            # ì—°ì† ì²˜ë¦¬ì—ì„œ ì˜¤ëŠ” partial ë¬¸ì œë“¤ ê°ì§€
            if 'PARTIAL_START' in question_text or 'PARTIAL_START' in question_id:
                partial_start_questions.append(q)
                print(f"ğŸ”— Partial START found: {question_id}")
            elif 'PARTIAL_END' in question_text or 'PARTIAL_END' in question_id:
                partial_end_questions.append(q)
                print(f"ğŸ”— Partial END found: {question_id}")
            elif q.get('special_handling') in ['incomplete_options', 'continues_next_page']:
                incomplete_questions.append(q)
                print(f"ğŸ”— Incomplete question found: Q{q.get('question_number')} - {q.get('special_handling')}")
            else:
                complete_questions.append(q)
        
        # ì—°ì† ì²˜ë¦¬ partial ë¬¸ì œë“¤ ë³‘í•©
        merged_questions = await self._merge_partial_questions(partial_start_questions, partial_end_questions)
        
        if not incomplete_questions and not partial_start_questions and not partial_end_questions:
            print("âœ… No cross-page questions found")
            return questions
        
        # ê¸°ì¡´ ë°©ì‹ì˜ ì—°ê²° ì²˜ë¦¬ ë¡œì§ (í˜¸í™˜ì„± ìœ ì§€)
        print(f"ğŸ”— Processing {len(incomplete_questions)} incomplete questions...")
        
        for incomplete_q in incomplete_questions:
            q_num = incomplete_q.get('question_number')
            
            # ë‹¤ìŒ ì²­í¬ì—ì„œ ê°™ì€ ë²ˆí˜¸ì˜ ì™„ì „í•œ ë¬¸ì œ ì°¾ê¸°
            for complete_q in complete_questions:
                if (complete_q.get('question_number') == q_num and 
                    len(complete_q.get('options', [])) >= 3):
                    
                    # ë¶ˆì™„ì „í•œ ê²ƒì„ ì™„ì „í•œ ê²ƒìœ¼ë¡œ êµì²´
                    print(f"  âœ… Linked Q{q_num} across pages")
                    incomplete_q.update(complete_q)
                    incomplete_q['special_handling'] = 'linked_from_continuous_processing'
                    break
        
        # ëª¨ë“  ê²°ê³¼ ë³‘í•©
        all_linked = complete_questions + incomplete_questions + merged_questions
        print(f"ğŸ”— Cross-page linking completed: {len(all_linked)} total questions")
        print(f"  - Complete questions: {len(complete_questions)}")
        print(f"  - Incomplete questions processed: {len(incomplete_questions)}")
        print(f"  - Merged from partials: {len(merged_questions)}")
        
        return all_linked
    
    async def _merge_partial_questions(self, partial_starts: List[Dict], partial_ends: List[Dict]) -> List[Dict]:
        """ì—°ì† ì²˜ë¦¬ì—ì„œ ë¶„í• ëœ partial ë¬¸ì œë“¤ì„ ë³‘í•©"""
        merged = []
        
        if not partial_starts and not partial_ends:
            return merged
            
        print(f"ğŸ”— Merging {len(partial_starts)} partial starts with {len(partial_ends)} partial ends")
        
        # ë¬¸ì œ ë²ˆí˜¸ë¥¼ ê¸°ì¤€ìœ¼ë¡œ startì™€ end ë§¤ì¹­
        start_by_number = {}
        end_by_number = {}
        
        # partial start ë¬¸ì œë“¤ ì •ë¦¬
        for start_q in partial_starts:
            # ë¬¸ì œ ë²ˆí˜¸ ì¶”ì¶œ ì‹œë„
            question_text = start_q.get('question_text', '')
            question_number = self._extract_question_number_from_text(question_text)
            if question_number:
                start_by_number[question_number] = start_q
        
        # partial end ë¬¸ì œë“¤ ì •ë¦¬  
        for end_q in partial_ends:
            question_text = end_q.get('question_text', '')
            question_number = self._extract_question_number_from_text(question_text)
            if question_number:
                end_by_number[question_number] = end_q
        
        # ë§¤ì¹­ë˜ëŠ” start-end ìŒë“¤ ë³‘í•©
        for q_num in start_by_number.keys():
            if q_num in end_by_number:
                start_q = start_by_number[q_num]
                end_q = end_by_number[q_num]
                
                # ë‘ ë¶€ë¶„ì„ ë³‘í•©í•˜ì—¬ ì™„ì „í•œ ë¬¸ì œ ìƒì„±
                merged_question = {
                    'question_id': f"MERGED_Q{q_num:02d}",
                    'question_number': q_num,
                    'question_text': self._merge_question_texts(start_q.get('question_text', ''), end_q.get('question_text', '')),
                    'passage': start_q.get('passage', '') + ' ' + end_q.get('passage', ''),
                    'options': self._merge_options(start_q.get('options', []), end_q.get('options', [])),
                    'has_table': start_q.get('has_table', False) or end_q.get('has_table', False),
                    'has_code': start_q.get('has_code', False) or end_q.get('has_code', False),
                    'has_figure': start_q.get('has_figure', False) or end_q.get('has_figure', False),
                    'special_handling': 'merged_from_partials',
                    'confidence': 'high',
                    'extraction_notes': f'Merged from PARTIAL_START and PARTIAL_END in continuous processing'
                }
                
                merged.append(merged_question)
                print(f"  âœ… Successfully merged Q{q_num} from partials")
        
        # ë§¤ì¹­ë˜ì§€ ì•Šì€ partialë“¤ì€ ê²½ê³  ì¶œë ¥
        unmatched_starts = set(start_by_number.keys()) - set(end_by_number.keys())
        unmatched_ends = set(end_by_number.keys()) - set(start_by_number.keys())
        
        if unmatched_starts:
            print(f"  âš ï¸ Unmatched partial starts: {unmatched_starts}")
        if unmatched_ends:
            print(f"  âš ï¸ Unmatched partial ends: {unmatched_ends}")
            
        return merged
    
    def _extract_question_number_from_text(self, text: str) -> Optional[int]:
        """í…ìŠ¤íŠ¸ì—ì„œ ë¬¸ì œ ë²ˆí˜¸ ì¶”ì¶œ"""
        import re
        # ë‹¤ì–‘í•œ ë¬¸ì œ ë²ˆí˜¸ íŒ¨í„´ ë§¤ì¹­
        patterns = [
            r'ë¬¸ì œ\s*(\d+)',
            r'Q\s*(\d+)',
            r'^(\d+)\.',
            r'^(\d+)\)',
            r'PARTIAL_(?:START|END)_Q(\d+)',
        ]
        
        for pattern in patterns:
            match = re.search(pattern, text, re.IGNORECASE)
            if match:
                try:
                    return int(match.group(1))
                except (ValueError, IndexError):
                    continue
        return None
    
    def _merge_question_texts(self, start_text: str, end_text: str) -> str:
        """ë¬¸ì œ í…ìŠ¤íŠ¸ ë³‘í•©"""
        # PARTIAL ë§ˆì»¤ ì œê±°
        start_clean = re.sub(r'PARTIAL_START.*?[\n\r]', '', start_text, flags=re.IGNORECASE)
        end_clean = re.sub(r'PARTIAL_END.*?[\n\r]', '', end_text, flags=re.IGNORECASE)
        
        # ë‘ í…ìŠ¤íŠ¸ë¥¼ ìì—°ìŠ¤ëŸ½ê²Œ ì—°ê²°
        merged = start_clean.strip() + ' ' + end_clean.strip()
        return merged.strip()
    
    def _merge_options(self, start_options: List[str], end_options: List[str]) -> List[str]:
        """ì„ íƒì§€ ë³‘í•© (ì¤‘ë³µ ì œê±° ë° ì •ë ¬)"""
        all_options = []
        
        # start ì˜µì…˜ë“¤ ì¶”ê°€
        for option in start_options:
            if option and option.strip():
                all_options.append(option.strip())
        
        # end ì˜µì…˜ë“¤ ì¶”ê°€ (ì¤‘ë³µ ì²´í¬)
        for option in end_options:
            if option and option.strip() and option.strip() not in all_options:
                all_options.append(option.strip())
        
        # ì„ íƒì§€ ë²ˆí˜¸ ìˆœìœ¼ë¡œ ì •ë ¬ ì‹œë„
        def get_option_number(option_text):
            import re
            patterns = [r'^[â‘ â‘¡â‘¢â‘£â‘¤]', r'^[1-5]\)', r'^[A-E]\)', r'^\([1-5]\)', r'^\([A-E]\)']
            for i, pattern in enumerate(patterns):
                if re.match(pattern, option_text):
                    return i
            return 999  # íŒ¨í„´ì— ë§ì§€ ì•ŠëŠ” ê²½ìš° ë§ˆì§€ë§‰ìœ¼ë¡œ
        
        all_options.sort(key=get_option_number)
        
        return all_options
    
    async def _comprehensive_validation(self, questions: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """Stage 4: í¬ê´„ì  í’ˆì§ˆ ê²€ì¦"""
        
        validated = []
        
        for q in questions:
            # ê¸°ë³¸ í•„ë“œ ê²€ì¦
            question_text = q.get('question_text', '').strip()
            options = q.get('options', [])
            question_number = q.get('question_number')
            
            # ê²€ì¦ ê¸°ì¤€ë“¤
            validations = {
                'has_question_text': len(question_text) >= 10,
                'has_question_number': question_number and question_number > 0,
                'has_sufficient_options': len(options) >= 2,
                'options_have_content': all(len(opt.strip()) >= 3 for opt in options if opt),
                'not_answer_explanation': not any(keyword in question_text.lower() 
                                                for keyword in ['í•´ì„¤', 'ì •ë‹µ', 'í’€ì´', 'ë‹µì•ˆ']),
                'valid_option_format': any(pattern in str(options) 
                                         for pattern in ['â‘ ', 'â‘¡', '1)', '2)', 'A)', 'B)'])
            }
            
            # í†µê³¼ ì¡°ê±´: ëª¨ë“  ê¸°ë³¸ ê²€ì¦ í†µê³¼
            if all(validations.values()):
                q['validation_status'] = 'passed'
                q['validation_details'] = validations
                validated.append(q)
                print(f"âœ… Q{question_number}: PASSED ({len(options)} options)")
            else:
                failed_checks = [k for k, v in validations.items() if not v]
                print(f"âŒ Q{question_number}: FAILED - {', '.join(failed_checks)}")
        
        print(f"ğŸ” Validation completed: {len(validated)}/{len(questions)} questions passed")
        return validated
    
    async def _advanced_deduplication(self, questions: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """Stage 5: ê³ ê¸‰ ì¤‘ë³µ ì œê±°"""
        
        # ë¬¸ì œ ë²ˆí˜¸ë³„ë¡œ ê·¸ë£¹í™”
        by_number = {}
        for q in questions:
            num = q.get('question_number')
            if num not in by_number:
                by_number[num] = []
            by_number[num].append(q)
        
        # ì¤‘ë³µëœ ë²ˆí˜¸ë“¤ ì²˜ë¦¬
        final_questions = []
        for num, q_list in by_number.items():
            if len(q_list) == 1:
                final_questions.append(q_list[0])
            else:
                print(f"ğŸ”„ Deduplicating Q{num}: {len(q_list)} versions found")
                
                # ê°€ì¥ ì™„ì „í•œ ë²„ì „ ì„ íƒ (ì˜µì…˜ ìˆ˜ ê¸°ì¤€)
                best_q = max(q_list, key=lambda x: len(x.get('options', [])))
                best_q['dedup_status'] = f'selected_from_{len(q_list)}_versions'
                final_questions.append(best_q)
                
                print(f"  âœ… Selected version with {len(best_q.get('options', []))} options")
        
        # ë²ˆí˜¸ ìˆœ ì •ë ¬
        final_questions.sort(key=lambda x: x.get('question_number', 0))
        
        print(f"ğŸ”„ Deduplication completed: {len(final_questions)} unique questions")
        return final_questions