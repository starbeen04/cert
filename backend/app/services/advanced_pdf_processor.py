"""
ìƒˆë¡œìš´ ê³ ê¸‰ PDF ì²˜ë¦¬ ì‹œìŠ¤í…œ
OpenAI APIë¥¼ í™œìš©í•œ ë¬¸ì„œ ë¶„ì„ê³¼ ì „ì²˜ë¦¬, OCR ë¶„ë¦¬ ì²˜ë¦¬
"""

import asyncio
import json
import os
import base64
import io
from typing import Dict, List, Any, Optional, Tuple
from pathlib import Path
import cv2
import numpy as np
from PIL import Image
import fitz  # PyMuPDF
from sqlalchemy.orm import Session
from sqlalchemy import text
import openai
import anthropic
from datetime import datetime
import logging
import time
try:
    from .ultra_enhanced_prompts import UltraEnhancedPrompts
except ImportError:
    UltraEnhancedPrompts = None
    
try:
    from .image_processor_fix import ImageProcessorFix
except ImportError:
    ImageProcessorFix = None

try:
    # from .smart_pdf_analyzer import SmartPDFAnalyzer  # ì‚­ì œë¨ - UltraPrecisePDFAnalyzer ì‚¬ìš©
    SmartPDFAnalyzer = None
except ImportError:
    SmartPDFAnalyzer = None
import random

logger = logging.getLogger(__name__)

class AdvancedPDFProcessor:
    """ê³ ê¸‰ PDF ì²˜ë¦¬ ì‹œìŠ¤í…œ - OpenAI + Claude í•˜ì´ë¸Œë¦¬ë“œ"""
    
    def __init__(self):
        self.openai_client = None
        self.claude_client = None
        self.thumbnail_size = (1024, -1)  # í­ 1024px, ë†’ì´ ìë™
        self.temp_dir = Path("temp_processing")
        self.temp_dir.mkdir(exist_ok=True)
        
    def set_openai_key(self, api_key: str):
        """OpenAI API í‚¤ ì„¤ì •"""
        self.openai_client = openai.AsyncOpenAI(api_key=api_key)
        
    def set_claude_key(self, api_key: str):
        """Claude API í‚¤ ì„¤ì •"""
        self.claude_client = anthropic.AsyncAnthropic(api_key=api_key)
        
    async def process_pdf_complete_pipeline(self, upload_id: int, pdf_path: str, db: Session) -> Dict[str, Any]:
        """ì „ì²´ PDF ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ - GPT Vision ê¸°ë°˜ í˜ì´ì§€ë³„ Markdown ë³€í™˜"""
        try:
            print(f"[ì‹œì‘] ìƒˆë¡œìš´ GPT Vision ê¸°ë°˜ PDF ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ - Upload {upload_id}")
            
            # 1ë‹¨ê³„: PDFë¥¼ í˜ì´ì§€ë³„ ê³ í•´ìƒë„ ì´ë¯¸ì§€ë¡œ ë³€í™˜
            pages_result = await self._convert_pdf_to_pages(upload_id, pdf_path, db)
            if not pages_result['success']:
                return pages_result
                
            # 2ë‹¨ê³„: PDFì—ì„œ ì´ë¯¸ì§€ ì¶”ì¶œ ë° ì €ì¥
            image_extraction_result = await self._extract_images_from_pdf(upload_id, pdf_path)
            
            # ğŸš¨ 2.5ë‹¨ê³„: PDF ì§ì ‘ í‘œ ì¶”ì¶œ ì™„ì „ ë¹„í™œì„±í™”
            print("ğŸš« PDF ì§ì ‘ í‘œ ì¶”ì¶œ ì™„ì „ ë¹„í™œì„±í™” - ê°€ì§œ í‘œ ìƒì„± ë°©ì§€")
            # direct_table_result = await self._extract_tables_directly_from_pdf(pdf_path, upload_id)
            direct_table_result = {'success': False, 'tables_count': 0, 'tables_data': []}
            
            # 3ë‹¨ê³„: ê¸°ì¡´ GPT Vision ì‹œìŠ¤í…œìœ¼ë¡œ ë³µê·€ (ì´ë¯¸ì§€ ì •ë³´ í¬í•¨)
            extracted_images = image_extraction_result.get('images', []) if image_extraction_result['success'] else []
            markdown_result = await self._convert_pages_to_markdown(upload_id, pages_result['pages'], db, extracted_images)
            if not markdown_result['success']:
                return markdown_result
                
            # 4ë‹¨ê³„: Claudeë¡œ ì „ì²´ Markdownì„ ë¬¸ì œë‹¨ìœ„ë¡œ êµ¬ì¡°í™” (ê°œì„ ëœ ë²„ì „)
            final_results = await self._structure_markdown_content_enhanced(upload_id, markdown_result, db)
            
            # ì´ë¯¸ì§€ ì •ë³´ë¥¼ ìµœì¢… ê²°ê³¼ì— ì¶”ê°€
            if image_extraction_result['success']:
                final_results['extracted_images'] = image_extraction_result['images']
                final_results['images_count'] = image_extraction_result['images_extracted']
            
            return {
                'success': True,
                'upload_id': upload_id,
                'processing_summary': final_results,
                'total_pages': pages_result.get('total_pages', 0),
                'questions_extracted': len(final_results.get('questions', [])),
                'materials_generated': len(final_results.get('materials', []))
            }
            
        except Exception as e:
            logger.error(f"PDF ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ ì˜¤ë¥˜: {str(e)}")
            await self._log_processing_step(upload_id, "pipeline_error", "failed", 
                                          {"error": str(e)}, db)
            return {'success': False, 'error': str(e)}
    
    async def _convert_pages_to_markdown_enhanced(self, upload_id: int, pages_info: List[Dict], direct_table_data: Dict, db: Session) -> Dict[str, Any]:
        """ğŸš€ í–¥ìƒëœ ë§ˆí¬ë‹¤ìš´ ë³€í™˜: GPT Vision + PDF ì§ì ‘ í‘œ ì¶”ì¶œ ê²°í•©"""
        try:
            print(f"\nğŸ”„ í–¥ìƒëœ í˜ì´ì§€ â†’ ë§ˆí¬ë‹¤ìš´ ë³€í™˜ ì‹œì‘...")
            print(f"ğŸ“Š ì§ì ‘ ì¶”ì¶œëœ í‘œ: {direct_table_data.get('tables_count', 0)}ê°œ")
            
            # ê¸°ì¡´ GPT Vision ì²˜ë¦¬ ì§„í–‰
            basic_result = await self._convert_pages_to_markdown(upload_id, pages_info, db)
            
            if not basic_result['success']:
                return basic_result
            
            # ë‹¨ìˆœí™”: ì§ì ‘ í‘œ ì¶”ì¶œ ë¹„í™œì„±í™”ë˜ì—ˆìœ¼ë¯€ë¡œ ê¸°ë³¸ ê²°ê³¼ë§Œ ë°˜í™˜
            print("ğŸ“„ ë‹¨ìˆœí™”ëœ GPT Vision ê²°ê³¼ ì‚¬ìš©")
            return basic_result
                
        except Exception as e:
            print(f"âŒ í–¥ìƒëœ ë§ˆí¬ë‹¤ìš´ ë³€í™˜ ì‹¤íŒ¨: {e}")
            # ì‹¤íŒ¨ì‹œ ê¸°ë³¸ GPT Vision ê²°ê³¼ë¼ë„ ë°˜í™˜
            try:
                return await self._convert_pages_to_markdown(upload_id, pages_info, db)
            except:
                return {'success': False, 'error': str(e)}
    
    async def _enhance_markdown_with_direct_tables(self, original_markdown: str, table_data: Dict) -> str:
        """ë§ˆí¬ë‹¤ìš´ì— ì§ì ‘ ì¶”ì¶œëœ í‘œ ë°ì´í„° ì‚½ì…/ë³´ê°•"""
        try:
            print(f"ğŸ”„ ë§ˆí¬ë‹¤ìš´ í‘œ ë³´ê°• ì²˜ë¦¬...")
            
            enhanced_markdown = original_markdown
            tables_info = table_data.get('tables_data', [])
            
            # í˜ì´ì§€ë³„ë¡œ í‘œ ì •ë³´ ì •ë¦¬
            tables_by_page = {}
            for table in tables_info:
                page_num = table['page_number']
                if page_num not in tables_by_page:
                    tables_by_page[page_num] = []
                tables_by_page[page_num].append(table)
            
            # ê° í˜ì´ì§€ë³„ë¡œ í‘œ ì‚½ì…/ë³´ê°•
            for page_num, page_tables in tables_by_page.items():
                print(f"   ğŸ“Š í˜ì´ì§€ {page_num}: {len(page_tables)}ê°œ í‘œ ë³´ê°•...")
                
                # í•´ë‹¹ í˜ì´ì§€ ì„¹ì…˜ ì°¾ê¸°
                page_pattern = f"# Page {page_num}"
                if page_pattern in enhanced_markdown:
                    
                    # í˜ì´ì§€ ì„¹ì…˜ ë ì°¾ê¸°
                    next_page_pattern = f"# Page {page_num + 1}"
                    page_start = enhanced_markdown.find(page_pattern)
                    
                    if next_page_pattern in enhanced_markdown:
                        page_end = enhanced_markdown.find(next_page_pattern)
                        page_section = enhanced_markdown[page_start:page_end]
                        remaining_markdown = enhanced_markdown[page_end:]
                    else:
                        page_section = enhanced_markdown[page_start:]
                        remaining_markdown = ""
                    
                    # í˜ì´ì§€ ì„¹ì…˜ì— í‘œ ì¶”ê°€
                    enhanced_page_section = page_section
                    
                    for table_idx, table in enumerate(page_tables):
                        table_markdown = self._convert_table_to_markdown(table, table_idx + 1)
                        
                        # ê¸°ì¡´ í˜ì´ì§€ì— í‘œ ê´€ë ¨ ë‚´ìš©ì´ ìˆëŠ”ì§€ í™•ì¸
                        has_existing_table = any(indicator in page_section.lower() for indicator in [
                            '|', 'í‘œ', 'table', 'í”„ë¡œì„¸ìŠ¤', 'ë°ì´í„°', 'ì‹œê°„', 'ê²°ê³¼'
                        ])
                        
                        if has_existing_table:
                            # ê¸°ì¡´ í‘œ ê´€ë ¨ ë‚´ìš© ë’¤ì— ì •í™•í•œ í‘œ ì¶”ê°€
                            enhanced_page_section += f"\n\n## ğŸš€ ì •í™•í•œ í‘œ ë°ì´í„° (PDF ì§ì ‘ ì¶”ì¶œ)\n\n{table_markdown}"
                        else:
                            # ìƒˆë¡œìš´ í‘œë¡œ ì¶”ê°€
                            enhanced_page_section += f"\n\n## ğŸ“Š ë°œê²¬ëœ í‘œ ë°ì´í„°\n\n{table_markdown}"
                        
                        print(f"      í‘œ {table_idx+1}: {table['total_columns']}ì»¬ëŸ¼ Ã— {len(table['data_rows'])}í–‰ ì¶”ê°€")
                    
                    # í–¥ìƒëœ í˜ì´ì§€ ì„¹ì…˜ìœ¼ë¡œ êµì²´
                    enhanced_markdown = enhanced_markdown[:page_start] + enhanced_page_section + remaining_markdown
            
            print(f"âœ… í‘œ ë³´ê°• ì™„ë£Œ: {len(tables_info)}ê°œ í‘œê°€ ë§ˆí¬ë‹¤ìš´ì— í†µí•©ë¨")
            return enhanced_markdown
            
        except Exception as e:
            print(f"âš ï¸ í‘œ ë³´ê°• ì‹¤íŒ¨: {e}, ì›ë³¸ ë§ˆí¬ë‹¤ìš´ ë°˜í™˜")
            return original_markdown
    
    def _convert_table_to_markdown(self, table_info: Dict, table_num: int) -> str:
        """í‘œ ì •ë³´ë¥¼ ë§ˆí¬ë‹¤ìš´ í‘œ í˜•ì‹ìœ¼ë¡œ ë³€í™˜"""
        try:
            headers = table_info.get('headers', [])
            data_rows = table_info.get('data_rows', [])
            method = table_info.get('extraction_method', 'unknown')
            
            if not headers and not data_rows:
                return f"**í‘œ {table_num}**: ë¹ˆ í‘œ"
            
            markdown_lines = []
            markdown_lines.append(f"**í‘œ {table_num}** ({method} ë°©ë²•ìœ¼ë¡œ ì¶”ì¶œ):")
            markdown_lines.append("")
            
            # í—¤ë”ê°€ ìˆëŠ” ê²½ìš°
            if headers:
                # í—¤ë” í–‰
                header_line = "| " + " | ".join(str(cell) if cell else "" for cell in headers) + " |"
                markdown_lines.append(header_line)
                
                # êµ¬ë¶„ì„ 
                separator_line = "|" + "|".join("-------" for _ in headers) + "|"
                markdown_lines.append(separator_line)
            
            # ë°ì´í„° í–‰ë“¤
            for row_idx, row in enumerate(data_rows):
                if row:  # ë¹ˆ í–‰ì´ ì•„ë‹Œ ê²½ìš°
                    # í—¤ë” ì»¬ëŸ¼ ìˆ˜ì— ë§ì¶° í–‰ ë°ì´í„° ì¡°ì •
                    if headers:
                        adjusted_row = row[:len(headers)]  # í—¤ë” ìˆ˜ë§Œí¼ë§Œ ì‚¬ìš©
                        while len(adjusted_row) < len(headers):  # ë¶€ì¡±í•œ ì»¬ëŸ¼ì€ ë¹ˆ ê°’ìœ¼ë¡œ ì±„ì›€
                            adjusted_row.append("")
                    else:
                        adjusted_row = row
                    
                    row_line = "| " + " | ".join(str(cell) if cell else "" for cell in adjusted_row) + " |"
                    markdown_lines.append(row_line)
            
            # í‘œ ì •ë³´ ì¶”ê°€
            markdown_lines.append("")
            markdown_lines.append(f"*ğŸ“Š í‘œ ì •ë³´: {len(data_rows)}ê°œ ë°ì´í„° í–‰, {len(headers) if headers else len(data_rows[0]) if data_rows else 0}ê°œ ì»¬ëŸ¼*")
            
            return "\n".join(markdown_lines)
            
        except Exception as e:
            return f"**í‘œ {table_num}**: ë³€í™˜ ì‹¤íŒ¨ - {str(e)}"
    
    async def _convert_pages_to_markdown_ultra_enhanced(self, upload_id: int, pages_info: List[Dict], db: Session) -> Dict[str, Any]:
        """ğŸš€ ULTRA ê°œì„ ëœ GPT Vision í‘œ ì¸ì‹ (ê°€ì§œ í‘œ ë°©ì§€)"""
        try:
            await self._log_processing_step(upload_id, "ultra_enhanced_markdown", "processing", {}, db)
            
            if not self.openai_client:
                return {'success': False, 'error': 'OpenAI API key not set'}
            
            print(f"\nğŸ”¥ ULTRA ê°œì„ ëœ GPT Vision ì²˜ë¦¬ ì‹œì‘...")
            
            all_markdown_content = []
            individual_pages = [p for p in pages_info if not p.get('is_full_connected', False)]
            
            for i, page_info in enumerate(individual_pages):
                page_num = page_info['page_number']
                image_path = page_info['image_path']
                
                print(f"ğŸ“„ í˜ì´ì§€ {page_num}/{len(individual_pages)} - ULTRA í‘œ ì¸ì‹ ì²˜ë¦¬")
                
                try:
                    with open(image_path, "rb") as image_file:
                        base64_image = base64.b64encode(image_file.read()).decode('utf-8')
                    
                    # ğŸ¯ ê·¼ë³¸ì ìœ¼ë¡œ ë‹¤ë¥¸ ì ‘ê·¼ë²•: í‘œ vs ë¹„í‘œ ëª…í™•í•œ êµ¬ë¶„
                    ultra_prompt = f"""
ğŸ¯ **í˜ì´ì§€ {page_num} ì •ë°€ ë¶„ì„** - í‘œ vs ì„ íƒì§€ êµ¬ë¶„

ì´ë¯¸ì§€ë¥¼ ë³´ê³  ë‹¤ìŒ ë‹¨ê³„ë¡œ ë¶„ì„í•´ì£¼ì„¸ìš”:

**1ë‹¨ê³„: í‘œ ì¡´ì¬ ì—¬ë¶€ í™•ì¸**
- ì‹¤ì œ í‘œ(table)ê°€ ìˆëŠ”ê°€? âœ… ë˜ëŠ” âŒ
- í‘œì˜ ì •ì˜: í–‰ê³¼ ì—´ë¡œ êµ¬ì„±ëœ ê²©ì êµ¬ì¡°, í—¤ë” í–‰ + ìµœì†Œ 2ê°œ ì´ìƒì˜ ë°ì´í„° í–‰

**2ë‹¨ê³„: í‘œê°€ ì•„ë‹Œ ê²ƒë“¤ ì œì™¸**
âŒ **ì´ê²ƒë“¤ì€ í‘œê°€ ì•„ë‹˜**:
- ì„ íƒì§€ (â‘ â‘¡â‘¢â‘£)
- ë¬¸ì œ ë²ˆí˜¸ ë‚˜ì—´
- ë‹¨ìˆœ í…ìŠ¤íŠ¸ ì •ë ¬
- í•œ ì¤„ì§œë¦¬ í•­ëª©ë“¤

**3ë‹¨ê³„: ì‹¤ì œ í‘œë¼ë©´ ì™„ì „ ì¶”ì¶œ**
âœ… **ì§„ì§œ í‘œì˜ íŠ¹ì§•**:
- ëª…í™•í•œ í—¤ë” í–‰ (í”„ë¡œì„¸ìŠ¤, ë„ì°©ì‹œê°„, ì‹¤í–‰ì‹œê°„ ë“±)
- ì—¬ëŸ¬ ê°œì˜ ë°ì´í„° í–‰ (P1,P2,P3... ë˜ëŠ” ê¸°íƒ€ ë°ì´í„°)
- ê²©ì êµ¬ì¡° ë˜ëŠ” ì¼ì •í•œ ê°„ê²©
- ìˆ«ìë‚˜ ì²´ê³„ì ì¸ ë°ì´í„°

**ğŸš¨ ì¤‘ìš”**: 
- í‘œê°€ ì—†ìœ¼ë©´ "ì´ í˜ì´ì§€ì—ëŠ” í‘œê°€ ì—†ìŠµë‹ˆë‹¤" ëª…ì‹œ
- í‘œê°€ ìˆìœ¼ë©´ í—¤ë”ë¶€í„° ë§ˆì§€ë§‰ ë°ì´í„° í–‰ê¹Œì§€ ì™„ì „ ì¶”ì¶œ
- ì„ íƒì§€ë‚˜ ë¬¸ì œ ë²ˆí˜¸ë¥¼ í‘œë¡œ ì°©ê°í•˜ì§€ ë§ ê²ƒ

**ì¶œë ¥ í˜•ì‹**:

# í˜ì´ì§€ {page_num}

## í‘œ ë¶„ì„ ê²°ê³¼
[í‘œ ìˆìŒ / í‘œ ì—†ìŒ]

## í˜ì´ì§€ ë‚´ìš©
[ë¬¸ì œë“¤ì„ ì¼ë°˜ì ìœ¼ë¡œ ì¶”ì¶œ]

## í‘œ ë°ì´í„° (ìˆëŠ” ê²½ìš°ì—ë§Œ)
| í—¤ë”1 | í—¤ë”2 | í—¤ë”3 |
|-------|-------|-------|
| ë°ì´í„°1 | ë°ì´í„°2 | ë°ì´í„°3 |
| ë°ì´í„°4 | ë°ì´í„°5 | ë°ì´í„°6 |

í‘œ ì™„ì „ì„±: [âœ… ì™„ì „ / âŒ ë¶ˆì™„ì „ - ì´ìœ ]
                    """.strip()
                    
                    response = await self.openai_client.chat.completions.create(
                        model="gpt-4o",
                        messages=[{
                            "role": "user",
                            "content": [
                                {"type": "text", "text": ultra_prompt},
                                {"type": "image_url", "image_url": {"url": f"data:image/png;base64,{base64_image}", "detail": "high"}}
                            ]
                        }],
                        max_tokens=4000,
                        temperature=0.1  # ë§¤ìš° ì •í™•í•œ ì¶”ì¶œ
                    )
                    
                    page_markdown = response.choices[0].message.content.strip()
                    all_markdown_content.append(page_markdown)
                    
                    print(f"âœ… í˜ì´ì§€ {page_num} ULTRA ì²˜ë¦¬ ì™„ë£Œ: {len(page_markdown)}chars")
                    await asyncio.sleep(3)  # API ì•ˆì •ì„± ê°•í™”
                    
                except Exception as page_error:
                    print(f"âŒ í˜ì´ì§€ {page_num} ì²˜ë¦¬ ì‹¤íŒ¨: {page_error}")
                    all_markdown_content.append(f"# í˜ì´ì§€ {page_num}\n\nâš ï¸ ì²˜ë¦¬ ì‹¤íŒ¨: {str(page_error)}")
            
            complete_markdown = "\n\n---\n\n".join(all_markdown_content)
            
            print(f"ğŸ¯ ULTRA ì²˜ë¦¬ ì™„ë£Œ:")
            print(f"   ì´ {len(individual_pages)}í˜ì´ì§€ ì²˜ë¦¬")
            print(f"   ì´ ë§ˆí¬ë‹¤ìš´: {len(complete_markdown)}chars")
            
            return {
                'success': True,
                'markdown_content': complete_markdown,
                'cross_page_issues': '',  # ULTRA ëª¨ë“œì—ì„œëŠ” ë‹¨ìˆœí™”
                'pages_processed': len(individual_pages),
                'tokens_used': 0,
                'estimated_cost': 0
            }
            
        except Exception as e:
            print(f"âŒ ULTRA ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
            return {'success': False, 'error': str(e)}
    
    async def _convert_pdf_to_pages(self, upload_id: int, pdf_path: str, db: Session) -> Dict[str, Any]:
        """1ë‹¨ê³„: PDFë¥¼ í˜ì´ì§€ë³„ ê³ í•´ìƒë„ ì´ë¯¸ì§€ë¡œ ë³€í™˜"""
        try:
            await self._log_processing_step(upload_id, "pdf_to_pages_conversion", "processing", {}, db)
            
            doc = fitz.open(pdf_path)
            total_pages = len(doc)
            
            print(f"Converting {total_pages} pages to individual high-resolution images...")
            
            # 1. ê° í˜ì´ì§€ë¥¼ ê°œë³„ ì´ë¯¸ì§€ë¡œ ë³€í™˜
            page_files = []
            page_pixmaps = []  # ì „ì²´ ì—°ê²° ì´ë¯¸ì§€ìš©
            
            for page_num in range(total_pages):
                page = doc[page_num]
                
                # ì´ˆê³ í•´ìƒë„ ë Œë”ë§ (4ë°° í™•ëŒ€ë¡œ ë” ì„ ëª…í•˜ê²Œ)
                mat = fitz.Matrix(4.0, 4.0)  # 4ë°° í™•ëŒ€ë¡œ í‘œì™€ ì‘ì€ ê¸€ì”¨ë„ ì„ ëª…í•˜ê²Œ
                pix = page.get_pixmap(matrix=mat, alpha=False)
                
                # ê°œë³„ í˜ì´ì§€ ì´ë¯¸ì§€ ì €ì¥
                page_image_path = self.temp_dir / f"page_{upload_id}_{page_num:03d}.png"
                pix.save(str(page_image_path))
                
                # ì „ì²´ ì—°ê²° ì´ë¯¸ì§€ìš© pixmap ì €ì¥
                page_pixmaps.append(pix)
                
                page_info = {
                    'page_number': page_num + 1,
                    'image_path': str(page_image_path),
                    'width': pix.width,
                    'height': pix.height,
                    'file_size': os.path.getsize(page_image_path)
                }
                page_files.append(page_info)
                
                print(f"Processed page {page_num + 1}/{total_pages} - {pix.width}x{pix.height}")
            
            # 2. ëª¨ë“  í˜ì´ì§€ë¥¼ ì„¸ë¡œë¡œ ì—°ê²°í•œ ì´ˆê³ í•´ìƒë„ ì „ì²´ ì´ë¯¸ì§€ ìƒì„±
            print(f"Creating full-length connected image for {total_pages} pages...")
            
            if page_pixmaps:
                total_width = max(pix.width for pix in page_pixmaps)
                total_height = sum(pix.height for pix in page_pixmaps)
                
                # PIL ì´ë¯¸ì§€ë¡œ ë³€í™˜í•´ì„œ ì—°ê²°
                from PIL import Image
                import io
                
                # ì „ì²´ ìº”ë²„ìŠ¤ ìƒì„±
                full_image = Image.new('RGB', (total_width, total_height), 'white')
                
                current_y = 0
                for i, pix in enumerate(page_pixmaps):
                    # Pixmapì„ PIL Imageë¡œ ë³€í™˜
                    img_data = pix.tobytes("png")
                    page_img = Image.open(io.BytesIO(img_data))
                    
                    # í˜ì´ì§€ ì´ë¯¸ì§€ë¥¼ ì „ì²´ ìº”ë²„ìŠ¤ì— ë¶™ì´ê¸°
                    full_image.paste(page_img, (0, current_y))
                    current_y += page_img.height
                    
                    print(f"Connected page {i+1}/{total_pages} at y={current_y}")
                
                # ì „ì²´ ì—°ê²° ì´ë¯¸ì§€ ì €ì¥
                full_image_path = self.temp_dir / f"full_connected_{upload_id}.png"
                full_image.save(str(full_image_path), 'PNG', optimize=True)
                
                print(f"âœ… Full connected image created: {total_width}x{total_height} -> {full_image_path}")
                
                # ì „ì²´ ì´ë¯¸ì§€ ì •ë³´ë¥¼ í˜ì´ì§€ ë¦¬ìŠ¤íŠ¸ì— ì¶”ê°€
                full_image_info = {
                    'page_number': 0,  # ì „ì²´ ì´ë¯¸ì§€ëŠ” 0ë²ˆ
                    'image_path': str(full_image_path),
                    'width': total_width,
                    'height': total_height,
                    'file_size': os.path.getsize(full_image_path),
                    'is_full_connected': True
                }
                page_files.insert(0, full_image_info)  # ì²« ë²ˆì§¸ë¡œ ì¶”ê°€
            
            doc.close()
            
            print(f"All {total_pages} pages converted + 1 full connected image created")
            
            await self._log_processing_step(upload_id, "pdf_to_pages_conversion", "completed", {
                'total_pages': total_pages,
                'has_full_connected_image': len(page_pixmaps) > 0,
                'pages_info': page_files[:3]  # ì²« 3ê°œ ì •ë³´ë§Œ ë¡œê·¸ì— ì €ì¥
            }, db)
            
            return {
                'success': True,
                'total_pages': total_pages,
                'pages': page_files,
                'has_full_connected': len(page_pixmaps) > 0
            }
            
        except Exception as e:
            logger.error(f"PDF to pages conversion error: {str(e)}")
            await self._log_processing_step(upload_id, "pdf_to_pages_conversion", "failed", 
                                          {"error": str(e)}, db)
            return {'success': False, 'error': str(e)}
    
    async def _convert_pages_to_markdown(self, upload_id: int, pages_info: List[Dict], db: Session, extracted_images: List[Dict] = None) -> Dict[str, Any]:
        """2ë‹¨ê³„: GPT Visionìœ¼ë¡œ ê° í˜ì´ì§€ë¥¼ Markdownìœ¼ë¡œ ë³€í™˜"""
        try:
            await self._log_processing_step(upload_id, "pages_to_markdown", "processing", {}, db)
            
            if not self.openai_client:
                print("ERROR: OpenAI API key not set - cannot perform GPT Vision conversion")
                return {
                    'success': False, 
                    'error': 'OpenAI API key not set'
                }
            
            print(f"Converting {len(pages_info)} pages to Markdown using GPT Vision...")
            
            all_markdown_content = []
            total_tokens_used = 0
            total_cost = 0.0
            
            # ğŸš« ì „ì²´ ì—°ê²° ë¶„ì„ ì™„ì „ ë¹„í™œì„±í™” - ê°œë³„ í˜ì´ì§€ ê²¹ì¹¨ ì²˜ë¦¬ë¡œ ëŒ€ì²´
            full_connected_content = ""
            has_full_connected = False  # ê°•ì œ ë¹„í™œì„±í™”
            
            if False:  # ì™„ì „ ë¹„í™œì„±í™”
                full_page_info = next(p for p in pages_info if p.get('is_full_connected', False))
                print(f"ğŸ”— Processing full connected image with smart chunking for cross-page analysis...")
                
                try:
                    from PIL import Image
                    full_image = Image.open(full_page_info['image_path'])
                    
                    # GPT Vision ìµœì  í¬ê¸°: ë” ì‘ì€ ì²­í¬ë¡œ ë³€ê²½ (í‘œ ì¸ì‹ ê°œì„ ìš©)
                    MAX_CHUNK_HEIGHT = 2000  # 4000 â†’ 2000ìœ¼ë¡œ ì¶•ì†Œ
                    OVERLAP_HEIGHT = 600     # ê²¹ì¹¨ ì˜ì—­ ì¶•ì†Œ
                    
                    width, height = full_image.size
                    print(f"Full image dimensions: {width}x{height}")
                    
                    # ì²­í‚¹ì´ í•„ìš”í•œì§€ í™•ì¸
                    if height <= MAX_CHUNK_HEIGHT:
                        print("Full image is small enough, processing as single chunk")
                        chunks = [full_image]
                        chunk_positions = [(0, height)]
                    else:
                        print(f"Splitting into chunks with {OVERLAP_HEIGHT}px overlap")
                        chunks = []
                        chunk_positions = []
                        
                        y_start = 0
                        while y_start < height:
                            y_end = min(y_start + MAX_CHUNK_HEIGHT, height)
                            
                            # ë§ˆì§€ë§‰ ì²­í¬ê°€ ë„ˆë¬´ ì‘ìœ¼ë©´ ì´ì „ ì²­í¬ì™€ í•©ì¹¨
                            if height - y_end < OVERLAP_HEIGHT:
                                y_end = height
                            
                            chunk = full_image.crop((0, y_start, width, y_end))
                            chunks.append(chunk)
                            chunk_positions.append((y_start, y_end))
                            
                            print(f"Created chunk: y={y_start}-{y_end} ({y_end-y_start}px height)")
                            
                            # ë‹¤ìŒ ì²­í¬ ì‹œì‘ì  (ê²¹ì¹¨ ì ìš©)
                            if y_end >= height:
                                break
                            y_start = y_end - OVERLAP_HEIGHT
                    
                    # ê° ì²­í¬ë³„ë¡œ í˜ì´ì§€ ê²½ê³„ ë¶„ì„
                    chunk_results = []
                    for idx, chunk in enumerate(chunks):
                        y_start, y_end = chunk_positions[idx]
                        
                        # ì„ì‹œ ì²­í¬ íŒŒì¼ ì €ì¥
                        chunk_path = self.temp_dir / f"chunk_{upload_id}_{idx}.png"
                        chunk.save(str(chunk_path), 'PNG', optimize=True)
                        
                        with open(chunk_path, "rb") as chunk_file:
                            chunk_base64 = base64.b64encode(chunk_file.read()).decode('utf-8')
                        
                        chunk_prompt = f"""
ğŸ“‹ **ì—°ê²° ë¬¸ì„œ ì²­í¬ {idx+1}/{len(chunks)} í…ìŠ¤íŠ¸ ë¶„ì„**
êµ¬ê°„: y={y_start}-{y_end} ({y_end-y_start}px)

**ë¶„ì„ ëª©í‘œ**:
ì´ ì´ë¯¸ì§€ ì²­í¬ì—ì„œ í•œêµ­ì–´ í…ìŠ¤íŠ¸ë¥¼ ëª¨ë‘ ì½ì–´ì„œ ë‹¤ìŒ íŒ¨í„´ë“¤ì„ ì°¾ì•„ì£¼ì„¸ìš”.

**ğŸ” ì°¾ì„ íŒ¨í„´ë“¤**:
1. **ë°ì´í„° í…Œì´ë¸”**: í–‰ê³¼ ì—´ë¡œ êµ¬ì„±ëœ ê²©ìí˜• ë°ì´í„° êµ¬ì¡°
   - í—¤ë” í–‰ì´ ìˆê³  ê·¸ ì•„ë˜ ì—¬ëŸ¬ ë°ì´í„° í–‰ì´ ìˆëŠ” êµ¬ì¡°
   - ì˜ˆ: í”„ë¡œì„¸ìŠ¤, ë„ì°©ì‹œê°„, ì‹¤í–‰ì‹œê°„ ë“±ì˜ ì»¬ëŸ¼ì´ ìˆëŠ” í‘œ

2. **ë¶„í• ëœ ì½˜í…ì¸ **: ì²­í¬ ê²½ê³„ì—ì„œ ì˜ë¦° í…ìŠ¤íŠ¸ ë¸”ë¡
   - ìƒë‹¨ì—ì„œ ì‹œì‘ë˜ì§€ë§Œ ì•ë¶€ë¶„ì´ ì—†ëŠ” í…ìŠ¤íŠ¸
   - í•˜ë‹¨ì—ì„œ ëë‚˜ì§€ë§Œ ë’·ë¶€ë¶„ì´ ë‹¤ìŒ ì²­í¬ì— ìˆëŠ” í…ìŠ¤íŠ¸
   - ë²ˆí˜¸ê°€ ìˆëŠ” í•­ëª©ì—ì„œ ì„ íƒì§€ê°€ ë¶ˆì™„ì „í•œ ê²½ìš°

**ğŸ“‹ ì¶œë ¥ í˜•ì‹**:

**ë°ì´í„° í…Œì´ë¸” ë°œê²¬ì‹œ**:
### ğŸ“Š í…Œì´ë¸” ë°œê²¬: [ìœ„ì¹˜ ì„¤ëª…]
| ì»¬ëŸ¼1 | ì»¬ëŸ¼2 | ì»¬ëŸ¼3 |
|-------|-------|-------|
| í–‰1ë°ì´í„°1 | í–‰1ë°ì´í„°2 | í–‰1ë°ì´í„°3 |
| í–‰2ë°ì´í„°1 | í–‰2ë°ì´í„°2 | í–‰2ë°ì´í„°3 |
| í–‰3ë°ì´í„°1 | í–‰3ë°ì´í„°2 | í–‰3ë°ì´í„°3 |

**ë¶„í•  ì½˜í…ì¸  ë°œê²¬ì‹œ**:
### ğŸ”— ë¶„í•  ì½˜í…ì¸ : í•­ëª© X
- ë‚´ìš©: [ì™„ì „í•œ í…ìŠ¤íŠ¸ ë‚´ìš©]
- ì„ íƒì§€: [â‘ â‘¡â‘¢â‘£â‘¤ ëª¨ë“  ì˜µì…˜]
- ìƒíƒœ: [ì™„ì „í•¨/ë¶ˆì™„ì „í•¨]

**ì•„ë¬´ê²ƒë„ íŠ¹ë³„í•œ ê²Œ ì—†ìœ¼ë©´**: "ì²­í¬ {idx+1}: íŠ¹ë³„í•œ íŒ¨í„´ ì—†ìŒ"

âš¡ **ì¤‘ìš”**: ëª¨ë“  í•œêµ­ì–´ í…ìŠ¤íŠ¸ë¥¼ ì •í™•íˆ ì½ì–´ì„œ ì™„ì „í•œ ì •ë³´ë¥¼ ì œê³µí•´ì£¼ì„¸ìš”.
                        """.strip()
                        
                        chunk_response = await self.openai_client.chat.completions.create(
                            model="gpt-4o",
                            messages=[{
                                "role": "user",
                                "content": [
                                    {"type": "text", "text": chunk_prompt},
                                    {"type": "image_url", "image_url": {"url": f"data:image/png;base64,{chunk_base64}"}}
                                ]
                            }],
                            max_tokens=3000,
                            temperature=0.1
                        )
                        
                        chunk_content = chunk_response.choices[0].message.content.strip()
                        chunk_results.append(f"=== ì²­í¬ {idx+1} ê²°ê³¼ ===\n{chunk_content}")
                        print(f"âœ… Chunk {idx+1} analysis completed: {len(chunk_content)} chars")
                        
                        # ì„ì‹œ ì²­í¬ íŒŒì¼ ì‚­ì œ
                        os.unlink(chunk_path)
                    
                    # ëª¨ë“  ì²­í¬ ê²°ê³¼ í†µí•©
                    full_connected_content = "\n\n".join(chunk_results)
                    print(f"âœ… Full connected chunked analysis completed: {len(full_connected_content)} total chars")
                    
                except Exception as e:
                    print(f"âš ï¸ Full connected image processing failed: {e}")
            
            # ê°œë³„ í˜ì´ì§€ ì²˜ë¦¬ (ë‹¨ìˆœí™” - í‘œ ê°ì§€ ì‹œìŠ¤í…œ ì œê±°)
            individual_pages = [p for p in pages_info if not p.get('is_full_connected', False)]
            
            for i, page_info in enumerate(individual_pages):
                page_num = page_info['page_number']
                image_path = page_info['image_path']
                
                print(f"ğŸ“„ í˜ì´ì§€ {page_num}/{len(individual_pages)} ë‹¨ìˆœ ì²˜ë¦¬")
                
                try:
                    # ì´ë¯¸ì§€ë¥¼ base64ë¡œ ì¸ì½”ë”©
                    with open(image_path, "rb") as image_file:
                        base64_image = base64.b64encode(image_file.read()).decode('utf-8')
                    
                    # ì „ì²´ ì—°ê²° ë¶„ì„ì—ì„œ í‘œ ì •ë³´ ì¶”ì¶œ
                    table_hints = ""
                    boundary_hints = ""
                    
                    if full_connected_content:
                        # í‘œ ì •ë³´ ì¶”ì¶œ
                        if "ğŸ“Š í‘œ ë°œê²¬:" in full_connected_content:
                            table_section = full_connected_content.split("ğŸ“Š í‘œ ë°œê²¬:")[1].split("ğŸ”— ê²½ê³„ ë¬¸ì œ")[0] if "ğŸ”— ê²½ê³„ ë¬¸ì œ" in full_connected_content else full_connected_content.split("ğŸ“Š í‘œ ë°œê²¬:")[1]
                            table_hints = f"ğŸ¯ ì „ì²´ ë¶„ì„ì—ì„œ ë°œê²¬ëœ í‘œ:\n{table_section[:400]}..."
                        
                        # í˜ì´ì§€ ê²½ê³„ ì •ë³´ ì¶”ì¶œ  
                        if "ğŸ”— ê²½ê³„ ë¬¸ì œ" in full_connected_content:
                            boundary_section = full_connected_content.split("ğŸ”— ê²½ê³„ ë¬¸ì œ")[1]
                            if f"{page_num}ë²ˆ" in boundary_section:
                                boundary_hints = f"âš ï¸ í˜ì´ì§€ {page_num} ê²½ê³„ ë¬¸ì œ ì£¼ì˜:\n{boundary_section[:300]}..."

                    # í˜ì´ì§€ ì´ë¯¸ì§€ ë¶„ì„
                    from PIL import Image
                    page_image = Image.open(image_path)
                    width, height = page_image.size

                    # ğŸ–¼ï¸ í˜„ì¬ í˜ì´ì§€ ì´ë¯¸ì§€ë“¤ í•„í„°ë§
                    page_images = []
                    if extracted_images:
                        page_images = [img for img in extracted_images if img['page_number'] == page_num]
                    
                    # ğŸ“· ì´ë¯¸ì§€ ì°¸ì¡° íŒíŠ¸ ìƒì„±
                    image_hints = ""
                    if page_images:
                        image_hints = f"""
ğŸ“· **í˜ì´ì§€ {page_num}ì—ì„œ ì¶”ì¶œëœ ì´ë¯¸ì§€ë“¤**:
"""
                        for img in page_images[:5]:  # ìµœëŒ€ 5ê°œ ì´ë¯¸ì§€ë§Œ í‘œì‹œ
                            image_hints += f"- {img['image_id']}: {img['web_path']}\n"
                        
                        if len(page_images) > 5:
                            image_hints += f"- ... ì´ {len(page_images)}ê°œ ì´ë¯¸ì§€ ì¶”ì¶œë¨\n"
                    
                    # ğŸš€ Ultra-Enhanced í‘œ ë° ì´ë¯¸ì§€ ì²˜ë¦¬ í”„ë¡¬í”„íŠ¸
                    prompt = f"""
ğŸ” **ì •ë°€ í•œêµ­ì–´ í…ìŠ¤íŠ¸ ì¶”ì¶œ - í˜ì´ì§€ {page_num}** (Plain Text ì¶œë ¥)

{image_hints}

ğŸ“‹ **ë¯¸ì…˜**: ì´ í˜ì´ì§€ì˜ **ì‹¤ì œ ë‚´ìš©ì„ Plain Text**ë¡œ ì •í™•íˆ ì¶”ì¶œí•˜ì„¸ìš”.

ğŸš¨ **í•µì‹¬ ë³€ê²½**: Markdown ì‚¬ìš© ê¸ˆì§€ â†’ Plain Textë¡œ ì¶œë ¥
- **íŠ¹ìˆ˜ë¬¸ì ë¬¸ì œ í•´ê²°**: *, #, |, [], % ë“±ì„ ê·¸ëŒ€ë¡œ í‘œí˜„
- **ì´ë¯¸ì§€ ì°¸ì¡° ë³€ê²½**: ![IMG_XXX] â†’ IMG_XXX_IMAGE
- **í‘œ í˜•ì‹ ë³€ê²½**: | í…Œì´ë¸” â†’ ë‹¨ìˆœ í…ìŠ¤íŠ¸

âš¡ **ì¶”ì¶œ ê·œì¹™**:

1ï¸âƒ£ **í‘œ ë°ì´í„° ì™„ì „ ì¶”ì¶œ** (ìµœìš°ì„ ):
   - **í—¤ë” í–‰**: ëª¨ë“  ì»¬ëŸ¼ëª… ì¶”ì¶œ
   - **ëª¨ë“  ë°ì´í„° í–‰**: P1,P2,P3... ë˜ëŠ” ê¸°íƒ€ ëª¨ë“  í–‰
   - **ë¹ˆ ì…€ í‘œì‹œ**: ê³µë°±ì´ë‚˜ "-"ë¡œ í‘œì‹œ
   - **í‘œ ì—†ìœ¼ë©´**: "í‘œ ì—†ìŒ" ëª…ì‹œ

2ï¸âƒ£ **ì´ë¯¸ì§€ ì„ íƒì§€ ì²˜ë¦¬** (ğŸš¨ ë§¤ìš° ì¤‘ìš”!):
   - **ì´ë¯¸ì§€ê°€ ì„ íƒì§€ì¸ ê²½ìš°**: ë°˜ë“œì‹œ "IMG_XXX_IMAGE" í˜•íƒœë¡œ ì¶œë ¥
   - **ì˜ˆì‹œ**: â‘  IMG_001_IMAGE, â‘¡ IMG_002_IMAGE, â‘¢ í…ìŠ¤íŠ¸ ì„ íƒì§€, â‘£ IMG_003_IMAGE
   - **ë‹¤ì´ì–´ê·¸ë¨/ê·¸ë˜í”„**: "DIAGRAM_IMAGE" ë˜ëŠ” êµ¬ì²´ì  ì„¤ëª… + "_IMAGE"
   - **ğŸ” ì´ë¯¸ì§€ ì¸ì‹ ê°•í™”**: ì„ íƒì§€ ì˜ì—­ì—ì„œ í…ìŠ¤íŠ¸ê°€ ì•„ë‹Œ ê·¸ë¦¼/ë„í˜•/ê·¸ë˜í”„ê°€ ë³´ì´ë©´ ë°˜ë“œì‹œ IMG_XXX_IMAGEë¡œ í‘œê¸°
   - **ì´ë¯¸ì§€ íŒíŠ¸ í™œìš©**: í˜ì´ì§€ì— ì¶”ì¶œëœ ì´ë¯¸ì§€ ëª©ë¡ì„ ì°¸ì¡°í•˜ì—¬ í•´ë‹¹ ìœ„ì¹˜ì˜ ì´ë¯¸ì§€ ID ì‚¬ìš©

3ï¸âƒ£ **í˜ì´ì§€ ê²½ê³„ ì„ íƒì§€ ì™„ì „ ì²˜ë¦¬** (ğŸš¨ ì ˆëŒ€ ì‹¤ìˆ˜ ê¸ˆì§€!):
   - **í˜ì´ì§€ í•˜ë‹¨ ì„¸ì‹¬í•œ ê²€ì‚¬**: í˜ì´ì§€ ë§¨ ì•„ë˜ 5ì¤„ì„ ë°˜ë“œì‹œ ì •ë°€ ê²€ì‚¬
   - **ì„ íƒì§€ ë²ˆí˜¸ ì²´í¬**: â‘ â‘¡â‘¢â‘£â‘¤ ë˜ëŠ” 1)2)3)4) ì¤‘ í•˜ë‚˜ë¼ë„ ë¹ ì§€ë©´ ë‹¤ìŒ í˜ì´ì§€ì—ì„œ ì™„ì„±ëœ ì„ íƒì§€ë¥¼ ì°¾ì•„ ë³‘í•©
   - **ğŸ” ì™„ì „í•œ ì„ íƒì§€ ì¶”ì¶œ**:
     * í˜ì´ì§€ í•˜ë‹¨ì— â‘ â‘¡ë§Œ ìˆìœ¼ë©´ â†’ ë‹¤ìŒ í˜ì´ì§€ ìƒë‹¨ì—ì„œ â‘¢â‘£â‘¤ íƒì§€í•˜ì—¬ ì™„ì „í•œ ì„ íƒì§€ ìƒì„±
     * í˜ì´ì§€ í•˜ë‹¨ì— â‘ â‘¡â‘¢ë§Œ ìˆìœ¼ë©´ â†’ ë‹¤ìŒ í˜ì´ì§€ ìƒë‹¨ì—ì„œ â‘£â‘¤ íƒì§€í•˜ì—¬ ì™„ì „í•œ ì„ íƒì§€ ìƒì„±
     * í˜ì´ì§€ í•˜ë‹¨ì— â‘ â‘¡â‘¢â‘£ë§Œ ìˆìœ¼ë©´ â†’ ë‹¤ìŒ í˜ì´ì§€ ìƒë‹¨ì—ì„œ â‘¤ íƒì§€í•˜ì—¬ ì™„ì „í•œ ì„ íƒì§€ ìƒì„±
   - **ì„ íƒì§€ ë³‘í•© ìš°ì„ **: ë¶ˆì™„ì „í•œ ì„ íƒì§€ëŠ” ì ˆëŒ€ ì¶œë ¥í•˜ì§€ ë§ê³  ë‹¤ìŒ í˜ì´ì§€ ë‚´ìš©ê³¼ í•©ì³ì„œ ì™„ì „í•œ ì„ íƒì§€ë¡œë§Œ ì¶œë ¥
   - **ì„ íƒì§€ ë²ˆí˜¸ ë³´ì¡´**: â‘ â‘¡â‘¢â‘£â‘¤ ë˜ëŠ” 1)2)3)4)5) ë²ˆí˜¸ë¥¼ ì ˆëŒ€ ìƒëµí•˜ì§€ ë§ê³  ì •í™•íˆ í‘œê¸°

4ï¸âƒ£ **ì½”ë“œ ë“¤ì—¬ì“°ê¸° ì™„ì „ ë³´ì¡´** (ğŸš¨ ê³µë°± í•˜ë‚˜ë„ ë†“ì¹˜ì§€ ë§ ê²ƒ!):
   - **ì½”ë“œ íŒ¨í„´ ê°ì§€**: 
     * `int`, `public`, `class`, `function`, `while`, `for`, `if` í‚¤ì›Œë“œ ë°œê²¬ì‹œ
     * ì¤‘ê´„í˜¸ `{{}}`, ê´„í˜¸ `()`, ëŒ€ê´„í˜¸ `[]` í¬í•¨ëœ ì—¬ëŸ¬ ì¤„ êµ¬ì¡°
     * 4ì¹¸ ì´ìƒ ì¼ê´€ëœ ë“¤ì—¬ì“°ê¸°ê°€ ìˆëŠ” ë¸”ë¡
   - **ğŸ” ë“¤ì—¬ì“°ê¸° ë³´ì¡´ ê·œì¹™**:
     * ì›ë³¸ ê³µë°±/íƒ­ì„ 1:1ë¡œ ì •í™•íˆ ë³µì‚¬
     * ë“¤ì—¬ì“°ê¸° ë ˆë²¨ì„ ì„ì˜ë¡œ ë³€ê²½í•˜ì§€ ë§ ê²ƒ
     * ì½”ë“œ ì•ë’¤ë¡œ [CODE_START]/[CODE_END] ë§ˆì»¤ í•„ìˆ˜ ì‚½ì…
   - **ì½”ë“œ ì˜ˆì‹œ**:
     [CODE_START]
     int a = 0, sum = 0;
     do {{
         a++;          â† ì´ ë“¤ì—¬ì“°ê¸° ì •í™•íˆ ë³´ì¡´
         sum += a;     â† ì´ ë“¤ì—¬ì“°ê¸° ì •í™•íˆ ë³´ì¡´
     }} while(a > 10);
     [CODE_END]

ğŸ¯ **Plain Text ì¶œë ¥ í˜•ì‹**:

=== í˜ì´ì§€ {page_num} ===

ë¬¸ì œ 1ë²ˆ
ì§ˆë¬¸ ë‚´ìš©ì„ ì •í™•íˆ ì‘ì„±
ì„ íƒì§€ 1: ë‚´ìš©
ì„ íƒì§€ 2: ë‚´ìš©  
ì„ íƒì§€ 3: IMG_XXX_IMAGE
ì„ íƒì§€ 4: ë‚´ìš©

[í‘œ ë°ì´í„° - ë²”ìš© ì²˜ë¦¬]
ì œëª©: (í‘œ ìœ„ë‚˜ ì•„ë˜ ì œëª©ì´ ìˆë‹¤ë©´ ì¶”ì¶œ)
í—¤ë”: ì»¬ëŸ¼1 | ì»¬ëŸ¼2 | ì»¬ëŸ¼3 | ... (ì‹¤ì œ í—¤ë”ëª…)
í–‰1: ë°ì´í„°1 | ë°ì´í„°2 | ë°ì´í„°3 | ... (ì²« ë²ˆì§¸ ë°ì´í„° í–‰)
í–‰2: ë°ì´í„°4 | ë°ì´í„°5 | ë°ì´í„°6 | ... (ë‘ ë²ˆì§¸ ë°ì´í„° í–‰)
í–‰3: ë°ì´í„°7 | ë°ì´í„°8 | ë°ì´í„°9 | ... (ì„¸ ë²ˆì§¸ ë°ì´í„° í–‰)
... (í‘œì— ìˆëŠ” ëª¨ë“  ë°ì´í„° í–‰ì„ ìˆœì„œëŒ€ë¡œ)

ğŸš¨ **ë²”ìš© í‘œ ì²˜ë¦¬ ê·œì¹™**:
- **ìë™ í‘œ ê°ì§€**: ê²©ì êµ¬ì¡°, ì¼ì •í•œ ê°„ê²©, ì •ë ¬ëœ ë°ì´í„° íŒ¨í„´ ì¸ì‹
- **í—¤ë” ìë™ íŒë‹¨**: ì²« ë²ˆì§¸ í–‰ì„ í—¤ë”ë¡œ ì¶”ì • (ì»¬ëŸ¼ëª… íŠ¹ì„± í™•ì¸)
- **ëª¨ë“  í–‰ ì¶”ì¶œ**: í—¤ë” ë‹¤ìŒì˜ ëª¨ë“  ë°ì´í„° í–‰ì„ ìˆœì„œëŒ€ë¡œ ì¶”ì¶œ
- **ë‹¤ì–‘í•œ í˜•íƒœ ì§€ì›**: P1,P2,P3 / A,B,C / 1,2,3 / í•­ëª©1,í•­ëª©2 / íšŒì‚¬ëª…,ë§¤ì¶œì•¡ ë“±
- **ë¹ˆ ì…€ ì²˜ë¦¬**: ë¹ˆ ì¹¸ì€ "ë¹ˆì¹¸" ë˜ëŠ” "-" ë¡œ í‘œì‹œ
- **ê°€ì§œ ë°ì´í„° ì ˆëŒ€ ê¸ˆì§€**: ì‹¤ì œ ë³´ì´ëŠ” ë°ì´í„°ë§Œ ì¶”ì¶œ
- **íŠ¹ìˆ˜ë¬¸ì**: *, %, &, @ ë“± ëª¨ë‘ ê·¸ëŒ€ë¡œ í‘œì‹œ

ğŸ“‹ **ìµœì¢… ì¶œë ¥**: Plain Textë§Œ ì‚¬ìš© (Markdown ê¸ˆì§€)
                    """.strip()
                    
                    response = await self.openai_client.chat.completions.create(
                        model="gpt-4o",
                        messages=[
                            {
                                "role": "user",
                                "content": [
                                    {"type": "text", "text": prompt},
                                    {
                                        "type": "image_url",
                                        "image_url": {
                                            "url": f"data:image/png;base64,{base64_image}",
                                            "detail": "high"
                                        }
                                    }
                                ]
                            }
                        ],
                        max_tokens=10000,
                        temperature=0.01
                    )
                    
                    page_content = response.choices[0].message.content.strip()
                    print(f"âœ… í˜ì´ì§€ {page_num} ì²˜ë¦¬ ì™„ë£Œ: {len(page_content)}chars")
                    
                    # ğŸ”— í˜ì´ì§€ê°„ ì„ íƒì§€ ì—°ê²° ì „ì²˜ë¦¬
                    processed_content = self._preprocess_page_connections(page_content, page_num, all_markdown_content)
                    
                    # í˜ì´ì§€ë³„ ë‚´ìš© ì¶”ê°€
                    if i > 0:  # ì²« í˜ì´ì§€ê°€ ì•„ë‹Œ ê²½ìš°
                        all_markdown_content.append("\n---\n")  # í˜ì´ì§€ êµ¬ë¶„ì„ 
                    
                    all_markdown_content.append(f"=== í˜ì´ì§€ {page_num} ===\n\n{processed_content}")
                    
                    # í† í° ì‚¬ìš©ëŸ‰ ëˆ„ì 
                    tokens_used = response.usage.total_tokens
                    cost = tokens_used * 0.00001  # GPT-4O ëŒ€ëµ ë¹„ìš©
                    total_tokens_used += tokens_used
                    total_cost += cost
                    
                    # ì¶©ë¶„í•œ ì§€ì—°ìœ¼ë¡œ API ì•ˆì •ì„± í–¥ìƒ
                    await asyncio.sleep(4)  # ê°•í™”ëœ ëŒ€ê¸°
                    
                except Exception as page_error:
                    print(f"Failed to convert page {page_num}: {page_error}")
                    # ì‹¤íŒ¨í•œ í˜ì´ì§€ëŠ” ë¹ˆ ë‚´ìš©ìœ¼ë¡œ ì²˜ë¦¬
                    all_markdown_content.append(f"# Page {page_num}\n\n[í˜ì´ì§€ ë³€í™˜ ì‹¤íŒ¨: {str(page_error)}]")
            
            # ğŸ”— ì „ì²´ ì—°ê²° ì´ë¯¸ì§€ ë¶„ì„ ê²°ê³¼ ì €ì¥ (ë§ˆí¬ë‹¤ìš´ì— í¬í•¨í•˜ì§€ ì•ŠìŒ - ë³„ë„ ì²˜ë¦¬)
            cross_page_issues = ""
            if full_connected_content.strip():
                cross_page_issues = full_connected_content
                print(f"âœ… Full connected image analysis completed: {len(full_connected_content)} chars")
                print("í˜ì´ì§€ ê²½ê³„ ë¶„ì„ ê²°ê³¼ëŠ” ë³„ë„ ì €ì¥ë¨ - ë§ˆí¬ë‹¤ìš´ ì¶”ì¶œê³¼ ë¶„ë¦¬ ì²˜ë¦¬")
            
            # ì „ì²´ Markdown ë¬¸ì„œ ìƒì„±
            complete_markdown = "\n\n".join(all_markdown_content)
            
            print(f"GPT Vision conversion completed:")
            print(f"  - Total pages processed: {len(individual_pages)}")
            print(f"  - Has full connected analysis: {bool(full_connected_content)}")
            print(f"  - Total markdown length: {len(complete_markdown)} characters") 
            print(f"  - Total tokens used: {total_tokens_used}")
            print(f"  - Total estimated cost: ${total_cost:.4f}")
            
            # ìƒ˜í”Œ ì¶œë ¥
            print(f"=== Markdown ë³€í™˜ ê²°ê³¼ ìƒ˜í”Œ (ì²˜ìŒ 1000ì) ===")
            print(complete_markdown[:1000])
            print("=== ìƒ˜í”Œ ë ===")
            
            await self._log_processing_step(upload_id, "pages_to_markdown", "completed", {
                'total_pages': len(pages_info),
                'markdown_length': len(complete_markdown),
                'tokens_used': total_tokens_used,
                'estimated_cost': total_cost
            }, db)
            
            return {
                'success': True,
                'markdown_content': complete_markdown,
                'cross_page_issues': cross_page_issues,  # í˜ì´ì§€ ê²½ê³„ ë¬¸ì œ ë³„ë„ ì „ë‹¬
                'pages_processed': len(pages_info),
                'tokens_used': total_tokens_used,
                'estimated_cost': total_cost
            }
            
        except Exception as e:
            logger.error(f"Pages to Markdown conversion error: {str(e)}")
            await self._log_processing_step(upload_id, "pages_to_markdown", "failed", 
                                          {"error": str(e)}, db)
            return {'success': False, 'error': str(e)}
    
    async def _structure_markdown_content(self, upload_id: int, markdown_result: Dict, db: Session) -> Dict[str, Any]:
        """3ë‹¨ê³„: Claudeë¡œ ì „ì²´ Markdownì„ ë¬¸ì œë‹¨ìœ„ë¡œ êµ¬ì¡°í™”"""
        try:
            await self._log_processing_step(upload_id, "markdown_structuring", "processing", {}, db)
            
            if not self.claude_client:
                print("ERROR: Claude API key not set - cannot perform markdown structuring")
                return {
                    'success': False,
                    'questions': [],
                    'materials': [],
                    'chapters': [],
                    'error': 'Claude API key not set'
                }
            
            markdown_content = markdown_result.get('markdown_content', '')
            if not markdown_content:
                print("No markdown content to structure")
                return {
                    'success': False,
                    'questions': [],
                    'materials': [],
                    'chapters': [],
                    'error': 'No markdown content provided'
                }
            
            print(f"Structuring {len(markdown_content)} characters of Markdown content with Claude...")
            
            # ğŸ¯ GPT Visionì´ ì¸ì‹í•œ ì‹¤ì œ ë¬¸ì œ ìˆ˜ íŒŒì•…
            cross_page_issues = markdown_result.get('cross_page_issues', '')
            estimated_question_count = self._estimate_question_count_from_markdown(markdown_content)
            print(f"ğŸ“Š GPT Vision ë¶„ì„ ê²°ê³¼:")
            print(f"  - ì¶”ì • ì‹¤ì œ ë¬¸ì œ ìˆ˜: {estimated_question_count}ê°œ")
            if cross_page_issues:
                print(f"  - í˜ì´ì§€ ê²½ê³„ ë¬¸ì œ: ê°ì§€ë¨ ({len(cross_page_issues)} chars)")
            
            # ëŒ€ìš©ëŸ‰ ë¬¸ì„œ ìŠ¤ë§ˆíŠ¸ ì²­í‚¹ ì²˜ë¦¬ - Claude í† í° ì œí•œ í•´ê²° + ì‹¤ì œ ë¬¸ì œ ìˆ˜ ê¸°ë°˜
            max_chunk_size = 80000  # Claudeì˜ ì…ë ¥ í† í° ì œí•œì„ ê³ ë ¤í•œ ì²­í¬ í¬ê¸°
            chunks = []
            
            # ğŸ§  ìŠ¤ë§ˆíŠ¸ ì²­í‚¹ ì „ëµ - ì‹¤ì œ ë¬¸ì œ ìˆ˜ ê¸°ë°˜
            if len(markdown_content) > max_chunk_size:
                print(f"Large document detected ({len(markdown_content)} chars). Implementing question-aware smart chunking...")
                
                # í˜ì´ì§€ë³„ ë¬¸ì œ ë¶„í¬ ë¶„ì„
                import re
                page_pattern = r'# Page (\d+)\n\n(.*?)(?=# Page \d+|$)'
                page_matches = re.findall(page_pattern, markdown_content, re.DOTALL)
                
                print(f"ğŸ“„ Detected {len(page_matches)} pages with content")
                
                # ê° í˜ì´ì§€ì˜ ë¬¸ì œ ìˆ˜ ì¶”ì •
                page_question_counts = []
                total_estimated_questions = 0
                for page_num, page_content in page_matches:
                    question_count = self._count_questions_in_page(page_content)
                    page_question_counts.append((int(page_num), question_count, page_content))
                    total_estimated_questions += question_count
                    print(f"  Page {page_num}: ~{question_count} questions")
                
                print(f"ğŸ“Š Total estimated questions from page analysis: {total_estimated_questions}")
                
                # ë¬¸ì œ ë°€ë„ ê¸°ë°˜ ìŠ¤ë§ˆíŠ¸ ì²­í‚¹ (15-20ë¬¸ì œì”© ê·¸ë£¹í™”) - ë¹ˆ í˜ì´ì§€ ì œì™¸
                # ë” í° ì²­í¬ë¡œ ê´€ë ¨ ë¬¸ì œë“¤ì„ í•¨ê»˜ ê·¸ë£¹í™”í•˜ì—¬ ë‹¨í¸í™” ë°©ì§€ ë° ë¬¸ì œ ëˆ„ë½ ë°©ì§€
                target_questions_per_chunk = 25  # ëŒ€í­ ì¦ê°€: í˜ì´ì§€ ê²½ê³„ ë¬¸ì œ ìµœì†Œí™”
                current_chunk_pages = []
                current_chunk_questions = 0
                
                # ë¬¸ì œê°€ ìˆëŠ” í˜ì´ì§€ë§Œ í•„í„°ë§
                valid_pages = [(page_num, question_count, page_content) 
                              for page_num, question_count, page_content in page_question_counts 
                              if question_count > 0]
                
                print(f"ğŸ“„ Filtering: {len(valid_pages)} pages with questions out of {len(page_question_counts)} total pages")
                
                for page_num, question_count, page_content in valid_pages:
                    page_header = f"# Page {page_num}\n\n"
                    full_page = page_header + page_content
                    
                    # ì²­í¬ì— ì¶”ê°€í• ì§€ íŒë‹¨
                    if (current_chunk_questions + question_count <= target_questions_per_chunk or 
                        not current_chunk_pages):
                        current_chunk_pages.append(full_page)
                        current_chunk_questions += question_count
                    else:
                        # í˜„ì¬ ì²­í¬ ì™„ë£Œ
                        chunk_content = "\n\n".join(current_chunk_pages)
                        chunks.append(chunk_content)
                        print(f"Created chunk: {len(current_chunk_pages)} pages, ~{current_chunk_questions} questions ({len(chunk_content)} chars)")
                        
                        # ìƒˆ ì²­í¬ ì‹œì‘
                        current_chunk_pages = [full_page]
                        current_chunk_questions = question_count
                
                # ë§ˆì§€ë§‰ ì²­í¬ ì²˜ë¦¬
                if current_chunk_pages:
                    chunk_content = "\n\n".join(current_chunk_pages)
                    chunks.append(chunk_content)
                    print(f"Final chunk: {len(current_chunk_pages)} pages, ~{current_chunk_questions} questions ({len(chunk_content)} chars)")
                
                print(f"ğŸ“¦ Document split into {len(chunks)} question-aware chunks")
                
            else:
                chunks = [markdown_content]
            
            # ê° ì²­í¬ë³„ë¡œ Claude ë¶„ì„ ì‹¤í–‰
            all_questions = []
            all_materials = []
            all_chapters = []
            
            for chunk_idx, chunk in enumerate(chunks):
                print(f"Processing chunk {chunk_idx + 1}/{len(chunks)} ({len(chunk)} characters)...")
                
                # Claudeë¡œ êµ¬ì¡°í™”ëœ ë§ˆí¬ë‹¤ìš´ì„ JSONìœ¼ë¡œ ë³€í™˜ (í¬ë¡œìŠ¤ í˜ì´ì§€ ë³´ì™„)
                cross_page_context = cross_page_issues if cross_page_issues else ""
                
                structuring_prompt = f"""
ğŸ¤– **Plain Text êµìœ¡ ì½˜í…ì¸  JSON ë³€í™˜ê¸°** (íŠ¹ìˆ˜ë¬¸ì ì•ˆì „ ì²˜ë¦¬)

ë‹¤ìŒì€ GPT Visionì´ Plain Textë¡œ ë¶„ì„í•œ êµìœ¡ ë¬¸ì„œì…ë‹ˆë‹¤.
íŠ¹ìˆ˜ë¬¸ì(*, %, &, @)ì™€ ì´ë¯¸ì§€ ì°¸ì¡°ë¥¼ ì•ˆì „í•˜ê²Œ ì²˜ë¦¬í•˜ì—¬ ì™„ì „í•œ JSONì„ ìƒì„±í•˜ì„¸ìš”.

ğŸ“Š **í˜ì´ì§€ ë¶„ì„ ê²°ê³¼ (ì²­í¬ {chunk_idx + 1}/{len(chunks)})**:
```text
{chunk}
```

{f'''ğŸ”— **ì¶”ê°€ ë¶„ì„ ê²°ê³¼**:
```text
{cross_page_context[:1000]}...
```

ì´ ì¶”ê°€ ì •ë³´ë¥¼ í™œìš©í•˜ì—¬ ë†“ì¹œ ë‚´ìš©ì„ ë³´ì™„í•˜ì„¸ìš”.
''' if cross_page_context else ''}

ğŸ¯ **ë²”ìš© Plain Text ë³€í™˜ ê·œì¹™**:
1. **ë²”ìš© í‘œ ë°ì´í„° ì¶”ì¶œ**: 
   - ìë™ í‘œ ê°ì§€ (ê²©ìêµ¬ì¡°, ì •ë ¬ëœ ë°ì´í„° ë“±)
   - í—¤ë” ìë™ íŒë‹¨ (ì²« ë²ˆì§¸ í–‰ì˜ ì»¬ëŸ¼ëª… íŠ¹ì„± í™•ì¸)
   - ëª¨ë“  ë°ì´í„° í–‰ ìˆœì„œëŒ€ë¡œ ì¶”ì¶œ (í˜•íƒœ ë¬´ê´€: A,B,C / 1,2,3 / í•­ëª©ëª… ë“±)
   - ë¹ˆ ì…€ì€ null ë˜ëŠ” ë¹ˆ ë¬¸ìì—´ë¡œ ì²˜ë¦¬
2. **ì´ë¯¸ì§€ ì°¸ì¡° ì²˜ë¦¬**: "IMG_XXX_IMAGE" â†’ ![IMG_XXX](/images/upload_XXX/IMG_XXX.png)
3. **íŠ¹ìˆ˜ë¬¸ì ì•ˆì „ ì²˜ë¦¬**: *, %, &, @ ë“±ì„ JSONì—ì„œ ì•ˆì „í•˜ê²Œ ì²˜ë¦¬
4. **í˜ì´ì§€ ì—°ê²° ì²˜ë¦¬**: ì „ì²˜ë¦¬ì—ì„œ ë³‘í•©ëœ ì„ íƒì§€ í™œìš©

ğŸš¨ **ì¤‘ë³µ ë¬¸ì œ ì œê±° ê·œì¹™** (í•µì‹¬ ê°œì„ !):
5. **ë¬¸ì œ ë²ˆí˜¸ ê³ ìœ ì„±**: ì´ë¯¸ ì²˜ë¦¬ëœ ë¬¸ì œ ë²ˆí˜¸ëŠ” ë‹¤ì‹œ ì¶”ì¶œí•˜ì§€ ì•ŠìŒ
6. **ë¶ˆì™„ì „ ì„ íƒì§€ ê°ì§€**: "(ì„ íƒì§€ ë¶ˆì™„ì „ - ë‹¤ìŒ í˜ì´ì§€ í™•ì¸)" í‘œì‹œëœ ë¬¸ì œëŠ” ì„ íƒì§€ë¥¼ 2ê°œ ì´í•˜ë¡œ ì œí•œ
7. **í˜ì´ì§€ ê²½ê³„ ì²˜ë¦¬**: "(ë‹¤ìŒ í˜ì´ì§€ ê³„ì†)" í‘œì‹œëœ ë¬¸ì œëŠ” incomplete_choices: true ë§ˆí‚¹
8. **ì½”ë“œ ë¸”ë¡ ë³´ì¡´**: [CODE_BLOCK_START]...[CODE_BLOCK_END] ì‚¬ì´ ë‚´ìš©ì€ ë“¤ì—¬ì“°ê¸° ì™„ì „ ë³´ì¡´

âš¡ **íŠ¹ë³„ ì²˜ë¦¬ ê·œì¹™**:
- ğŸš¨ **ê°€ì§œ ë°ì´í„° ìƒì„± ì ˆëŒ€ ê¸ˆì§€**: ì—†ëŠ” í‘œ ë°ì´í„° ì„ì˜ ìƒì„± ê¸ˆì§€
- **ì´ë¯¸ì§€ ë³€í™˜**: "IMG_XXX_IMAGE" â†’ ì‹¤ì œ ì´ë¯¸ì§€ ë§ˆí¬ë‹¤ìš´ ì°¸ì¡°ë¡œ ë³€í™˜
- **í‘œ ë°ì´í„° ìš°ì„ **: ì‹¤ì œ ì¶”ì¶œëœ ëª¨ë“  ë°ì´í„° í–‰ ì‚¬ìš©
- **íŠ¹ìˆ˜ë¬¸ì**: JSON ë¬¸ìì—´ì—ì„œ ì´ìŠ¤ì¼€ì´í”„ ì²˜ë¦¬

ğŸ¯ **ì´ˆì—„ê²© ì¶”ì¶œ ê·œì¹™**:
1. **ë¬¸ì œë§Œ ì¶”ì¶œ**: ë°˜ë“œì‹œ ë²ˆí˜¸(1., 2., 3. ë˜ëŠ” 1), 2), 3))ê°€ ìˆëŠ” ì‹¤ì œ ì‹œí—˜ë¬¸ì œë§Œ
   - **ì§ˆë¬¸í˜• íŒ¨í„´**: "~ì€?", "~ëŠ”?", "~ì¸ê°€?", "~í• ê¹Œ?", "~ë‹¤ë©´?", "~ì€ ë¬´ì—‡ì¸ê°€?", "~ì„ êµ¬í•˜ì‹œì˜¤", "~ì„ ê³ ë¥´ì‹œì˜¤", "~í•˜ì‹œì˜¤"
   - **ì§€ì‹œí˜• íŒ¨í„´**: "ë‹¤ìŒ ì¤‘", "ì•„ë˜ì—ì„œ", "ìœ„ì—ì„œ", "ë³´ê¸°ì—ì„œ", "ë‹¤ìŒìœ¼ë¡œ", "ì•Œë§ì€ ê²ƒì€", "ì˜³ì€ ê²ƒì€", "í‹€ë¦° ê²ƒì€", "ê°€ì¥ ì ì ˆí•œ ê²ƒì€"
   - **ì„¤ëª…ë¬¸ ì œì™¸**: "~ì…ë‹ˆë‹¤", "~í•©ë‹ˆë‹¤", "~ë‹¤", "~ì´ë‹¤", "~ëœë‹¤", "~í•œë‹¤", "~ì´ë©°", "~í•˜ì—¬" ë¡œ ëë‚˜ëŠ” ë‹¨ìˆœ ì„œìˆ ë¬¸ ë¬´ì‹œ

2. **í•´ì„¤/ì •ë‹µ í˜ì´ì§€ ì ˆëŒ€ ì œì™¸**: 
   - "í•´ì„¤", "ì •ë‹µ", "í’€ì´", "ë‹µì•ˆ", "í•´ë‹µ", "ì„¤ëª…", "í•´ì„", "ë‹µ:", "ì •ë‹µì€", "í•´ë‹µì§€", "ì±„ì ê¸°ì¤€", "ì ìˆ˜" í¬í•¨ì‹œ ë¬´ì‹œ
   - "~ì˜ ì£¼ìš” ìƒíƒœëŠ”", "~ì˜ íŠ¹ì§•ì€", "~ì€ ë‹¤ìŒê³¼ ê°™ë‹¤", "~ë¼ê³  í•  ìˆ˜ ìˆë‹¤", "~ë¡œ ì •ì˜ëœë‹¤" ë“± ì„¤ëª…ë¬¸ ë¬´ì‹œ
   - "~ë¥¼ ì„¤ëª…í•˜ë©´", "~ì— ëŒ€í•´ ì•Œì•„ë³´ë©´", "~ë¥¼ ì‚´í´ë³´ë©´", "~ëŠ” ì˜ë¯¸í•œë‹¤", "~ë¥¼ ëœ»í•œë‹¤" ë“± í•´ì„¤ íŒ¨í„´ ë¬´ì‹œ
   - ë¬¸ì¥ì´ "ì…ë‹ˆë‹¤", "í•©ë‹ˆë‹¤", "ë‹¤", "ì´ë‹¤", "ëœë‹¤", "í•œë‹¤"ë¡œ ëë‚˜ë©´ í•´ì„¤ë¡œ ê°„ì£¼

3. **í‘œ ì™„ì „ì„± ë³´ì¥**: 
   - í‘œê°€ ìˆìœ¼ë©´ í—¤ë” + ìµœì†Œ 2ê°œ ì´ìƒì˜ ë°ì´í„° í–‰ í•„ìˆ˜
   - ì „ì²´ ë¶„ì„ì—ì„œ ë°œê²¬ëœ í‘œ ë°ì´í„°ë¡œ ë¶ˆì™„ì „í•œ í‘œ ë³´ì™„
   - í—¤ë”ë§Œ ìˆê³  ë°ì´í„°ê°€ ì—†ìœ¼ë©´ í•´ë‹¹ ë¬¸ì œ ì œì™¸ ê³ ë ¤

3. **í•œêµ­ì–´ íŠ¹í™” í•´ì„¤ íŒ¨í„´ ì œê±°**:
   - "ë‹¤ìŒê³¼ ê°™ì´", "ì•„ë˜ì™€ ê°™ì´", "ìœ„ì™€ ê°™ì´", "ì¦‰", "ë”°ë¼ì„œ", "ê·¸ëŸ¬ë¯€ë¡œ", "ê²°ë¡ ì ìœ¼ë¡œ", "ìš”ì•½í•˜ë©´"
   - "~ë¼ í•˜ì˜€ë‹¤", "~ë¼ê³  í–ˆë‹¤", "~ë¼ê³  ë³¼ ìˆ˜ ìˆë‹¤", "~ë¼ê³  ì—¬ê²¨ì§„ë‹¤", "~ë¼ëŠ” ê²ƒì´ë‹¤"
   - "ì˜ˆë¥¼ ë“¤ë©´", "ì˜ˆì‹œ", "ì‚¬ë¡€", "ì‹¤ìŠµ", "ì—°ìŠµ", "ë³µìŠµ", "ì •ë¦¬", "ìš”ì ", "í•µì‹¬"
   - "ì°¸ê³ :", "ì£¼ì˜:", "ì¤‘ìš”:", "ì•Œë¦¼:", "íŒ:", "íŒíŠ¸:", "ê¸°ì–µí•  ì "

3. **ì™„ì „í•œ ë¬¸ì œë§Œ**:
   - question_text: ì™„ì „í•œ ì§ˆë¬¸ ë‚´ìš© (ë°˜ë“œì‹œ ë¬¼ìŒí‘œë‚˜ ì§ˆë¬¸ íŒ¨í„´ í¬í•¨, 15ì ì´ìƒ)
   - passage: ì§€ë¬¸/í‘œ/ì½”ë“œ/ë³´ê¸° ë“± (ì„ íƒì‚¬í•­)
   - options: ë°˜ë“œì‹œ 3ê°œ ì´ìƒì˜ ì™„ì „í•œ ì„ íƒì§€ (â‘ â‘¡â‘¢â‘£â‘¤ ë˜ëŠ” 1)2)3)4)5) í˜•íƒœ)

4. **ì—„ê²©í•œ ì„ íƒì§€ ê²€ì¦**:
   - ê° ì„ íƒì§€ëŠ” 5ê¸€ì ì´ìƒì˜ ì˜ë¯¸ìˆëŠ” ë‚´ìš©
   - ë²ˆí˜¸ íŒ¨í„´ (â‘ â‘¡â‘¢â‘£â‘¤ ë˜ëŠ” 1)2)3)4)5)) í•„ìˆ˜
   - ì—°ì†ì ì¸ ë²ˆí˜¸ ìˆœì„œ (â‘ â‘¡â‘¢â‘£ ë˜ëŠ” 1)2)3)4))
   - ëª¨ë“  ì„ íƒì§€ê°€ ì™„ì „íˆ ì¶”ì¶œë˜ì–´ì•¼ í•¨ (í˜ì´ì§€ ëì—ì„œ ì˜ë¦° ê²½ìš° ì œì™¸)

5. **ë¬´íš¨ íŒ¨í„´ ì œì™¸**:
   - "í˜ì´ì§€", "ëª©ì°¨", "í‘œì§€", "ë¹ˆ í˜ì´ì§€", "ì°¸ê³ ìë£Œ"
   - ë‹¨ìˆœ ë‚˜ì—´ë¬¸, ì •ì˜ë¬¸, ê°œë… ì„¤ëª…ë¬¸
   - ë¬¸ì œë²ˆí˜¸ê°€ 'N/A'ì´ê±°ë‚˜ 0ì¸ ê²½ìš°

6. **ì¤‘ë³µ ì™„ì „ ë°©ì§€**: ê°™ì€ ë²ˆí˜¸ ë¬¸ì œ ë°œê²¬ì‹œ ì²« ë²ˆì§¸ë§Œ ì¶”ì¶œ

ğŸš¨ **ì ˆëŒ€ ê·œì¹™**: 
- ì™„ì „í•˜ì§€ ì•Šì€ ë¬¸ì œëŠ” ì ˆëŒ€ ì¶”ì¶œí•˜ì§€ ë§ˆì„¸ìš”
- í•´ì„¤ì´ë‚˜ ì •ë‹µ í˜ì´ì§€ëŠ” ì ˆëŒ€ ë¬¸ì œë¡œ ì¸ì‹í•˜ì§€ ë§ˆì„¸ìš”
- ì„ íƒì§€ê°€ ì—†ê±°ë‚˜ ë¶€ì¡±í•œ ë¬¸ì œëŠ” ì ˆëŒ€ ì¶”ì¶œí•˜ì§€ ë§ˆì„¸ìš”

ğŸ“‹ **ì„ íƒì§€ ì¶”ì¶œ ê°•í™” ê·œì¹™**:
- ì„ íƒì§€ëŠ” ë°˜ë“œì‹œ ì—°ì†ëœ ë²ˆí˜¸ ìˆœì„œë¡œ ì¶”ì¶œ (â‘ â†’â‘¡â†’â‘¢â†’â‘£â†’â‘¤)
- ì¤‘ê°„ì— ë²ˆí˜¸ê°€ ë¹ ì§€ë©´ í•´ë‹¹ ë¬¸ì œ ì œì™¸ (â‘ â‘¢â‘£ ê°™ì€ íŒ¨í„´ ê¸ˆì§€)
- í˜ì´ì§€ ê²½ê³„ì—ì„œ ì˜ë¦° ì„ íƒì§€ê°€ ìˆìœ¼ë©´ í•´ë‹¹ ë¬¸ì œ ì œì™¸
- ì„ íƒì§€ê°€ 2ê°œ ì´í•˜ì¸ ê²½ìš° ë¬¸ì œì—ì„œ ì œì™¸
- "ë³´ê¸°ì—†ìŒ", "ì£¼ê´€ì‹", "ì„œìˆ í˜•" í‘œì‹œê°€ ìˆìœ¼ë©´ optionsë¥¼ ë¹ˆ ë°°ì—´ë¡œ ì²˜ë¦¬

ğŸ” **ì§ˆë¬¸ íŒ¨í„´ ì¸ì‹ ê°•í™”**:
- ë°˜ë“œì‹œ ì§ˆë¬¸í˜• ì–´ë¯¸ë‚˜ ì§€ì‹œí˜• íŒ¨í„´ì„ í¬í•¨í•´ì•¼ í•¨
- "ë‹¤ìŒ ì¤‘ ì˜³ì€ ê²ƒì€?", "ì•„ë˜ ì„¤ëª…ì— í•´ë‹¹í•˜ëŠ” ê²ƒì€?", "ì˜¬ë°”ë¥¸ ê²ƒì„ ê³ ë¥´ì‹œì˜¤" ë“±
- ë‹¨ìˆœ ì„œìˆ ë¬¸("~ì…ë‹ˆë‹¤", "~ë‹¤", "~ì´ë‹¤")ì€ í•´ì„¤ë¡œ ê°„ì£¼í•˜ì—¬ ì œì™¸

ë‹¤ìŒ JSON í˜•íƒœë¡œ ì •í™•íˆ ë¶„ì„í•´ì£¼ì„¸ìš”:

{{
  "document_analysis": {{
    "title": "ë¬¸ì„œ ì œëª©",
    "subject": "ê³¼ëª©ëª…",
    "exam_type": "ê¸°ì¶œë¬¸ì œ/í•´ì„¤ì§‘/ìš”ì•½ì„œ/êµì¬/ë¬¸ì œì§‘",
    "academic_level": "ì´ˆë“±/ì¤‘ë“±/ê³ ë“±/ëŒ€í•™/ìê²©ì¦",
    "total_pages_analyzed": "ë¶„ì„ëœ í˜ì´ì§€ ìˆ˜",
    "extraction_method": "GPT Vision + Claude"
  }},
  
  "chapters": [
    {{
      "chapter_number": 1,
      "chapter_title": "ë‹¨ì›/ì±•í„° ì œëª©", 
      "main_topics": ["ì£¼ìš” ê°œë…1", "ì£¼ìš” ê°œë…2"],
      "learning_objectives": ["í•™ìŠµëª©í‘œ1", "í•™ìŠµëª©í‘œ2"],
      "page_range": "ì‹œì‘í˜ì´ì§€-ëí˜ì´ì§€"
    }}
  ],
  
  "questions": [
    {{
      "question_id": "Q001",
      "chapter_id": 1,
      "question_number": 1,
      "question_text": "ìˆœìˆ˜í•œ ë¬¸ì œ ë‚´ìš©ë§Œ (ì§€ë¬¸/ë³´ê¸° ì œì™¸)",
      "passage": "ë¬¸ì œì˜ ì§€ë¬¸/ë³´ê¸°/í‘œ/ê·¸ë˜í”„/ì½”ë“œ ë“± (ìˆëŠ” ê²½ìš°)",
      "question_type": "ê°ê´€ì‹/ì£¼ê´€ì‹/ì„œìˆ í˜•/ê³„ì‚°í˜•/ì„ íƒí˜•",
      "options": ["â‘  ì„ íƒì§€1", "â‘¡ ì„ íƒì§€2", "â‘¢ ì„ íƒì§€3", "â‘£ ì„ íƒì§€4"],
      "correct_answer": "ì •ë‹µ ë²ˆí˜¸ë‚˜ ë‚´ìš©",
      "explanation": "í•´ì„¤ (ìˆëŠ” ê²½ìš°)",
      "difficulty_level": "ìƒ/ì¤‘/í•˜",
      "bloom_taxonomy": "ì§€ì‹/ì´í•´/ì ìš©/ë¶„ì„/ì¢…í•©/í‰ê°€",
      "topic_tags": ["ì£¼ì œ1", "ì£¼ì œ2"],
      "estimated_time": "ì˜ˆìƒ ì‹œê°„",
      "keywords": ["í•µì‹¬ì–´1", "í•µì‹¬ì–´2"],
      "source_page": "í˜ì´ì§€ ë²ˆí˜¸",
      "page_location": "ì •í™•í•œ í˜ì´ì§€ ìœ„ì¹˜ (ì˜ˆ: P.3 ìƒë‹¨, P.7 í•˜ë‹¨)",
      "table_data": {{
        "table_id": "TABLE_001",
        "headers": ["í—¤ë”1", "í—¤ë”2", "í—¤ë”3"],
        "rows": [["ë°ì´í„°1", "ë°ì´í„°2", "ë°ì´í„°3"], ["ë°ì´í„°4", "ë°ì´í„°5", "ë°ì´í„°6"]],
        "title": "í‘œ ì œëª©",
        "description": "í‘œ ì„¤ëª…"
      }},
      "related_images": ["IMG_001", "IMG_002"],
      "image_options": {{
        "option_1": "IMG_001",
        "option_2": "IMG_002"
      }},
      "has_table": true,
      "has_code": false,
      "has_figure": true,
      "cross_page_resolved": false
    }}
  ],
  
  "study_materials": [
    {{
      "material_id": "M001",
      "chapter_id": 1, 
      "material_type": "ê°œë…ì„¤ëª…/ê³µì‹/ì •ë¦¬/ì˜ˆì œ/ìš”ì•½/ë„í‘œ/ê·¸ë˜í”„",
      "title": "í•™ìŠµìë£Œ ì œëª©",
      "content": "ìƒì„¸ ë‚´ìš©",
      "importance_level": "í•µì‹¬/ì¤‘ìš”/ì°¸ê³ ",
      "related_questions": ["Q001", "Q002"],
      "prerequisites": ["ì„ ìˆ˜í•™ìŠµìš”ì†Œ1", "ì„ ìˆ˜í•™ìŠµìš”ì†Œ2"],
      "source_page": "í•´ë‹¹ í˜ì´ì§€ ë²ˆí˜¸"
    }}
  ],
  
  "learning_analytics": {{
    "total_questions": 0,
    "question_distribution": {{
      "ê°ê´€ì‹": 0, "ì£¼ê´€ì‹": 0, "ì„œìˆ í˜•": 0, "ê³„ì‚°í˜•": 0
    }},
    "difficulty_distribution": {{
      "ìƒ": 0, "ì¤‘": 0, "í•˜": 0
    }},
    "bloom_taxonomy_distribution": {{
      "ì§€ì‹": 0, "ì´í•´": 0, "ì ìš©": 0, "ë¶„ì„": 0, "ì¢…í•©": 0, "í‰ê°€": 0
    }},
    "chapter_coverage": "ì±•í„°ë³„ ë¬¸ì œ/ìë£Œ ë¶„í¬",
    "main_topics": ["ì „ì²´ ì£¼ìš” ì£¼ì œë“¤"],
    "estimated_study_time": "ì „ì²´ í•™ìŠµ ì˜ˆìƒ ì†Œìš”ì‹œê°„(ì‹œê°„)",
    "quality_score": "ì¶”ì¶œ í’ˆì§ˆ ì ìˆ˜ (1-10)",
    "extraction_completeness": "ì¶”ì¶œ ì™„ì„±ë„ (%)"
  }}
}}

ğŸ¯ **ì¤‘ìš” ì§€ì¹¨**:
- ë°˜ë“œì‹œ JSON í˜•íƒœë¡œë§Œ ë‹µë³€, ì¶”ê°€ ì„¤ëª… ê¸ˆì§€
- **ì‹œí—˜ë¬¸ì œë§Œ ì¶”ì¶œ**: í•´ì„¤, ì •ë‹µ, í’€ì´ í˜ì´ì§€ëŠ” ì™„ì „ ì œì™¸
- **êµ¬ì¡°ì  ë¶„ë¦¬ í•„ìˆ˜**: 
  * question_text = ìˆœìˆ˜ ì§ˆë¬¸ ë‚´ìš© (ë°˜ë“œì‹œ "?" ë˜ëŠ” ì§ˆë¬¸ íŒ¨í„´ í¬í•¨)
  * passage = ì§€ë¬¸/í‘œ/ê·¸ë˜í”„/ì½”ë“œ (ë³„ë„ êµ¬ë¶„)
  * options = ì™„ì „í•œ ì„ íƒì§€ ë°°ì—´ (3ê°œ ì´ìƒ, ì—°ì† ë²ˆí˜¸)
- **ì¤‘ë³µ ì œê±°**: ë¬¸ì œ ë²ˆí˜¸ ì¤‘ë³µ ì‹œ ì²« ë²ˆì§¸ë§Œ ìœ íš¨

ğŸ¯ **í‘œ ë°ì´í„° ì²˜ë¦¬ ê·œì¹™**:
- í‘œê°€ ë°œê²¬ë˜ë©´ `table_data` ê°ì²´ì— ì™„ì „í•œ êµ¬ì¡°í™”ëœ ë°ì´í„° ì €ì¥
- `headers`: í‘œ í—¤ë” ë°°ì—´
- `rows`: ê° í–‰ì˜ ë°ì´í„°ë¥¼ ë°°ì—´ë¡œ ì €ì¥ (ë¹ˆì…€ì€ `""` ì²˜ë¦¬)
- `title`/`description`: í‘œ ìœ„ì•„ë˜ ì„¤ëª… ì¶”ì¶œ
- `passage`ì—ëŠ” í‘œë¥¼ ë§ˆí¬ë‹¤ìš´ í˜•íƒœë¡œë„ í¬í•¨

ğŸ¯ **ì´ë¯¸ì§€ ì²˜ë¦¬ ê·œì¹™**:
- ê·¸ë¦¼ ì„ íƒì§€ë‚˜ ë³´ê¸° ì´ë¯¸ì§€ ë°œê²¬ì‹œ `related_images` ë°°ì—´ì— ì´ë¯¸ì§€ ID ê¸°ë¡
- ì„ íƒì§€ë³„ ì´ë¯¸ì§€ëŠ” `image_options`ì— ë§¤í•‘ (option_1: "IMG_001")
- `options` ë°°ì—´ì—ëŠ” "â‘  ![IMG_001]" í˜•íƒœë¡œ ì´ë¯¸ì§€ ì°¸ì¡° í¬í•¨
- `has_figure`: trueë¡œ ì„¤ì •
- **í’ˆì§ˆ ê²€ì¦**: ë¶ˆì™„ì „í•œ ë¬¸ì œ, ì„ íƒì§€ ëˆ„ë½ ë¬¸ì œ ì™„ì „ ì œì™¸
- **í‘œ/ì½”ë“œ ê°ì§€**: has_table, has_code, has_figure í”Œë˜ê·¸ ì •í™•íˆ ì„¤ì •
- **ëª©í‘œ ë¬¸ì œìˆ˜**: ì‹¤ì œ PDFì— ë”°ë¼ ë™ì  ê°ì§€ (20~100ê°œ ë²”ìœ„)

âš ï¸ **ì¶”ì¶œ ê¸ˆì§€ í•­ëª©**:
- ì„¤ëª…ë¬¸, ì •ì˜ë¬¸, ê°œë… ì„œìˆ  ("~ì…ë‹ˆë‹¤", "~ë‹¤", "~ì´ë‹¤" ë“±)
- í•´ì„¤ í˜ì´ì§€ ("ì •ë‹µ:", "í•´ì„¤:", "í’€ì´:" ë“±)
- ë¶ˆì™„ì „í•œ ì„ íƒì§€ (ë²ˆí˜¸ ëˆ„ë½, í˜ì´ì§€ ì˜ë¦¼)
- ì§ˆë¬¸ íŒ¨í„´ ì—†ëŠ” ë¬¸ì¥

âœ… **ì¶”ì¶œ í•„ìˆ˜ ì¡°ê±´**:
- ëª…í™•í•œ ì§ˆë¬¸ ì˜ë„ ("~ì€?", "ë‹¤ìŒ ì¤‘", "ì˜¬ë°”ë¥¸ ê²ƒì€?" ë“±)
- ì™„ì „í•œ ì„ íƒì§€ ì„¸íŠ¸ (â‘ â‘¡â‘¢â‘£ ë˜ëŠ” 1)2)3)4) ì—°ì†)
- ì˜ë¯¸ìˆëŠ” ë¬¸ì œ ë‚´ìš© (15ì ì´ìƒ)
                """.strip()
                
                print(f"Sending chunk {chunk_idx + 1} to Claude for advanced structuring...")
                
                # ğŸ”„ Claude API í˜¸ì¶œì— ì¬ì‹œë„ ë¡œì§ ëŒ€í­ ê°•í™” (529 ì˜¤ë¥˜ í•´ê²°)
                max_retries = 8  # ìµœëŒ€ 8íšŒ ì¬ì‹œë„
                base_delay = 5.0  # 5ì´ˆ ì´ˆê¸° ëŒ€ê¸°
                
                for attempt in range(max_retries):
                    try:
                        # ì²­í¬ë³„ Claude ë¶„ì„ ì‹¤í–‰ - ë” ë†’ì€ í† í°ê³¼ ì •ë°€ë„
                        response = await self.claude_client.messages.create(
                            model="claude-3-5-sonnet-20241022",
                            max_tokens=8000,
                            temperature=0.05,  # ë” ì¼ê´€ëœ ê²°ê³¼ë¥¼ ìœ„í•´ ë‚®ì€ ì˜¨ë„
                            messages=[
                                {
                                    "role": "user", 
                                    "content": structuring_prompt
                                }
                            ]
                        )
                        
                        result_text = response.content[0].text.strip()
                        print(f"âœ… Claude structuring successful on attempt {attempt + 1}, response length: {len(result_text)} characters")
                        break
                        
                    except Exception as api_error:
                        error_str = str(api_error)
                        print(f"âŒ Claude API attempt {attempt + 1} failed: {error_str}")
                        
                        # 529 ì˜¤ë¥˜ ë˜ëŠ” rate limit ê°ì§€
                        is_rate_limit = '529' in error_str or 'overload' in error_str.lower() or 'rate limit' in error_str.lower()
                        
                        if is_rate_limit and attempt < max_retries - 1:
                            # ëŒ€í­ ê°•í™”ëœ ì§€ìˆ˜ ë°±ì˜¤í”„ (5ë°° ì¦ê°€)
                            delay = base_delay * (5 ** attempt) + random.uniform(5, 15)  # 5-15ì´ˆ ëœë¤ ëŒ€ê¸°
                            print(f"ğŸ”„ Rate limit detected (attempt {attempt + 1}/{max_retries}), waiting {delay:.1f} seconds...")
                            await asyncio.sleep(delay)
                        elif attempt == max_retries - 1:
                            print(f"ğŸ’€ All {max_retries} attempts failed for chunk {chunk_idx + 1}")
                            # ì‹¤íŒ¨í•œ ì²­í¬ëŠ” ë¹ˆ ê²°ê³¼ë¡œ ì²˜ë¦¬í•˜ì—¬ ì „ì²´ í”„ë¡œì„¸ìŠ¤ ì¤‘ë‹¨ ë°©ì§€
                            result_text = json.dumps({
                                'questions': [],
                                'study_materials': [],
                                'chapters': [],
                                'document_analysis': {
                                    'title': 'Processing Failed',
                                    'total_pages_analyzed': '0',
                                    'extraction_method': 'Failed - API Error'
                                }
                            })
                            break
                        else:
                            # ë‹¤ë¥¸ ì¢…ë¥˜ì˜ ì˜¤ë¥˜ëŠ” ì¦‰ì‹œ ì¬ì‹œë„
                            continue
                
                # ğŸ› ï¸ JSON íŒŒì‹± ì „ ë°ì´í„° ì •ì œ (íŠ¹ìˆ˜ë¬¸ì ì´ìŠˆ í•´ê²°)
                def sanitize_json_content(text: str) -> str:
                    """JSON íŒŒì‹± ì˜¤ë¥˜ë¥¼ ì¼ìœ¼í‚¤ëŠ” íŠ¹ìˆ˜ë¬¸ìë“¤ì„ ì •ì œ"""
                    # ì œì–´ ë¬¸ì ì œê±° (íƒ­, ê°œí–‰ ë“±ì€ ìœ ì§€)
                    import unicodedata
                    sanitized = ''.join(char for char in text if unicodedata.category(char)[0] != 'C' or char in '\t\n\r')
                    
                    # ë¬¸ì œê°€ ë˜ëŠ” ë”°ì˜´í‘œ íŒ¨í„´ ì •ë¦¬
                    sanitized = sanitized.replace('\\n', ' ').replace('\\t', ' ').replace('\\r', ' ')
                    
                    # í‘œ ë°ì´í„°ì—ì„œ ë¬¸ì œê°€ ë˜ëŠ” íŠ¹ìˆ˜ ë¬¸ìë“¤ ì •ë¦¬
                    sanitized = re.sub(r'[\x00-\x08\x0b\x0c\x0e-\x1f\x7f-\x9f]', ' ', sanitized)
                    
                    return sanitized
                
                # JSON íŒŒì‹±
                try:
                    sanitized_text = sanitize_json_content(result_text)
                    chunk_result = json.loads(sanitized_text)
                    print(f"âœ… JSON parsing successful for chunk {chunk_idx + 1}")
                except json.JSONDecodeError as json_error:
                    print(f"âŒ JSON parsing failed for chunk {chunk_idx + 1}: {json_error}")
                    print(f"Error location: line {json_error.lineno}, column {json_error.colno}")
                    print(f"Raw response preview: {result_text[:500]}...")
                    
                    # ğŸ”§ ê°•í™”ëœ JSON ë³µêµ¬ ì‹œë„
                    try:
                        # Claude ì‘ë‹µì—ì„œ ì‹¤ì œ JSONë§Œ ì¶”ì¶œí•˜ëŠ” ë‹¤ë‹¨ê³„ ì ‘ê·¼ë²•
                        recovered_json = None
                        
                        # 1ì°¨: ë°±í‹±ìœ¼ë¡œ ë‘˜ëŸ¬ì‹¸ì¸ JSON ì¶”ì¶œ
                        json_match = re.search(r'```(?:json)?\s*([\s\S]*?)\s*```', result_text)
                        if json_match:
                            json_content = json_match.group(1).strip()
                            try:
                                recovered_json = json.loads(sanitize_json_content(json_content))
                                print(f"âœ… JSON ë³µêµ¬ ì„±ê³µ (ë°±í‹± ì¶”ì¶œ)")
                            except:
                                pass
                        
                        # 2ì°¨: ì²« ë²ˆì§¸ ì¤‘ê´„í˜¸ë¶€í„° ë§ˆì§€ë§‰ ì¤‘ê´„í˜¸ê¹Œì§€ ì¶”ì¶œ
                        if not recovered_json:
                            first_brace = result_text.find('{')
                            last_brace = result_text.rfind('}')
                            if first_brace != -1 and last_brace != -1 and last_brace > first_brace:
                                json_content = result_text[first_brace:last_brace+1]
                                try:
                                    # ì¼ë°˜ì ì¸ JSON ì˜¤ë¥˜ ìˆ˜ì •
                                    json_content = self._fix_common_json_errors(json_content)
                                    recovered_json = json.loads(sanitize_json_content(json_content))
                                    print(f"âœ… JSON ë³µêµ¬ ì„±ê³µ (ì¤‘ê´„í˜¸ ì¶”ì¶œ + ì˜¤ë¥˜ ìˆ˜ì •)")
                                except:
                                    pass
                        
                        # 3ì°¨: Claude í…ìŠ¤íŠ¸ ì‘ë‹µì„ êµ¬ì¡°í™”ëœ ë°ì´í„°ë¡œ ë³€í™˜
                        if not recovered_json:
                            recovered_json = await self._parse_claude_text_response(result_text, chunk_idx + 1)
                            if recovered_json:
                                print(f"âœ… JSON ë³µêµ¬ ì„±ê³µ (í…ìŠ¤íŠ¸ íŒŒì‹±)")
                        
                        chunk_result = recovered_json or {
                            'questions': [],
                            'study_materials': [],
                            'chapters': []
                        }
                        
                    except Exception as recovery_error:
                        print(f"ğŸ’€ JSON ë³µêµ¬ ì‹¤íŒ¨: {recovery_error}")
                        chunk_result = {
                            'questions': [],
                            'study_materials': [],
                            'chapters': []
                        }
                
                # ì²­í¬ë³„ ê²°ê³¼ë¥¼ ì „ì²´ ê²°ê³¼ì— ë³‘í•©
                chunk_questions = chunk_result.get('questions', [])
                chunk_materials = chunk_result.get('study_materials', [])
                chunk_chapters = chunk_result.get('chapters', [])
                
                # ğŸ”¥ ì´ˆê°•í™”ëœ í’ˆì§ˆ ê²€ì¦ ë° í•„í„°ë§
                validated_questions = []
                question_numbers_seen = set()
                
                for question in chunk_questions:
                    question_text = question.get('question_text', '').strip()
                    passage = question.get('passage', '').strip()
                    options = question.get('options', [])
                    question_number = question.get('question_number', 'N/A')
                    
                    # ğŸš¨ ë¬¸ì œ ë²ˆí˜¸ ì¤‘ë³µ ë°©ì§€ ë° ì •ê·œí™”
                    if isinstance(question_number, str):
                        # ë¬¸ì œ ë²ˆí˜¸ì—ì„œ ìˆ«ìë§Œ ì¶”ì¶œ (ì˜ˆ: "Q2_11" -> 11)
                        import re
                        number_match = re.search(r'(\d+)$', str(question_number))
                        if number_match:
                            actual_number = int(number_match.group(1))
                            question_number = actual_number
                        else:
                            question_number = 'N/A'
                    
                    # === 1. ë‹µì•ˆ/í•´ì„¤ í˜ì´ì§€ ê°•í™” í•„í„°ë§ ===
                    full_text = (question_text + ' ' + passage).lower()
                    answer_keywords = [
                        'í•´ì„¤', 'ì •ë‹µ', 'í’€ì´', 'ë‹µì•ˆ', 'í•´ë‹µ', 'ì •ë‹µ ë° í•´ì„¤',
                        'ì„¤ëª…', 'ë¶€ì—° ì„¤ëª…', 'ìì„¸í•œ ì„¤ëª…', 'í•´ë‹µ', 'í•´ì„',
                        'explanation', 'answer', 'solution', 'ë‹µ :', 'ì •ë‹µ:', 'í•´ì„¤:',
                        'â‘  í•´ì„¤', 'â‘¡ í•´ì„¤', 'â‘¢ í•´ì„¤', 'â‘£ í•´ì„¤',
                        'ì— ëŒ€í•œ ì„¤ëª…', 'ì— ëŒ€í•œ í•´ì„¤', 'ë‹µì•ˆ ì„¤ëª…'
                    ]
                    
                    if any(keyword in full_text for keyword in answer_keywords):
                        print(f"ğŸš« Skipping answer/explanation page: Q{question_number}")
                        continue
                    
                    # === 2. ë¬¸ì œ ë‚´ìš© ê¸°ë³¸ ê²€ì¦ ===
                    if not question_text or len(question_text.strip()) < 15:
                        print(f"ğŸš« Skipping invalid question (too short): Q{question_number}")
                        continue
                    
                    # === 3. ë¬¸ì œ ë²ˆí˜¸ ìœ íš¨ì„± ê²€ì¦ ===
                    if question_number == 'N/A' or question_number is None or question_number == 0:
                        print(f"ğŸš« Skipping question with invalid number: {question_number}")
                        continue
                    
                    if question_number in question_numbers_seen:
                        print(f"ğŸš« Skipping duplicate question number: Q{question_number}")
                        continue
                    
                    # === 4. ì„ íƒì§€ ê°•í™” ê²€ì¦ (ê·¸ë¦¼ ì„ íƒì§€ í¬í•¨) ===
                    valid_options = []
                    if isinstance(options, list):
                        for opt in options:
                            if opt and isinstance(opt, str):
                                clean_opt = opt.strip()
                                # ì„ íƒì§€ ê¸¸ì´ì™€ ë‚´ìš© ê²€ì¦
                                if len(clean_opt) >= 1 and not clean_opt.lower() in ['', 'null', 'none', 'n/a']:
                                    # ë²ˆí˜¸ íŒ¨í„´ í™•ì¸ (â‘ , â‘¡, â‘¢, â‘£ ë˜ëŠ” 1), 2), 3), 4))
                                    if any(pattern in clean_opt for pattern in ['â‘ ', 'â‘¡', 'â‘¢', 'â‘£', 'â‘¤', '1)', '2)', '3)', '4)', '5)']):
                                        valid_options.append(clean_opt)
                                    # ê·¸ë¦¼ ì„ íƒì§€ íŒ¨í„´ í™•ì¸
                                    elif '[ê·¸ë¦¼:' in clean_opt or '[ìˆ˜ì‹:' in clean_opt or '[ë„í‘œ:' in clean_opt:
                                        valid_options.append(clean_opt)
                                    elif len(clean_opt) > 5:  # ë²ˆí˜¸ê°€ ì—†ì–´ë„ ì¶©ë¶„íˆ ê¸´ ì„ íƒì§€ëŠ” í—ˆìš©
                                        valid_options.append(clean_opt)
                                    elif len(clean_opt) >= 1 and any(char.isdigit() for char in clean_opt[:3]):  # ìµœì†Œ ë²ˆí˜¸ë¼ë„ ìˆìœ¼ë©´
                                        valid_options.append(clean_opt)
                    
                    if len(valid_options) < 2:
                        print(f"ğŸš« Skipping Q{question_number}: insufficient options ({len(valid_options)})")
                        continue
                    
                    # === 5. ë¬¸ì œ ìœ í˜• ê²€ì¦ ===
                    # ë¬¸ì œì²˜ëŸ¼ ë³´ì´ì§€ ì•ŠëŠ” ë‚´ìš© í•„í„°ë§
                    invalid_patterns = [
                        'í˜ì´ì§€', 'page', 'ëª©ì°¨', 'í‘œì§€', 'cover',
                        'ë‹¤ìŒì€', 'ë‹¤ìŒ ì¤‘', 'ë¬´ì—‡ì¸ê°€', 'ì–´ë–¤ ê²ƒì€',
                        'ì„¤ëª…ìœ¼ë¡œ ì˜³', 'ì„¤ëª…ìœ¼ë¡œ í‹€ë¦°', 'í•´ë‹¹í•˜ì§€ ì•ŠëŠ”',
                        'ì— ëŒ€í•œ ì„¤ëª…', 'ëŒ€í•œ ì„¤ëª…'
                    ]
                    
                    # ë¬¸ì œë‹¤ìš´ íŒ¨í„´ì´ ìˆëŠ”ì§€ í™•ì¸
                    question_patterns = [
                        'ë‹¤ìŒ ì¤‘', 'ë¬´ì—‡ì¸ê°€', 'ì–´ë–¤ ê²ƒ', 'ì˜³ì€ ê²ƒ', 'í‹€ë¦° ê²ƒ',
                        'í•´ë‹¹í•˜ëŠ” ê²ƒ', 'ì•„ë‹Œ ê²ƒ', 'ì„¤ëª…ìœ¼ë¡œ', 'ì— ëŒ€í•œ'
                    ]
                    
                    has_question_pattern = any(pattern in question_text for pattern in question_patterns)
                    if not has_question_pattern and len(valid_options) < 3:
                        print(f"ğŸš« Skipping Q{question_number}: doesn't look like a proper question")
                        continue
                    
                    # === 6. ë‚´ìš© í’ˆì§ˆ ê²€ì¦ ===
                    # ë¹ˆ ë‚´ìš©ì´ë‚˜ ì˜ë¯¸ì—†ëŠ” ë‚´ìš© í•„í„°ë§
                    meaningless_patterns = [
                        'ì œì‹œëœ ë‚´ìš©', 'ë‹¤ìŒê³¼ ê°™', 'ìœ„ì˜ ë‚´ìš©', 'ì•„ë˜ ë‚´ìš©',
                        'ì„¤ëª…ì´ ìˆ', 'ë‚´ìš©ì´ ìˆ', 'ë‹¤ìŒ í‘œ', 'ë‹¤ìŒ ê·¸ë¦¼'
                    ]
                    
                    if len(question_text) < 30 and any(pattern in question_text for pattern in meaningless_patterns):
                        print(f"ğŸš« Skipping Q{question_number}: meaningless content")
                        continue
                    
                    # === 7. ìµœì¢… ê²€ì¦ í†µê³¼ - ë¬¸ì œ ì •ì œ ë° ì¶”ê°€ ===
                    question['options'] = valid_options
                    question['question_text'] = question_text.strip()
                    question['passage'] = passage.strip()
                    
                    # ë©”íƒ€ë°ì´í„° ì¶”ê°€
                    has_table = 'í‘œ' in passage or '|' in passage
                    question['has_table'] = has_table
                    question['has_code'] = any(code_word in passage.lower() for code_word in ['class', 'function', 'public', 'int', 'string', 'void'])
                    question['has_figure'] = 'ê·¸ë¦¼' in passage or 'ë„í‘œ' in passage
                    question['validation_passed'] = True
                    
                    # ğŸ” í‘œ ë°ì´í„° ì™„ì „ì„± ê²€ì¦ (P1, P2, P3 í–‰ í™•ì¸)
                    if has_table:
                        table_validation = self._validate_table_completeness(passage, question_number)
                        question['table_complete'] = table_validation['is_complete']
                        question['table_issues'] = table_validation['issues']
                        
                        if not table_validation['is_complete']:
                            print(f"âš ï¸ Q{question_number} table incomplete: {table_validation['issues']}")
                        else:
                            print(f"âœ… Q{question_number} table complete with data rows")
                    
                    question_numbers_seen.add(question_number)
                    validated_questions.append(question)
                    print(f"âœ… Valid Q{question_number}: {len(valid_options)} options, passage={len(passage)} chars")
                
                all_questions.extend(validated_questions)
                all_materials.extend(chunk_materials)
                all_chapters.extend(chunk_chapters)
                
                # ğŸ”¥ íŠ¹ìˆ˜ ì½˜í…ì¸  í›„ì²˜ë¦¬ (í‘œ/ê·¸ë¦¼/ì½”ë“œ)
                validated_questions = await self._post_process_special_content(validated_questions, chunk_idx)
                
                print(f"Chunk {chunk_idx + 1} processed: {len(validated_questions)}/{len(chunk_questions)} questions validated")
                
                # API ë ˆì´íŠ¸ ë¦¬ë¯¸íŠ¸ ë°©ì§€ - ëŒ€í­ ê°•í™”
                if chunk_idx < len(chunks) - 1:
                    await asyncio.sleep(10)  # ì²­í¬ ê°„ 10ì´ˆ ëŒ€ê¸°ë¡œ ì¦ê°€
            
            # ğŸ”¥ í˜ì´ì§€ê°„ ë¬¸ì œ ì—°ê²° í›„ì²˜ë¦¬ ë° ë¬¸ì œ ë²ˆí˜¸ ì •ê·œí™”
            all_questions = await self._handle_cross_page_questions(all_questions, full_connected_content)
            
            # ğŸ” ìµœì¢… í’ˆì§ˆ ê´€ë¦¬ ë° ì¤‘ë³µ ì œê±°
            print(f"\nğŸ” ìµœì¢… í’ˆì§ˆ ê²€ì¦ ì¤‘...")
            
            # ë¬¸ì œ ë²ˆí˜¸ë³„ë¡œ ê·¸ë£¹í™”í•˜ì—¬ ì¤‘ë³µ ì œê±°
            final_questions = {}
            for question in all_questions:
                q_num = question.get('question_number')
                if q_num and q_num not in final_questions:
                    # ìµœì¢… ê²€ì¦
                    if (question.get('question_text', '').strip() and 
                        len(question.get('question_text', '').strip()) >= 15 and
                        question.get('options') and 
                        len(question.get('options', [])) >= 2):
                        final_questions[q_num] = question
                        print(f"âœ… Final Q{q_num}: PASSED")
                    else:
                        print(f"ğŸš« Final Q{q_num}: FAILED quality check")
                else:
                    print(f"ğŸš« Final Q{q_num}: DUPLICATE or INVALID number")
            
            # ìµœì¢… ì •ì œëœ ë¬¸ì œ ë¦¬ìŠ¤íŠ¸
            final_questions_list = list(final_questions.values())
            print(f"\nğŸ“Š ìµœì¢… ê²°ê³¼: {len(final_questions_list)} ë¬¸ì œ ì¶”ì¶œ ì™„ë£Œ")
            
            # ğŸ¯ ê³¼ë„í•œ ë¬¸ì œ ìˆ˜ í’ˆì§ˆ í•„í„°ë§ (ì‹¤ì œ ë¬¸ì œ ìˆ˜ ê¸°ì¤€)
            # ì‹¤ì œ PDFì˜ ë¬¸ì œ ìˆ˜ë¥¼ ì¶”ì • (ê°€ì¥ ë†’ì€ ë¬¸ì œ ë²ˆí˜¸ ê¸°ì¤€)
            question_numbers = [q.get('question_number', 0) for q in final_questions_list]
            valid_numbers = [n for n in question_numbers if isinstance(n, (int, str)) and str(n).isdigit()]
            
            if valid_numbers:
                max_question_number = max([int(n) for n in valid_numbers])
                estimated_total_questions = max_question_number
                
                print(f"ğŸ“Š ì¶”ì¶œëœ ë¬¸ì œ ë¶„ì„:")
                print(f"  - ì¶”ì¶œëœ ë¬¸ì œ ìˆ˜: {len(final_questions_list)}ê°œ")
                print(f"  - ìµœê³  ë¬¸ì œ ë²ˆí˜¸: {max_question_number}ë²ˆ")
                print(f"  - ì¶”ì • ì‹¤ì œ ë¬¸ì œ ìˆ˜: {estimated_total_questions}ê°œ")
                
                # ì¶”ì¶œëœ ë¬¸ì œê°€ ì¶”ì • ì‹¤ì œ ë¬¸ì œ ìˆ˜ì˜ 1.5ë°°ë¥¼ ë„˜ìœ¼ë©´ í’ˆì§ˆ í•„í„°ë§ ì ìš©
                if len(final_questions_list) > estimated_total_questions * 1.5:
                    print(f"âš ï¸ ê³¼ë„í•œ ë¬¸ì œ ì¶”ì¶œ ê°ì§€: {len(final_questions_list)} > {estimated_total_questions * 1.5:.0f}")
                    print("í’ˆì§ˆ ê¸°ë°˜ í•„í„°ë§ì„ ì ìš©í•˜ì—¬ ì¤‘ë³µ ë° ì €í’ˆì§ˆ ë¬¸ì œë¥¼ ì œê±°í•©ë‹ˆë‹¤...")
                    
                    # í’ˆì§ˆ ì ìˆ˜ ê³„ì‚°
                    def calculate_quality_score(q):
                        score = 0
                        # ì„ íƒì§€ ê°œìˆ˜ (4ê°œ ì´ìƒì´ ì´ìƒì )
                        score += min(len(q.get('options', [])) * 10, 40)
                        # ì§€ë¬¸ ê¸¸ì´ (ë³µì¡í•œ ë¬¸ì œì¼ìˆ˜ë¡ ì¢‹ìŒ)
                        score += min(len(q.get('passage', '')) / 10, 30)
                        # ì§ˆë¬¸ ê¸¸ì´ (ë„ˆë¬´ ì§§ì§€ ì•Šê²Œ)
                        score += min(len(q.get('question_text', '')) / 5, 20)
                        # í‘œ/ê·¸ë¦¼/ì½”ë“œ í¬í•¨ì‹œ ë³´ë„ˆìŠ¤
                        if q.get('has_table', False): score += 15
                        if q.get('has_figure', False): score += 10
                        if q.get('has_code', False): score += 10
                        # ë¬¸ì œ ë²ˆí˜¸ê°€ ìœ íš¨í•˜ë©´ ë³´ë„ˆìŠ¤
                        try:
                            int(q.get('question_number', 0))
                            score += 5
                        except:
                            pass
                        return score
                    
                    # í’ˆì§ˆ ì ìˆ˜ë¡œ ì •ë ¬
                    scored_questions = [(q, calculate_quality_score(q)) for q in final_questions_list]
                    scored_questions.sort(key=lambda x: x[1], reverse=True)
                    
                    # ì¶”ì • ë¬¸ì œ ìˆ˜ì— ë§ì¶° ìƒìœ„ ë¬¸ì œë“¤ ì„ íƒ (ì—¬ìœ ë¶„ 10% ì¶”ê°€)
                    target_count = int(estimated_total_questions * 1.1)
                    final_questions_list = [q for q, score in scored_questions[:target_count]]
                    print(f"âœ… í’ˆì§ˆ í•„í„°ë§ ì™„ë£Œ: {len(scored_questions)} â†’ {len(final_questions_list)}ê°œ ë¬¸ì œ")
                    
                    # ì„ íƒëœ ë¬¸ì œë“¤ì˜ ë²ˆí˜¸ ë²”ìœ„ ì¶œë ¥
                    remaining_numbers = [q.get('question_number', 0) for q in final_questions_list]
                    remaining_numbers = [n for n in remaining_numbers if isinstance(n, (int, str)) and str(n).isdigit()]
                    if remaining_numbers:
                        remaining_numbers = sorted([int(n) for n in remaining_numbers])
                        print(f"ğŸ“‹ ìµœì¢… ë¬¸ì œ ë²”ìœ„: {min(remaining_numbers)}~{max(remaining_numbers)}ë²ˆ")
                else:
                    print(f"âœ… ì ì • ìˆ˜ì¤€ì˜ ë¬¸ì œ ì¶”ì¶œ: {len(final_questions_list)}ê°œ (ì¶”ì • {estimated_total_questions}ê°œ ê¸°ì¤€)")
            else:
                print(f"âœ… ì¶”ì¶œ ì™„ë£Œ: {len(final_questions_list)}ê°œ ë¬¸ì œ (ë²ˆí˜¸ ë¶„ì„ ë¶ˆê°€)")
            
            # ì „ì²´ ê²°ê³¼ êµ¬ì„±
            structured_result = {
                'document_analysis': {
                    'title': 'GPT Vision Processed Document',
                    'subject': 'General Education',
                    'exam_type': 'Educational Content',
                    'academic_level': 'General',
                    'total_pages_analyzed': markdown_result.get('pages_processed', 0),
                    'extraction_method': 'GPT Vision + Claude Ultra Enhanced'
                },
                'chapters': all_chapters,
                'questions': final_questions_list,  # ìµœì¢… ê²€ì¦ëœ ë¬¸ì œë§Œ
                'study_materials': all_materials,
                'learning_analytics': {
                    'total_questions': len(final_questions_list),
                    'chunks_processed': len(chunks),
                    'validation_applied': True,
                    'quality_control': 'Ultra Enhanced'
                }
            }
            
            # êµ¬ì¡°í™”ëœ ê²°ê³¼ ì²˜ë¦¬
            document_analysis = structured_result.get('document_analysis', {})
            chapters = all_chapters
            questions = final_questions_list  # ìµœì¢… ê²€ì¦ëœ ë¬¸ì œ ì‚¬ìš©
            materials = all_materials
            analytics = structured_result.get('learning_analytics', {})
            
            # ìµœì¢… í’ˆì§ˆ ë³´ê³ ì„œ
            questions_with_full_options = [q for q in questions if len(q.get('options', [])) >= 3]
            questions_with_passage = [q for q in questions if q.get('passage', '').strip()]
            
            print(f"\nğŸ“Š ì´ˆê°•í™” ì¶”ì¶œ ì‹œìŠ¤í…œ ìµœì¢… ë³´ê³ ì„œ:")
            print(f"  - ğŸ“š ì±•í„°: {len(chapters)}ê°œ ì‹ë³„")
            print(f"  - â“ ìµœì¢… ë¬¸ì œ: {len(questions)}ê°œ ì¶”ì¶œ (í’ˆì§ˆ ê²€ì¦ ì™„ë£Œ)")  
            print(f"  - ğŸ¯ ì™„ì „í•œ ì„ íƒì§€: {len(questions_with_full_options)}ê°œ (3ê°œ ì´ìƒ)")
            print(f"  - ğŸ“ ì§€ë¬¸ í¬í•¨: {len(questions_with_passage)}ê°œ ë¬¸ì œ")
            print(f"  - ğŸ“– í•™ìŠµ ìë£Œ: {len(materials)}ê°œ ìƒì„±")
            print(f"  - ğŸ­ ì²­í¬ ì²˜ë¦¬: {len(chunks)}ê°œ ì²­í¬")
            
            if len(questions) > 0:
                quality_score = (len(questions_with_full_options) / len(questions)) * 100
                print(f"  - ğŸ“ˆ í’ˆì§ˆ ì ìˆ˜: {quality_score:.1f}% (Ultra Enhanced)")
                print(f"  - ğŸš€ ì²˜ë¦¬ ë°©ì‹: í•´ì„¤ í˜ì´ì§€ ì™„ì „ í•„í„°ë§, ì—„ê²©í•œ ì„ íƒì§€ ê²€ì¦")
            else:
                print(f"  - âš ï¸  ê²½ê³ : ì¶”ì¶œëœ ë¬¸ì œê°€ ì—†ìŠµë‹ˆë‹¤! ë” ì—„ê²©í•œ í•„í„°ë§ìœ¼ë¡œ ëª¨ë“  ë¬¸ì œê°€ ì œì™¸ë˜ì—ˆì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
            
            # ë°ì´í„°ë² ì´ìŠ¤ ì €ì¥ - ì±•í„° ì •ë³´ ë¨¼ì € ì €ì¥
            chapter_ids = {}
            if chapters:
                for chapter in chapters:
                    try:
                        result = db.execute(text("""
                            INSERT INTO chapters
                            (pdf_upload_id, chapter_number, title, main_topics, learning_objectives, page_range)
                            VALUES (:upload_id, :chapter_number, :title, :main_topics, :learning_objectives, :page_range)
                        """), {
                            'upload_id': upload_id,
                            'chapter_number': chapter.get('chapter_number', 0),
                            'title': chapter.get('chapter_title', ''),
                            'main_topics': json.dumps(chapter.get('main_topics', [])),
                            'learning_objectives': json.dumps(chapter.get('learning_objectives', [])),
                            'page_range': chapter.get('page_range', '')
                        })
                        chapter_ids[chapter.get('chapter_number', 0)] = result.lastrowid
                        print(f"Saved chapter: {chapter.get('chapter_title', 'Untitled')}")
                    except Exception as db_error:
                        print(f"Failed to save chapter to DB: {db_error}")
            
            # ê³ í’ˆì§ˆ ë¬¸ì œ ì €ì¥ (Enhanced GPT Vision + Claude ê¸°ë°˜)
            saved_questions = 0
            if questions:
                for question in questions:
                    try:
                        # ì¶”ê°€ í’ˆì§ˆ ê²€ì¦
                        question_text = question.get('question_text', '').strip()
                        if not question_text or len(question_text) < 10:
                            print(f"âš ï¸  Skipping short question: {question.get('question_id', 'unknown')}")
                            continue
                        
                        # ë¬´íš¨í•œ ë‚´ìš© í•„í„°ë§
                        if any(invalid in question_text.lower() for invalid in ['-|', 'í˜ì´ì§€ ë³€í™˜ ì‹¤íŒ¨', 'error', 'ë¹ˆ í˜ì´ì§€']):
                            print(f"âš ï¸  Skipping invalid question content: {question.get('question_id', 'unknown')}")
                            continue
                            
                        chapter_id = chapter_ids.get(question.get('chapter_id', 0))
                        # ì¶”ê°€ ë©”íƒ€ë°ì´í„° êµ¬ì„±
                        additional_info = {
                            'has_table': question.get('has_table', False),
                            'has_code': question.get('has_code', False),
                            'has_figure': question.get('has_figure', False),
                            'source_page': question.get('source_page', ''),
                            'extraction_method': 'Enhanced GPT Vision + Claude'
                        }
                        
                        db.execute(text("""
                            INSERT INTO extracted_questions
                            (pdf_upload_id, chapter_id, question_id, question_text, question_type,
                             difficulty_level, correct_answer, options, explanation, bloom_taxonomy,
                             topic_tags, estimated_time, keywords, passage, additional_info)
                            VALUES (:upload_id, :chapter_id, :question_id, :question_text, :question_type,
                                    :difficulty, :correct_answer, :options, :explanation, :bloom_taxonomy,
                                    :topic_tags, :estimated_time, :keywords, :passage, :additional_info)
                        """), {
                            'upload_id': upload_id,
                            'chapter_id': chapter_id,
                            'question_id': question.get('question_id', f'Q{saved_questions+1:03d}'),
                            'question_text': question_text,
                            'question_type': question.get('question_type', 'Multiple Choice'),
                            'difficulty': question.get('difficulty_level', 'Medium'),
                            'correct_answer': question.get('correct_answer', ''),
                            'options': json.dumps(question.get('options', [])),
                            'explanation': question.get('explanation', ''),
                            'bloom_taxonomy': question.get('bloom_taxonomy', 'Application'),
                            'topic_tags': json.dumps(question.get('topic_tags', [])),
                            'estimated_time': question.get('estimated_time', '2 minutes'),
                            'keywords': json.dumps(question.get('keywords', [])),
                            'passage': question.get('passage', ''),
                            'additional_info': json.dumps(additional_info)
                        })
                        saved_questions += 1
                        print(f"âœ… Saved enhanced question: {question.get('question_id', f'Q{saved_questions:03d}')} - {len(question.get('options', []))} options")
                    except Exception as db_error:
                        print(f"âŒ Failed to save question to DB: {db_error}")
                        
            print(f"ğŸ’¾ Database ì €ì¥ ì™„ë£Œ: {saved_questions}/{len(questions)}ê°œ ë¬¸ì œ ì €ì¥")
            
            # ê³ í’ˆì§ˆ í•™ìŠµ ìë£Œ ì €ì¥
            if materials:
                for material in materials:
                    try:
                        # Dummy certificate_id ê°€ì ¸ì˜¤ê¸°
                        dummy_cert = db.execute(text("SELECT id FROM certificates_info LIMIT 1")).fetchone()
                        cert_id = dummy_cert[0] if dummy_cert else 1
                        
                        chapter_id = chapter_ids.get(material.get('chapter_id', 0))
                        db.execute(text("""
                            INSERT INTO study_materials
                            (certificate_id, pdf_upload_id, chapter_id, material_id, material_type, title, content,
                             importance_level, related_questions, prerequisites)
                            VALUES (:cert_id, :upload_id, :chapter_id, :material_id, :material_type, :title, :content,
                                    :importance_level, :related_questions, :prerequisites)
                        """), {
                            'cert_id': cert_id,
                            'upload_id': upload_id,
                            'chapter_id': chapter_id,
                            'material_id': material.get('material_id', ''),
                            'material_type': material.get('material_type', ''),
                            'title': material.get('title', ''),
                            'content': material.get('content', ''),
                            'importance_level': material.get('importance_level', ''),
                            'related_questions': json.dumps(material.get('related_questions', [])),
                            'prerequisites': json.dumps(material.get('prerequisites', []))
                        })
                        print(f"Saved GPT Vision material: {material.get('title', 'Untitled')}")
                    except Exception as db_error:
                        print(f"Failed to save material to DB: {db_error}")
            
            db.commit()
            
            await self._log_processing_step(upload_id, "markdown_structuring", "completed", {
                'chapters_identified': len(chapters),
                'questions_extracted': len(questions),
                'materials_created': len(materials),
                'claude_tokens_used': response.usage.input_tokens + response.usage.output_tokens if hasattr(response, 'usage') else 0,
                'document_analysis': document_analysis,
                'learning_analytics': analytics,
                'gpt_vision_tokens': markdown_result.get('tokens_used', 0)
            }, db)
            
            return {
                'success': True,
                'questions': questions,
                'materials': materials,
                'chapters': chapters,
                'document_analysis': document_analysis,
                'learning_analytics': analytics,
                'gpt_vision_tokens': markdown_result.get('tokens_used', 0),
                'claude_tokens': response.usage.input_tokens + response.usage.output_tokens if hasattr(response, 'usage') else 0
            }
            
        except Exception as e:
            logger.error(f"Markdown structuring error: {str(e)}")
            await self._log_processing_step(upload_id, "markdown_structuring", "failed", {"error": str(e)}, db)
            return {
                'success': False,
                'questions': [],
                'materials': [],
                'chapters': [],
                'error': str(e)
            }
    
    async def _structure_markdown_content_enhanced(self, upload_id: int, markdown_result: Dict, db: Session) -> Dict[str, Any]:
        """3ë‹¨ê³„: Claudeë¡œ ì „ì²´ Markdownì„ ë¬¸ì œë‹¨ìœ„ë¡œ êµ¬ì¡°í™” (ê°œì„ ëœ ë²„ì „)"""
        try:
            await self._log_processing_step(upload_id, "enhanced_markdown_structuring", "processing", {}, db)
            
            if not self.claude_client:
                print("ERROR: Claude API key not set - cannot perform markdown structuring")
                return {
                    'success': False,
                    'questions': [],
                    'materials': [],
                    'chapters': [],
                    'error': 'Claude API key not set'
                }
            
            markdown_content = markdown_result.get('markdown_content', '')
            if not markdown_content:
                print("No markdown content to structure")
                return {
                    'success': False,
                    'questions': [],
                    'materials': [],
                    'chapters': [],
                    'error': 'No markdown content provided'
                }
            
            print(f"Enhanced structuring of {len(markdown_content)} characters with Claude...")
            
            # ğŸ” 0ë‹¨ê³„: ë¬¸ì„œ êµ¬ì¡° ë¶„ì„ ë° ë¶„ë¥˜ ì „ì²˜ë¦¬ 
            document_structure = await self._analyze_document_structure(
                upload_id=upload_id,
                markdown_content=markdown_content, 
                db=db
            )
            print(f"ğŸ“‹ ë¬¸ì„œ êµ¬ì¡° ë¶„ì„ ì™„ë£Œ: {document_structure.get('total_questions', 0)}ë¬¸ì œ, ë¶„ë¥˜: {document_structure.get('document_type', 'unknown')}")
            
            # ğŸ–¼ï¸ 1ë‹¨ê³„: êµ¬ì¡° ë¶„ì„ ê¸°ë°˜ ë‹¤ì´ì–´ê·¸ë¨/ì´ë¯¸ì§€ ìº¡ì²˜
            captured_diagrams = await self._enhanced_diagram_capture(
                upload_id=upload_id,
                structure_analysis=document_structure,
                pdf_path=self.get_pdf_path(upload_id),  # PDF ê²½ë¡œ ê°€ì ¸ì˜¤ê¸° í•„ìš”
                db=db
            )
            
            # í˜ì´ì§€ë³„ ì„¸ë°€í•œ ì²­í‚¹ ì²˜ë¦¬ - ê° í˜ì´ì§€ë¥¼ ê°œë³„ ì²˜ë¦¬
            pages = markdown_content.split('---')  # í˜ì´ì§€ êµ¬ë¶„ìë¡œ ë¶„í• 
            print(f"Found {len(pages)} pages in markdown content")
            
            # ğŸ¯ ì˜¤ë²„ë© ìœˆë„ìš° ì²­í¬ ìƒì„± (í˜ì´ì§€ ê²½ê³„ ë¬¸ì œ í•´ê²°)
            chunks = self._create_overlap_chunks(pages)
            print(f"ğŸ“¦ Created {len(chunks)} overlap chunks (including boundaries for Q11/Q41 recovery)")
            
            # ì „ì²´ ê²°ê³¼ ìˆ˜ì§‘
            all_questions = []
            all_materials = []
            all_chapters = []
            total_tokens = 0
            extracted_question_numbers = set()  # ğŸ†• ì¤‘ë³µ ë¬¸ì œ ë²ˆí˜¸ ì¶”ì 
            
            for chunk_idx, chunk_info in enumerate(chunks):
                chunk = chunk_info['content']
                page_start = chunk_info['page_start']
                page_end = chunk_info['page_end']
                estimated_questions = chunk_info['estimated_questions']
                chunk_type = chunk_info.get('chunk_type', 'single_page')
                
                print(f"Processing chunk {chunk_idx + 1}/{len(chunks)} (í˜ì´ì§€ {page_start}-{page_end}, {len(chunk)} chars, ì˜ˆìƒ {estimated_questions}ë¬¸ì œ)...")
                
                # ê²½ê³„ ì²­í¬ì— ëŒ€í•œ íŠ¹ë³„ ì²˜ë¦¬
                if chunk_type == 'boundary_overlap':
                    print(f"   ğŸ”— ê²½ê³„ ì²­í¬: ì„ íƒì§€ ë³µêµ¬ ë° ë¬¸ì œ ì—°ê²° íŠ¹í™” ì²˜ë¦¬")
                    chunk = await self._enhance_boundary_chunk_processing(chunk, chunk_info)
                
                # ì²­í¬ íƒ€ì…ë³„ íŠ¹í™” í”„ë¡¬í”„íŠ¸ ì„ íƒ
                if chunk_type == 'boundary_overlap':
                    enhanced_prompt = await self._get_boundary_recovery_prompt(chunk, page_start, page_end)
                else:
                    enhanced_prompt = f"""
ğŸ“š **ì‹œí—˜ë¬¸ì œ ì¶”ì¶œ ì „ë¬¸ê°€ (í•´ì„¤ ì œì™¸)**

**ì¤‘ìš”: ì‹œí—˜ë¬¸ì œë§Œ ì¶”ì¶œí•˜ê³  í•´ì„¤/ì •ë‹µ/í’€ì´ëŠ” ì™„ì „íˆ ì œì™¸í•˜ì„¸ìš”.**

ğŸ” **ì…ë ¥ í…ìŠ¤íŠ¸**:
{chunk}

ğŸ¯ **ì—„ê²©í•œ ë¬¸ì œ ì‹ë³„ ê·œì¹™**:

1. **ì‹œí—˜ë¬¸ì œ íŒë³„**:
   âœ… "1.", "2.", "3." ë“±ìœ¼ë¡œ ì‹œì‘í•˜ê³  ë¬¼ì–´ë³´ëŠ” í˜•íƒœ ("~ë¬´ì—‡ì¸ê°€?", "~ì˜³ì€ ê²ƒì€?", "~ì„¤ëª…í•˜ì‹œì˜¤")
   âŒ í•´ì„¤ ë¬¸ì¥: "~ì…ë‹ˆë‹¤", "~ì…ë‹ˆë‹¤.", "~ì— ëŒ€í•œ ì„¤ëª…", "ì •ë‹µì€ ~", "í’€ì´:"

2. **í•´ì„¤ í˜ì´ì§€ ì™„ì „ ì œì™¸**:
   âŒ "í”„ë¡œì„¸ìŠ¤ì˜ ì£¼ìš” 3ê°€ì§€ ìƒíƒœëŠ” ì¤€ë¹„(Ready), ì‹¤í–‰(Running), ëŒ€ê¸°(Wait, Block)ì…ë‹ˆë‹¤."
   âŒ "íŒŒìŠ¤-íƒ€(PaaS-TA)ëŠ” ì†Œí”„íŠ¸ì›¨ì–´ ê°œë°œ í™˜ê²½ì„ ì œê³µí•˜ê¸° ìœ„í•´ ê°œë°œí•œ..."
   âŒ "~ì— ëŒ€í•œ ì„¤ëª…ì…ë‹ˆë‹¤", "~ë¼ê³  í•©ë‹ˆë‹¤", "~ì…ë‹ˆë‹¤"

3. **ì‹¤ì œ ë¬¸ì œ ì˜ˆì‹œ**:
   âœ… "ë‹¤ìŒ ì¤‘ í”„ë¡œì„¸ìŠ¤ì˜ ìƒíƒœê°€ ì•„ë‹Œ ê²ƒì€?"
   âœ… "ì• ìì¼ ê°œë°œ ë°©ë²•ë¡ ì— í•´ë‹¹í•˜ì§€ ì•ŠëŠ” ê²ƒì€?"
   âœ… "ë‹¤ìŒ ì„¤ëª…ì— í•´ë‹¹í•˜ëŠ” ê²ƒì€ ë¬´ì—‡ì¸ê°€?"

4. **ì„ íƒì§€ ì¶”ì¶œ ê°•í™”**:
   - â‘ â‘¡â‘¢â‘£â‘¤ ë˜ëŠ” 1)2)3)4)5) í˜•íƒœ ì™„ì „ ì¶”ì¶œ
   - ê° ì„ íƒì§€ëŠ” ì™„ì „í•œ ë‹µì•ˆ ì˜µì…˜ì´ì–´ì•¼ í•¨
   - ğŸ” **ì„ íƒì§€ ë¶€ì¡± ì‹œ íŠ¹ë³„ ì²˜ë¦¬**:
     * ì„ íƒì§€ê°€ 2ê°œ ë¯¸ë§Œì´ë©´ â†’ "incomplete_choices": true ì¶”ê°€
     * ì„ íƒì§€ê°€ í˜ì´ì§€ ëì—ì„œ ì˜ë¦° ê²½ìš° â†’ "needs_merge_with_next_chunk": true ì¶”ê°€
     * í‘œ/ë‹¤ì´ì–´ê·¸ë¨ ì°¸ì¡°ê°€ ìˆìœ¼ë©´ â†’ "has_table_reference": true ì¶”ê°€
   - ğŸ–¼ï¸ **ì´ë¯¸ì§€/í‘œ ì„ íƒì§€**:
     * ì„ íƒì§€ì— ìˆ«ìë§Œ ìˆê±°ë‚˜ ê·¸ë˜í”„/í‘œê°€ ë³´ì´ë©´ â†’ "IMG_XXX_IMAGE" í˜•íƒœë¡œ í‘œì‹œ
     * í‘œ ë°ì´í„°ê°€ í¬í•¨ëœ ë¬¸ì œ â†’ passageì— í‘œ ì „ì²´ í¬í•¨
   - âš ï¸ **í˜ì´ì§€ ê²½ê³„ ì²˜ë¦¬**:
     * ë¬¸ì œê°€ ì¤‘ê°„ì— ëŠì–´ì§„ ê²ƒ ê°™ìœ¼ë©´ â†’ "question_text_incomplete": true ì¶”ê°€
     * ì„ íƒì§€ ë²ˆí˜¸ê°€ ì—°ì†ë˜ì§€ ì•Šìœ¼ë©´ â†’ "missing_choice_numbers": [ëˆ„ë½ë²ˆí˜¸] ì¶”ê°€
   - í•´ì„¤ ë²ˆí˜¸ëŠ” ì œì™¸: "â‘  í•´ì„¤", "â‘¡ ì„¤ëª…" ë“±
   - ì„ íƒì§€ í…ìŠ¤íŠ¸ ì™„ì „ ë³´ì¡´ (ë„ì–´ì“°ê¸°, íŠ¹ìˆ˜ë¬¸ì í¬í•¨)

5. **ì§€ë¬¸ ë¶„ë¦¬**: 
   - ë¬¸ì œ ì• í‘œ/ê·¸ë˜í”„/ì¡°ê±´ â†’ passage í•„ë“œ
   - ë¬¸ì œ ë³¸ë¬¸ë§Œ â†’ question_text í•„ë“œ

6. **ë¹ˆ í˜ì´ì§€ ì²˜ë¦¬**: 
   - ë¬¸ì œê°€ ì—†ìœ¼ë©´ ë¹ˆ ë°°ì—´ ë°˜í™˜: {{"questions": []}}
   - "-|", "|", ë¹ˆ í…ìŠ¤íŠ¸ â†’ {{"questions": []}}

ğŸ“‹ **JSON ì¶œë ¥ (ì˜ˆì‹œ)**:
{{
  "questions": [
    {{
      "question_id": "Q{page_start:02d}_{page_end:02d}_001",
      "question_number": 7,
      "question_text": "Processì˜ 3ê°€ì§€ ìƒíƒœì— í•´ë‹¹í•˜ì§€ ì•ŠëŠ” ê²ƒì€?",
      "passage": "",
      "options": [
        "â‘  Ready",
        "â‘¡ Running", 
        "â‘¢ Block",
        "â‘£ Indexing"
      ],
      "correct_answer": "â‘£",
      "explanation": "",
      "source_page": {page_start},
      "incomplete_choices": false,
      "needs_merge_with_next_chunk": false,
      "has_table_reference": false,
      "question_text_incomplete": false,
      "missing_choice_numbers": []
    }},
    {{
      "question_id": "Q{page_start:02d}_{page_end:02d}_002",
      "question_number": 6,
      "question_text": "ë‹¤ìŒ í‘œë¥¼ ë³´ê³  í‰ê·  ë°˜í™˜ ì‹œê°„ì„ êµ¬í•˜ì‹œì˜¤.",
      "passage": "í”„ë¡œì„¸ìŠ¤ | ë„ì°©ì‹œê°„ | ì‹¤í–‰ì‹œê°„\\nP1 | 0 | 3\\nP2 | 1 | 7\\nP3 | 2 | 2",
      "options": [
        "IMG_006_IMAGE",
        "7.2",
        "9.4",
        "IMG_006_IMAGE"
      ],
      "incomplete_choices": true,
      "needs_merge_with_next_chunk": true,
      "has_table_reference": true,
      "source_page": {page_start}
    }}
  ]
}}

âš ï¸ **ì¤‘ìš”**:
- ëª¨ë“  ë¬¸ì œ ì™„ì „ ì¶”ì¶œ (ë¹ íŠ¸ë¦¼ ì—†ì´)
- ì„ íƒì§€ í…ìŠ¤íŠ¸ ì •í™•íˆ ë³´ì¡´
- ì •ë‹µ ë²ˆí˜¸ ì •í™•íˆ ì‹ë³„
- JSONë§Œ ì¶œë ¥, ì„¤ëª… ê¸ˆì§€
- ë¹ˆ í˜ì´ì§€ë©´ {{"questions": []}}
                """.strip()
                
                try:
                    response = await self.claude_client.messages.create(
                        model="claude-3-5-sonnet-20241022",
                        max_tokens=8000,
                        temperature=0.01,  # ê°€ì¥ ì¼ê´€ëœ ê²°ê³¼
                        messages=[{
                            "role": "user",
                            "content": enhanced_prompt
                        }]
                    )
                    
                    result_text = response.content[0].text.strip()
                    total_tokens += response.usage.input_tokens + response.usage.output_tokens
                    
                    # ê°•í™”ëœ JSON ì¶”ì¶œ ë° íŒŒì‹±
                    clean_text = result_text.strip()
                    
                    # ë§ˆí¬ë‹¤ìš´ ì½”ë“œ ë¸”ë¡ ì œê±°
                    if clean_text.startswith('```json'):
                        clean_text = clean_text.split('```json', 1)[1].rsplit('```', 1)[0]
                    elif clean_text.startswith('```'):
                        clean_text = clean_text.split('```', 1)[1].rsplit('```', 1)[0]
                    
                    # JSON ì‹œì‘/ë ì°¾ê¸° (ì¤‘ë³µ JSON ë°©ì§€)
                    json_start = clean_text.find('{')
                    if json_start != -1:
                        # ì²« ë²ˆì§¸ { ë¶€í„° ë§ˆì§€ë§‰ } ê¹Œì§€ë§Œ ì¶”ì¶œ
                        brace_count = 0
                        json_end = -1
                        for i in range(json_start, len(clean_text)):
                            if clean_text[i] == '{':
                                brace_count += 1
                            elif clean_text[i] == '}':
                                brace_count -= 1
                                if brace_count == 0:
                                    json_end = i
                                    break
                        
                        if json_end != -1:
                            clean_text = clean_text[json_start:json_end + 1]
                    
                    # ì•ˆì „í•œ JSON íŒŒì‹± ì‚¬ìš©
                    chunk_result, parse_status = self.safe_json_parse(clean_text, chunk_idx + 1)
                    
                    # íŒŒì‹± ìƒíƒœì— ë”°ë¥¸ ë¡œê¹…
                    if parse_status is None:
                        print(f"   âœ… Chunk {chunk_idx + 1} JSON íŒŒì‹± ì„±ê³µ")
                    elif parse_status == "fixed":
                        print(f"   ğŸ”§ Chunk {chunk_idx + 1} JSON ì˜¤ë¥˜ ìˆ˜ì •ìœ¼ë¡œ ë³µêµ¬")
                    elif parse_status == "partial":
                        print(f"   âš ï¸ Chunk {chunk_idx + 1} ë¶€ë¶„ì  ë³µêµ¬")
                    else:
                        print(f"   âŒ Chunk {chunk_idx + 1} JSON íŒŒì‹± ì‹¤íŒ¨: {parse_status}")
                    
                    # ë¹ˆ ê²°ê³¼ í•„í„°ë§
                    if not chunk_result or not isinstance(chunk_result, dict):
                        chunk_result = {"questions": []}
                    elif "questions" not in chunk_result:
                        chunk_result["questions"] = []
                    
                    # ê²°ê³¼ ìˆ˜ì§‘ ë° í’ˆì§ˆ ê°œì„ 
                    chunk_questions = chunk_result.get('questions', [])
                    
                    # ë¬¸ì œ í’ˆì§ˆ ê²€ì¦ ë° ê°œì„ 
                    validated_questions = []
                    for question in chunk_questions:
                        # ê¸°ë³¸ ì •ë³´ ì¶”ê°€
                        question['chunk_origin'] = chunk_idx + 1
                        question['pages_processed'] = f"{page_start}-{page_end}"
                        
                        # ê´€ëŒ€í•œ í•„ë“œ ê²€ì¦ - ë¬¸ì œ ëˆ„ë½ ë°©ì§€ë¥¼ ìœ„í•´ ê¸°ì¤€ ì™„í™”
                        question_text = question.get('question_text', '').strip()
                        if not question_text or len(question_text) < 5:  # 10 â†’ 5ë¡œ ì™„í™”
                            print(f"Skipping invalid question (too short): {question.get('question_id', 'unknown')}")
                            continue
                        
                        # ì„ ë³„ì  ë¹„-ì§ˆë¬¸ íŒ¨í„´ í•„í„°ë§ - ëª…í™•í•œ ê²½ìš°ë§Œ ì œì™¸
                        if self._is_clearly_non_question(question_text):
                            print(f"Skipping clearly non-question: '{question_text[:50]}...'")
                            continue
                            
                        # ì„ íƒì§€ ì •ë¦¬ (ì´ë¯¸ì§€ ì°¸ì¡° ë³´ì¡´)
                        options = question.get('options', [])
                        if isinstance(options, list) and len(options) > 0:
                            clean_options = []
                            for opt in options:
                                if opt:
                                    opt_clean = opt.strip()
                                    # IMG_XXX_IMAGE í˜•íƒœì˜ ì´ë¯¸ì§€ ì°¸ì¡° ë³´ì¡´
                                    if 'IMG_' in opt_clean and 'IMAGE' in opt_clean:
                                        clean_options.append(opt_clean)
                                    # DIAGRAM_IMAGE í˜•íƒœì˜ ì´ë¯¸ì§€ ì°¸ì¡° ë³´ì¡´
                                    elif 'DIAGRAM_IMAGE' in opt_clean:
                                        clean_options.append(opt_clean)
                                    # ì¼ë°˜ í…ìŠ¤íŠ¸ ì„ íƒì§€ëŠ” ë¹„ì–´ìˆì§€ ì•Šì€ ê²½ìš°ë§Œ
                                    elif opt_clean and opt_clean not in ['-', '--', '---']:
                                        clean_options.append(opt_clean)
                            question['options'] = clean_options
                        else:
                            question['options'] = []
                            
                        # ì •ë‹µ ì •ë¦¬
                        correct_answer = question.get('correct_answer', '')
                        if correct_answer and correct_answer not in ['', 'null', 'None']:
                            question['correct_answer'] = correct_answer.strip()
                        else:
                            question['correct_answer'] = ''
                            
                        # ì§€ë¬¸ ì •ë¦¬
                        passage = question.get('passage', '')
                        if passage and passage.strip():
                            question['passage'] = passage.strip()
                        else:
                            question['passage'] = ''
                            
                        # ğŸ†• ì¤‘ë³µ ë¬¸ì œ ë²ˆí˜¸ í•„í„°ë§
                        question_num = question.get('question_number')
                        if question_num in extracted_question_numbers:
                            print(f"âš ï¸ ì¤‘ë³µ ë¬¸ì œ ë²ˆí˜¸ {question_num} ìŠ¤í‚µ - ì´ë¯¸ ì¶”ì¶œë¨")
                            continue
                        
                        # ğŸš¨ ê°•í™”ëœ ì½”ë“œ ë¸”ë¡ ì²˜ë¦¬ ([CODE_START]...[CODE_END])
                        question_text = question.get('question_text', '')
                        passage = question.get('passage', '')
                        
                        # ì½”ë“œ ë¸”ë¡ ì²˜ë¦¬ (ì—¬ëŸ¬ ë§ˆì»¤ ì§€ì›)
                        code_markers = [
                            ('[CODE_START]', '[CODE_END]'),
                            ('[CODE_BLOCK_START]', '[CODE_BLOCK_END]')
                        ]
                        
                        for start_marker, end_marker in code_markers:
                            if start_marker in question_text or start_marker in passage:
                                import re
                                # ì§ˆë¬¸ í…ìŠ¤íŠ¸ì—ì„œ ì½”ë“œ ë¸”ë¡ ì²˜ë¦¬
                                if start_marker in question_text:
                                    code_blocks = re.findall(f'{re.escape(start_marker)}(.*?){re.escape(end_marker)}', question_text, re.DOTALL)
                                    for code_block in code_blocks:
                                        # ë“¤ì—¬ì“°ê¸° ì™„ì „ ë³´ì¡´ (strip í•˜ì§€ ì•ŠìŒ)
                                        preserved_code = code_block
                                        question_text = question_text.replace(f'{start_marker}{code_block}{end_marker}', 
                                                                             f'```\n{preserved_code}\n```')
                                    question['question_text'] = question_text
                                
                                # ì§€ë¬¸ì—ì„œ ì½”ë“œ ë¸”ë¡ ì²˜ë¦¬
                                if start_marker in passage:
                                    code_blocks = re.findall(f'{re.escape(start_marker)}(.*?){re.escape(end_marker)}', passage, re.DOTALL)
                                    for code_block in code_blocks:
                                        preserved_code = code_block
                                        passage = passage.replace(f'{start_marker}{code_block}{end_marker}', 
                                                                f'```\n{preserved_code}\n```')
                                    question['passage'] = passage
                                
                                question['has_code'] = True
                                print(f"âœ… Q{question_num} ì½”ë“œ ë¸”ë¡ ì²˜ë¦¬ ì™„ë£Œ ({start_marker})")
                        
                        # ğŸš¨ ê°•í™”ëœ í˜ì´ì§€ ê²½ê³„ ì„ íƒì§€ ë³‘í•© ì²˜ë¦¬
                        options = question.get('options', [])
                        
                        # ì„ íƒì§€ ë¶€ì¡± ê°ì§€ ë° ì¦‰ì‹œ ë³µêµ¬ ì‹œë„ (3ê°œ ë¯¸ë§Œì´ë©´ ì˜ì‹¬) - ê¸°ì¤€ ì™„í™”
                        if len(options) < 3:
                            question['incomplete_choices'] = True
                            print(f"âš ï¸ Q{question_num} ì„ íƒì§€ ë¶€ì¡± ê°ì§€: {len(options)}ê°œë§Œ ìˆìŒ - ë³µêµ¬ ì‹œë„")
                            
                            # ì¦‰ì‹œ ë³µêµ¬ ì‹œë„: í˜„ì¬ ì²­í¬ì˜ ë‹¤ë¥¸ ë¬¸ì œë“¤ì—ì„œ ì—°ê²°ëœ ì„ íƒì§€ ì°¾ê¸°
                            recovered_options = self._attempt_immediate_choice_recovery(
                                question_num, options, chunk_questions
                            )
                            
                            if len(recovered_options) > len(options):
                                print(f"âœ… ì¦‰ì‹œ ì„ íƒì§€ ë³µêµ¬ ì„±ê³µ: {len(options)}ê°œ â†’ {len(recovered_options)}ê°œ")
                                question['options'] = recovered_options
                                question['recovered_from_page_boundary'] = True
                                del question['incomplete_choices']  # ë³µêµ¬ë˜ì—ˆìœ¼ë¯€ë¡œ ì‚­ì œ
                            else:
                                question['missing_choices_note'] = f"ì„ íƒì§€ {len(options)}ê°œë§Œ ì¸ì‹ë¨ - í˜ì´ì§€ ê²½ê³„ ë¬¸ì œ ì˜ì‹¬"
                            
                            # ë‹¤ìŒ ì²­í¬ì—ì„œ ê³ ì•„ ì„ íƒì§€ ì°¾ì•„ì„œ ë³‘í•© ì‹œë„
                            if chunk_idx < len(chunks) - 1:  # ë§ˆì§€ë§‰ ì²­í¬ê°€ ì•„ë‹Œ ê²½ìš°
                                question['needs_merge_with_next_chunk'] = True
                                print(f"ğŸ”— Q{question_num} ë‹¤ìŒ ì²­í¬ì™€ ë³‘í•© í•„ìš” í‘œì‹œ")
                        
                        # í˜ì´ì§€ ê²½ê³„ ë§ˆì»¤ê°€ ìˆëŠ” ê²½ìš° ì¶”ê°€ ì²˜ë¦¬
                        combined_text = question_text + passage + ' '.join(options)
                        boundary_markers = ['ë‹¤ìŒ í˜ì´ì§€', 'í˜ì´ì§€ ê²½ê³„', 'ì˜ë¦¼', 'â‘¢â‘£â‘¤ ë‹¤ìŒ', 'â‘£â‘¤ ë‹¤ìŒ', 'â‘¤ ë‹¤ìŒ']
                        for marker in boundary_markers:
                            if marker in combined_text:
                                question['page_boundary_detected'] = True
                                question['boundary_marker'] = marker
                                print(f"ğŸ” Q{question_num} í˜ì´ì§€ ê²½ê³„ ë§ˆì»¤ ë°œê²¬: '{marker}'")
                        
                        extracted_question_numbers.add(question_num)
                        validated_questions.append(question)
                    
                    all_questions.extend(validated_questions)
                    
                    print(f"Chunk {chunk_idx + 1} (í˜ì´ì§€ {page_start}-{page_end}): {len(validated_questions)} questions extracted")
                    if validated_questions:
                        question_numbers = [q.get('question_number', 'N/A') for q in validated_questions]
                        print(f"  Question numbers: {question_numbers}")
                        
                        # í’ˆì§ˆ ì²´í¬ ê²°ê³¼ ì¶œë ¥
                        questions_with_options = len([q for q in validated_questions if q.get('options')])
                        questions_with_answers = len([q for q in validated_questions if q.get('correct_answer')])
                        questions_with_passage = len([q for q in validated_questions if q.get('passage')])
                        
                        print(f"  Quality: Options:{questions_with_options}/{len(validated_questions)}, "
                              f"Answers:{questions_with_answers}/{len(validated_questions)}, "
                              f"Passages:{questions_with_passage}/{len(validated_questions)}")
                    
                except Exception as chunk_error:
                    print(f"Error processing chunk {chunk_idx + 1}: {chunk_error}")
                    continue
            
            print(f"Enhanced extraction completed: {len(all_questions)} total questions")
            
            # ğŸš€ ì„ íƒì  í›„ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ ì ìš© (ì•ˆì •ì„± ìš°ì„ )
            print(f"\nğŸ”§ ê¸°ë³¸ ì¶”ì¶œ ì™„ë£Œ: {len(all_questions)} ë¬¸ì œ")
            
            # ê°„ë‹¨í•œ í›„ì²˜ë¦¬ë§Œ ì ìš© (ë¬¸ì œ ì¸ì‹ë¥  ìš°ì„ )
            try:
                # 1. ê°œì„ ëœ ì¤‘ë³µ ë¬¸ì œ ë³‘í•© ì‹œìŠ¤í…œ
                all_questions = self.merge_duplicate_questions(all_questions)
                print(f"âœ… ì¤‘ë³µ ë³‘í•© ì™„ë£Œ: {len(all_questions)} ë¬¸ì œ")
                
                # 2. ê°œì„ ëœ ì´ë¯¸ì§€ ì„ íƒì§€ ì²˜ë¦¬ (ìŠ¤ë§ˆíŠ¸ ë§¤ì¹­)
                processed_count = await self._enhanced_image_choice_processing(all_questions, upload_id)
                if processed_count > 0:
                    print(f"âœ… ê°œì„ ëœ ì´ë¯¸ì§€ ì„ íƒì§€ ì²˜ë¦¬: {processed_count}ê°œ ë¬¸ì œ")
                
            except Exception as pipeline_error:
                print(f"âš ï¸ í›„ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ ì˜¤ë¥˜ (ê¸°ë³¸ ê²°ê³¼ ìœ ì§€): {pipeline_error}")
            
            # ğŸ” 2ë‹¨ê³„: êµ¬ì¡° ë¶„ì„ ê¸°ë°˜ í˜ì´ì§€ ê²½ê³„ ë¬¸ì œ í•´ê²°
            print("\nğŸ” êµ¬ì¡° ë¶„ì„ ê¸°ë°˜ í˜ì´ì§€ ê²½ê³„ ì„ íƒì§€ ê²€ìˆ˜ ì‹œì‘...")
            
            try:
                # êµ¬ì¡° ë¶„ì„ì—ì„œ ì‹ë³„ëœ í˜ì´ì§€ ê²½ê³„ ë¬¸ì œë“¤ì„ ìš°ì„  ì²˜ë¦¬
                all_questions = await self._resolve_boundary_issues_with_structure(
                    extracted_questions=all_questions,
                    structure_analysis=document_structure,
                    upload_id=upload_id,
                    db=db
                )
                print(f"âœ… êµ¬ì¡° ë¶„ì„ ê¸°ë°˜ í˜ì´ì§€ ê²½ê³„ ê²€ìˆ˜ ì™„ë£Œ: ìµœì¢… {len(all_questions)} ë¬¸ì œ")
                
            except Exception as verification_error:
                print(f"âš ï¸ í˜ì´ì§€ ê²½ê³„ ê²€ìˆ˜ ì¤‘ ì˜¤ë¥˜ (ê³„ì† ì§„í–‰): {verification_error}")
            
            # ğŸ” 3ë‹¨ê³„: ë³´ì™„ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ (OCR ë˜ëŠ” ëŒ€ì•ˆ ë°©ì‹)
            print(f"\nğŸ” ë¬¸ì œ ë³´ì™„ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ - ê¸°ë³¸ ì¶”ì¶œ ê²°ê³¼ ê°œì„ ...")
            
            try:
                # PDF ê²½ë¡œ íšë“ í›„ OCR ì‹œë„
                pdf_path = self.get_pdf_path(upload_id, db)
                if pdf_path:
                    # OCR ê¸°ë°˜ ë³´ì™„ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰
                    enhanced_questions = await self._ocr_based_enhancement_pipeline(
                        upload_id=upload_id,
                        basic_questions=all_questions,
                        pdf_path=pdf_path,
                        db=db
                    )
                    print(f"ğŸ¯ OCR ë³´ì™„ ì™„ë£Œ: {len(all_questions)} â†’ {len(enhanced_questions)} ë¬¸ì œ")
                    all_questions = enhanced_questions
                else:
                    # OCR ì—†ì´ ëŒ€ì•ˆ ë³´ì™„ ë°©ì‹ ì‚¬ìš©
                    print("ğŸ“ ëŒ€ì•ˆ ë³´ì™„ ë°©ì‹ ì‚¬ìš©: íŒ¨í„´ ê¸°ë°˜ ë¬¸ì œ ë³µêµ¬")
                    enhanced_questions = await self._alternative_enhancement_pipeline(
                        upload_id=upload_id,
                        basic_questions=all_questions,
                        markdown_content=markdown_result.get('markdown_content', ''),
                        db=db
                    )
                    print(f"ğŸ¯ ëŒ€ì•ˆ ë³´ì™„ ì™„ë£Œ: {len(all_questions)} â†’ {len(enhanced_questions)} ë¬¸ì œ")
                    all_questions = enhanced_questions
                    
            except Exception as enhance_error:
                print(f"âš ï¸ ë³´ì™„ íŒŒì´í”„ë¼ì¸ ì˜¤ë¥˜ (ê¸°ë³¸ ê²°ê³¼ ìœ ì§€): {enhance_error}")
            
            # ğŸ–¼ï¸ 4ë‹¨ê³„: ìµœì¢… ì´ë¯¸ì§€ ì°¸ì¡° í˜•ì‹ ë³€í™˜
            print(f"\nğŸ–¼ï¸ ìµœì¢… ì´ë¯¸ì§€ ì°¸ì¡° í˜•ì‹ ë³€í™˜...")
            try:
                all_questions = self._convert_image_references_to_markdown(all_questions, upload_id)
                print(f"âœ… ì´ë¯¸ì§€ ì°¸ì¡° ë³€í™˜ ì™„ë£Œ")
            except Exception as img_convert_error:
                print(f"âš ï¸ ì´ë¯¸ì§€ ì°¸ì¡° ë³€í™˜ ì˜¤ë¥˜: {img_convert_error}")
            
            # ê¸°ë³¸ ì±•í„° ìƒì„± (ê³¼ëª©ë³„)
            chapters = [
                {
                    'chapter_number': 1,
                    'chapter_title': 'ì •ë³´ì‹œìŠ¤í…œ ê¸°ë³¸ ê¸°ìˆ ',
                    'main_topics': ['UML', 'ì†Œí”„íŠ¸ì›¨ì–´ ê°œë°œ', 'ë„¤íŠ¸ì›Œí¬'],
                    'learning_objectives': ['ì •ë³´ì‹œìŠ¤í…œ ê¸°ë³¸ ê°œë… ì´í•´'],
                    'page_range': '1-8'
                }
            ]
            
            # ë°ì´í„°ë² ì´ìŠ¤ ì €ì¥
            chapter_ids = {}
            for chapter in chapters:
                try:
                    result = db.execute(text("""
                        INSERT INTO chapters
                        (pdf_upload_id, chapter_number, title, main_topics, learning_objectives, page_range)
                        VALUES (:upload_id, :chapter_number, :title, :main_topics, :learning_objectives, :page_range)
                    """), {
                        'upload_id': upload_id,
                        'chapter_number': chapter.get('chapter_number', 0),
                        'title': chapter.get('chapter_title', ''),
                        'main_topics': json.dumps(chapter.get('main_topics', [])),
                        'learning_objectives': json.dumps(chapter.get('learning_objectives', [])),
                        'page_range': chapter.get('page_range', '')
                    })
                    chapter_ids[chapter.get('chapter_number', 0)] = result.lastrowid
                    print(f"Saved chapter: {chapter.get('chapter_title', 'Untitled')}")
                except Exception as db_error:
                    print(f"Failed to save chapter: {db_error}")
            
            # ë¬¸ì œ ì €ì¥
            saved_questions = 0
            for question in all_questions:
                try:
                    chapter_id = chapter_ids.get(1)  # ê¸°ë³¸ ì±•í„° ì‚¬ìš©
                    db.execute(text("""
                        INSERT INTO extracted_questions
                        (pdf_upload_id, chapter_id, question_id, question_text, question_type,
                         difficulty_level, correct_answer, options, explanation, bloom_taxonomy,
                         topic_tags, estimated_time, keywords, passage, additional_info)
                        VALUES (:upload_id, :chapter_id, :question_id, :question_text, :question_type,
                                :difficulty, :correct_answer, :options, :explanation, :bloom_taxonomy,
                                :topic_tags, :estimated_time, :keywords, :passage, :additional_info)
                    """), {
                        'upload_id': upload_id,
                        'chapter_id': chapter_id,
                        'question_id': question.get('question_id', f'Q{saved_questions + 1:03d}'),
                        'question_text': question.get('question_text', ''),
                        'question_type': question.get('question_type', 'ê°ê´€ì‹'),
                        'difficulty': question.get('difficulty_level', 'ì¤‘'),
                        'correct_answer': question.get('correct_answer', ''),
                        'options': json.dumps(question.get('options', [])),
                        'explanation': question.get('explanation', ''),
                        'bloom_taxonomy': question.get('bloom_taxonomy', 'ì´í•´'),
                        'topic_tags': json.dumps(question.get('topic_tags', [])),
                        'estimated_time': question.get('estimated_time', '2'),
                        'keywords': json.dumps(question.get('keywords', [])),
                        'passage': question.get('passage', ''),
                        'additional_info': question.get('additional_info', '')
                    })
                    saved_questions += 1
                except Exception as db_error:
                    print(f"Failed to save question {question.get('question_id', 'unknown')}: {db_error}")
            
            db.commit()
            
            # í•™ìŠµ ë¶„ì„
            analytics = {
                'total_questions': len(all_questions),
                'extraction_method': 'Enhanced GPT Vision + Claude Chunked',
                'chunks_processed': len(chunks),
                'total_tokens_used': total_tokens,
                'questions_saved': saved_questions
            }
            
            await self._log_processing_step(upload_id, "enhanced_markdown_structuring", "completed", {
                'questions_extracted': len(all_questions),
                'questions_saved': saved_questions,
                'chunks_processed': len(chunks),
                'total_tokens': total_tokens
            }, db)
            
            return {
                'success': True,
                'questions': all_questions,
                'materials': all_materials,
                'chapters': chapters,
                'document_analysis': {
                    'extraction_method': 'Enhanced GPT Vision + Claude',
                    'total_pages_analyzed': markdown_result.get('pages_processed', 0)
                },
                'learning_analytics': analytics,
                'claude_tokens': total_tokens
            }
            
        except Exception as e:
            logger.error(f"Enhanced markdown structuring error: {str(e)}")
            await self._log_processing_step(upload_id, "enhanced_markdown_structuring", "failed", {"error": str(e)}, db)
            return {
                'success': False,
                'questions': [],
                'materials': [],
                'chapters': [],
                'error': str(e)
            }
    
    async def _convert_pdf_to_combined_image(self, upload_id: int, pdf_path: str, db: Session) -> Dict[str, Any]:
        """0ë‹¨ê³„: PDF ì „ì²´ë¥¼ í•˜ë‚˜ì˜ ê¸´ ì´ë¯¸ì§€ë¡œ ë³€í™˜"""
        try:
            await self._log_processing_step(upload_id, "pdf_to_image_conversion", "processing", {}, db)
            
            doc = fitz.open(pdf_path)
            total_pages = len(doc)
            
            print(f"Converting {total_pages} pages to combined image...")
            
            # ëª¨ë“  í˜ì´ì§€ë¥¼ ì´ë¯¸ì§€ë¡œ ë³€í™˜
            page_images = []
            max_width = 0
            
            for page_num in range(total_pages):
                page = doc[page_num]
                # ë§¤ìš° ê³ í•´ìƒë„ ë Œë”ë§ (OCR í’ˆì§ˆ í–¥ìƒì„ ìœ„í•´ 4ë°° í™•ëŒ€)
                mat = fitz.Matrix(4.0, 4.0)  # 4ë°° í™•ëŒ€ë¡œ í…ìŠ¤íŠ¸ ì„ ëª…ë„ ì¦ê°€
                pix = page.get_pixmap(matrix=mat, alpha=False)  # alpha ì±„ë„ ì œê±°ë¡œ ìš©ëŸ‰ ì ˆì•½
                
                # PIL ì´ë¯¸ì§€ë¡œ ë³€í™˜
                img_data = pix.tobytes("png")
                pil_image = Image.open(io.BytesIO(img_data))
                
                page_images.append(pil_image)
                max_width = max(max_width, pil_image.width)
                
                print(f"Processed page {page_num + 1}/{total_pages}")
            
            doc.close()
            
            # ëª¨ë“  ì´ë¯¸ì§€ë¥¼ ê°™ì€ í­ìœ¼ë¡œ ë¦¬ì‚¬ì´ì¦ˆí•˜ê³  ì„¸ë¡œë¡œ ì—°ê²°
            total_height = 0
            resized_images = []
            
            for img in page_images:
                # í­ì„ max_widthë¡œ í†µì¼
                if img.width != max_width:
                    ratio = max_width / img.width
                    new_height = int(img.height * ratio)
                    img = img.resize((max_width, new_height), Image.Resampling.LANCZOS)
                
                resized_images.append(img)
                total_height += img.height
            
            # ê¸´ ì´ë¯¸ì§€ ìƒì„±
            combined_image = Image.new('RGB', (max_width, total_height), color='white')
            
            current_y = 0
            for img in resized_images:
                combined_image.paste(img, (0, current_y))
                current_y += img.height
            
            # ê²°í•©ëœ ì´ë¯¸ì§€ ì €ì¥
            combined_image_path = self.temp_dir / f"combined_{upload_id}.png"
            combined_image.save(str(combined_image_path), 'PNG', optimize=True)
            
            print(f"Combined image created: {max_width}x{total_height} pixels")
            
            await self._log_processing_step(upload_id, "pdf_to_image_conversion", "completed", {
                'total_pages': total_pages,
                'combined_image_size': f"{max_width}x{total_height}",
                'image_path': str(combined_image_path)
            }, db)
            
            return {
                'success': True,
                'image_path': str(combined_image_path),
                'total_pages': total_pages,
                'image_width': max_width,
                'image_height': total_height
            }
            
        except Exception as e:
            logger.error(f"PDF to combined image conversion error: {str(e)}")
            await self._log_processing_step(upload_id, "pdf_to_image_conversion", "failed", 
                                          {"error": str(e)}, db)
            return {'success': False, 'error': str(e)}
    
    async def _analyze_combined_document(self, upload_id: int, image_path: str, db: Session) -> Dict[str, Any]:
        """1ë‹¨ê³„: í†µí•©ëœ ë¬¸ì„œ ì „ì²´ ë¶„ì„ (GPT Vision)"""
        try:
            await self._log_processing_step(upload_id, "combined_document_analysis", "processing", {}, db)
            
            print("Analyzing combined document with GPT Vision...")
            
            # GPT Visionìœ¼ë¡œ ì „ì²´ ë¬¸ì„œ ë¶„ì„
            gpt_analysis = await self._analyze_combined_document_with_gpt(image_path)
            
            # ë°ì´í„°ë² ì´ìŠ¤ì— ì €ì¥
            db.execute(text("""
                INSERT INTO document_analysis 
                (pdf_upload_id, total_pages, document_type, has_text_layer, is_scanned, 
                 layout_analysis, content_structure, document_metadata, api_key_used, 
                 total_tokens_used, analysis_cost)
                VALUES (:upload_id, :total_pages, :doc_type, :has_text, :is_scanned,
                        :layout, :structure, :doc_metadata, :api_key, :tokens, :cost)
            """), {
                'upload_id': upload_id,
                'total_pages': gpt_analysis.get('total_pages', 0),
                'doc_type': gpt_analysis.get('document_type'),
                'has_text': False,  # ì´ë¯¸ì§€ë¡œ ì²˜ë¦¬í•˜ë¯€ë¡œ False
                'is_scanned': True,  # ì´ë¯¸ì§€ë¡œ ì²˜ë¦¬í•˜ë¯€ë¡œ True
                'layout': json.dumps(gpt_analysis.get('layout_info', {})),
                'structure': json.dumps(gpt_analysis.get('content_structure', {})),
                'doc_metadata': json.dumps(gpt_analysis.get('metadata', {})),
                'api_key': gpt_analysis.get('api_key_used'),
                'tokens': gpt_analysis.get('tokens_used', 0),
                'cost': gpt_analysis.get('cost', 0.0)
            })
            
            doc_analysis_id = db.execute(text("SELECT last_insert_rowid()")).scalar()
            db.commit()
            
            await self._log_processing_step(upload_id, "combined_document_analysis", "completed", {
                'document_analysis_id': doc_analysis_id,
                'document_type': gpt_analysis.get('document_type'),
                'questions_detected': gpt_analysis.get('question_count', 0)
            }, db)
            
            return {
                'success': True,
                'document_analysis_id': doc_analysis_id,
                'analysis_result': gpt_analysis,
                'image_path': image_path
            }
            
        except Exception as e:
            logger.error(f"Combined document analysis error: {str(e)}")
            await self._log_processing_step(upload_id, "combined_document_analysis", "failed", 
                                          {"error": str(e)}, db)
            return {'success': False, 'error': str(e)}

    async def _analyze_document_structure(self, upload_id: int, pdf_path: str, db: Session) -> Dict[str, Any]:
        """1ë‹¨ê³„: ë¬¸ì„œ êµ¬ì¡° ë¶„ì„"""
        try:
            await self._log_processing_step(upload_id, "document_analysis", "processing", {}, db)
            
            # PDF ì—´ê¸°
            doc = fitz.open(pdf_path)
            total_pages = len(doc)
            
            # í…ìŠ¤íŠ¸ ë ˆì´ì–´ í™•ì¸
            has_text_layer = False
            text_quality_score = 0
            
            for page_num in range(min(3, total_pages)):  # ì²« 3í˜ì´ì§€ë§Œ ì²´í¬
                page = doc[page_num]
                text = page.get_text()
                if text.strip():
                    has_text_layer = True
                    # í…ìŠ¤íŠ¸ í’ˆì§ˆ í‰ê°€ (í•œê¸€, ì˜ë¬¸, ìˆ«ì ë¹„ìœ¨)
                    korean_chars = sum(1 for c in text if '\uac00' <= c <= '\ud7a3')
                    total_chars = len(text.strip())
                    if total_chars > 0:
                        text_quality_score += korean_chars / total_chars
            
            text_quality_score /= min(3, total_pages)
            is_scanned = not has_text_layer or text_quality_score < 0.1
            
            # ì²« í˜ì´ì§€ ì¸ë„¤ì¼ ìƒì„± (ë¬¸ì„œ íƒ€ì… ë¶„ì„ìš©)
            first_page = doc[0]
            pix = first_page.get_pixmap(matrix=fitz.Matrix(1.5, 1.5))  # 1.5ë°° í™•ëŒ€
            thumbnail_path = self.temp_dir / f"doc_{upload_id}_p0_thumb.png"
            pix.save(str(thumbnail_path))
            
            # GPT Visionìœ¼ë¡œ ë¬¸ì„œ íƒ€ì… ë¶„ì„
            gpt_analysis = await self._analyze_document_with_gpt(str(thumbnail_path), total_pages)
            
            # ë°ì´í„°ë² ì´ìŠ¤ì— ì €ì¥
            db.execute(text("""
                INSERT INTO document_analysis 
                (pdf_upload_id, total_pages, document_type, has_text_layer, is_scanned, 
                 layout_analysis, content_structure, document_metadata, api_key_used, 
                 total_tokens_used, analysis_cost)
                VALUES (:upload_id, :total_pages, :doc_type, :has_text, :is_scanned,
                        :layout, :structure, :doc_metadata, :api_key, :tokens, :cost)
            """), {
                'upload_id': upload_id,
                'total_pages': total_pages,
                'doc_type': gpt_analysis.get('document_type'),
                'has_text': has_text_layer,
                'is_scanned': is_scanned,
                'layout': json.dumps(gpt_analysis.get('layout_info', {})),
                'structure': json.dumps(gpt_analysis.get('content_structure', {})),
                'doc_metadata': json.dumps(gpt_analysis.get('metadata', {})),
                'api_key': gpt_analysis.get('api_key_used'),
                'tokens': gpt_analysis.get('tokens_used', 0),
                'cost': gpt_analysis.get('cost', 0.0)
            })
            
            doc_analysis_id = db.execute(text("SELECT last_insert_rowid()")).scalar()
            db.commit()
            
            doc.close()
            
            await self._log_processing_step(upload_id, "document_analysis", "completed", {
                'document_analysis_id': doc_analysis_id,
                'total_pages': total_pages,
                'document_type': gpt_analysis.get('document_type'),
                'is_scanned': is_scanned
            }, db)
            
            return {
                'success': True,
                'document_analysis_id': doc_analysis_id,
                'total_pages': total_pages,
                'is_scanned': is_scanned,
                'document_type': gpt_analysis.get('document_type')
            }
            
        except Exception as e:
            logger.error(f"ë¬¸ì„œ êµ¬ì¡° ë¶„ì„ ì˜¤ë¥˜: {str(e)}")
            await self._log_processing_step(upload_id, "document_analysis", "failed", 
                                          {"error": str(e)}, db)
            return {'success': False, 'error': str(e)}
    
    async def _analyze_document_with_gpt(self, image_path: str, total_pages: int) -> Dict[str, Any]:
        """GPT Visionìœ¼ë¡œ ë¬¸ì„œ ë¶„ì„"""
        try:
            if not self.openai_client:
                return {
                    'document_type': 'unknown',
                    'layout_info': {},
                    'content_structure': {},
                    'metadata': {},
                    'error': 'OpenAI API key not set'
                }
            
            # ì´ë¯¸ì§€ë¥¼ base64ë¡œ ì¸ì½”ë”©
            with open(image_path, "rb") as image_file:
                base64_image = base64.b64encode(image_file.read()).decode('utf-8')
            
            prompt = f"""
ì´ ë¬¸ì„œëŠ” ì´ {total_pages}í˜ì´ì§€ì˜ PDF ë¬¸ì„œì˜ ì²« ë²ˆì§¸ í˜ì´ì§€ì…ë‹ˆë‹¤. 
ë‹¤ìŒ ì‚¬í•­ë“¤ì„ ë¶„ì„í•´ì„œ JSON í˜•íƒœë¡œ ë‹µë³€í•´ì£¼ì„¸ìš”:

1. document_type: ë¬¸ì„œ ì¢…ë¥˜ (ê¸°ì¶œë¬¸ì œì§‘, í•´ì„¤ì§‘, ìš”ì•½ì„œ, ì •ë‹µí‘œ, ìˆ˜í—˜ì•ˆë‚´ì„œ ë“±)
2. layout_info: ë ˆì´ì•„ì›ƒ ì •ë³´
   - column_count: ë‹¨ ìˆ˜ (1ë‹¨, 2ë‹¨, 3ë‹¨ ë“±)
   - has_header_footer: ë¨¸ë¦¬ê¸€/ë°”ë‹¥ê¸€ ì¡´ì¬ ì—¬ë¶€
   - text_density: í…ìŠ¤íŠ¸ ë°€ë„ (high, medium, low)
3. content_structure: ë‚´ìš© êµ¬ì¡°
   - question_format: ë¬¸ì œ í˜•ì‹ (ê°ê´€ì‹, ì£¼ê´€ì‹, í˜¼í•© ë“±)
   - numbering_style: ë²ˆí˜¸ ìŠ¤íƒ€ì¼ (1., 1), â‘ , (1) ë“±)
   - answer_choices: ì„ íƒì§€ í˜•ì‹ (â‘ â‘¡â‘¢â‘£â‘¤, ABCDE, 1)2)3)4)5) ë“±)
   - has_explanations: í•´ì„¤ í¬í•¨ ì—¬ë¶€
4. metadata: ë©”íƒ€ë°ì´í„°
   - subject: ê³¼ëª©ëª… (ì¶”ì •)
   - exam_year: ì‹œí—˜ë…„ë„ (ì¶”ì •)
   - exam_session: íšŒì°¨ (ì¶”ì •)
   - difficulty_level: ë‚œì´ë„ ì¶”ì • (beginner, intermediate, advanced)

JSON í˜•íƒœë¡œë§Œ ë‹µë³€í•˜ê³ , ì¶”ê°€ ì„¤ëª…ì€ í•˜ì§€ ë§ˆì„¸ìš”.
            """.strip()
            
            response = await self.openai_client.chat.completions.create(
                model="gpt-4o",
                messages=[
                    {
                        "role": "user",
                        "content": [
                            {"type": "text", "text": prompt},
                            {
                                "type": "image_url",
                                "image_url": {
                                    "url": f"data:image/png;base64,{base64_image}",
                                    "detail": "high"
                                }
                            }
                        ]
                    }
                ],
                max_tokens=1000,
                temperature=0.1
            )
            
            result_text = response.choices[0].message.content.strip()
            
            # JSON íŒŒì‹± ì‹œë„
            try:
                result = json.loads(result_text)
            except json.JSONDecodeError:
                # JSON íŒŒì‹± ì‹¤íŒ¨ ì‹œ ê¸°ë³¸ê°’ ë°˜í™˜
                result = {
                    'document_type': 'unknown',
                    'layout_info': {'column_count': 1, 'text_density': 'medium'},
                    'content_structure': {'question_format': 'unknown'},
                    'metadata': {'subject': 'unknown'}
                }
            
            result['api_key_used'] = 'gpt-4o'
            result['tokens_used'] = response.usage.total_tokens
            result['cost'] = response.usage.total_tokens * 0.00001  # GPT-4O ëŒ€ëµ ë¹„ìš©
            
            return result
            
        except Exception as e:
            logger.error(f"GPT ë¬¸ì„œ ë¶„ì„ ì˜¤ë¥˜: {str(e)}")
            return {
                'document_type': 'unknown',
                'layout_info': {},
                'content_structure': {},
                'metadata': {},
                'error': str(e)
            }
    
    async def _analyze_combined_document_with_gpt(self, image_path: str) -> Dict[str, Any]:
        """GPT Visionìœ¼ë¡œ í†µí•©ëœ ê¸´ ì´ë¯¸ì§€ ë¶„ì„"""
        try:
            print(f"GPT Analysis - OpenAI client status: {self.openai_client is not None}")
            if not self.openai_client:
                print("ERROR: OpenAI API key not set - returning basic analysis")
                return {
                    'document_type': 'unknown',
                    'layout_info': {},
                    'content_structure': {},
                    'metadata': {},
                    'error': 'OpenAI API key not set'
                }
            
            # ì´ë¯¸ì§€ë¥¼ base64ë¡œ ì¸ì½”ë”©
            with open(image_path, "rb") as image_file:
                base64_image = base64.b64encode(image_file.read()).decode('utf-8')
            
            prompt = f"""
ì´ ì´ë¯¸ì§€ëŠ” ì—¬ëŸ¬ í˜ì´ì§€ì˜ PDFë¥¼ ì„¸ë¡œë¡œ ì—°ê²°í•œ ê¸´ ë¬¸ì„œ ì´ë¯¸ì§€ì…ë‹ˆë‹¤.
ì „ì²´ ë¬¸ì„œë¥¼ ë¶„ì„í•´ì„œ ë‹¤ìŒ ì •ë³´ë“¤ì„ JSON í˜•íƒœë¡œ ë‹µë³€í•´ì£¼ì„¸ìš”:

1. document_type: ë¬¸ì„œ ì¢…ë¥˜ (ê¸°ì¶œë¬¸ì œì§‘, í•´ì„¤ì§‘, ìš”ì•½ì„œ, ì •ë‹µí‘œ, ìˆ˜í—˜ì•ˆë‚´ì„œ ë“±)

2. layout_info: ì „ì²´ ë ˆì´ì•„ì›ƒ ì •ë³´
   - column_count: ì£¼ìš” ë‹¨ ìˆ˜ (1ë‹¨, 2ë‹¨, 3ë‹¨ ë“±)
   - has_header_footer: ë¨¸ë¦¬ê¸€/ë°”ë‹¥ê¸€ ì¡´ì¬ ì—¬ë¶€
   - text_density: ì „ì²´ì ì¸ í…ìŠ¤íŠ¸ ë°€ë„ (high, medium, low)
   - page_breaks: í˜ì´ì§€ êµ¬ë¶„ì„ ì´ë‚˜ ê²½ê³„ê°€ ë³´ì´ëŠ” ìœ„ì¹˜ë“¤

3. content_structure: ë¬¸ì œ êµ¬ì¡° ë¶„ì„
   - question_format: ë¬¸ì œ í˜•ì‹ (ê°ê´€ì‹, ì£¼ê´€ì‹, í˜¼í•© ë“±)
   - numbering_style: ë¬¸ì œ ë²ˆí˜¸ ìŠ¤íƒ€ì¼ (1., 1), â‘ , (1) ë“±)
   - answer_choices: ì„ íƒì§€ í˜•ì‹ (â‘ â‘¡â‘¢â‘£â‘¤, ABCDE, 1)2)3)4)5) ë“±)
   - has_explanations: í•´ì„¤ í¬í•¨ ì—¬ë¶€
   - question_count: ëŒ€ëµì ì¸ ë¬¸ì œ ê°œìˆ˜
   - question_regions: ë¬¸ì œê°€ ì‹œì‘ë˜ëŠ” ì£¼ìš” ìœ„ì¹˜ë“¤ (ìƒë‹¨ë¶€í„°ì˜ ëŒ€ëµì  ë¹„ìœ¨ë¡œ í‘œí˜„, ì˜ˆ: [0.1, 0.3, 0.5])

4. preprocessing_suggestions: ì „ì²˜ë¦¬ ì œì•ˆì‚¬í•­
   - brightness_adjustment: ë°ê¸° ì¡°ì • í•„ìš” ì—¬ë¶€ (true/false)
   - contrast_enhancement: ëŒ€ë¹„ ê°•í™” í•„ìš” ì—¬ë¶€ (true/false)  
   - noise_reduction: ë…¸ì´ì¦ˆ ì œê±° í•„ìš” ì—¬ë¶€ (true/false)
   - deskew_needed: ê¸°ìš¸ê¸° ë³´ì • í•„ìš” ì—¬ë¶€ (true/false)
   - crop_margins: ì—¬ë°± í¬ë¡­ í•„ìš” ì—¬ë¶€ (true/false)

5. metadata: ë©”íƒ€ë°ì´í„° ì¶”ì •
   - subject: ê³¼ëª©ëª… (ì¶”ì •)
   - exam_year: ì‹œí—˜ë…„ë„ (ì¶”ì •)
   - exam_session: íšŒì°¨ (ì¶”ì •)
   - difficulty_level: ë‚œì´ë„ ì¶”ì • (beginner, intermediate, advanced)

6. ocr_suggestions: OCR ìµœì í™” ì œì•ˆ
   - recommended_language: ì¶”ì²œ ì–¸ì–´ ì„¤ì • (ko+en, ko, en ë“±)
   - text_size: í…ìŠ¤íŠ¸ í¬ê¸° ì¶”ì • (small, medium, large)
   - special_characters: íŠ¹ìˆ˜ ë¬¸ìë‚˜ ìˆ˜ì‹ í¬í•¨ ì—¬ë¶€

JSON í˜•íƒœë¡œë§Œ ë‹µë³€í•˜ê³ , ì¶”ê°€ ì„¤ëª…ì€ í•˜ì§€ ë§ˆì„¸ìš”.
            """.strip()
            
            response = await self.openai_client.chat.completions.create(
                model="gpt-4o",
                messages=[
                    {
                        "role": "user", 
                        "content": [
                            {"type": "text", "text": prompt},
                            {
                                "type": "image_url",
                                "image_url": {
                                    "url": f"data:image/png;base64,{base64_image}",
                                    "detail": "high"
                                }
                            }
                        ]
                    }
                ],
                max_tokens=2000,
                temperature=0.1
            )
            
            result_text = response.choices[0].message.content.strip()
            
            # JSON íŒŒì‹± ì‹œë„
            try:
                result = json.loads(result_text)
            except json.JSONDecodeError:
                # JSON íŒŒì‹± ì‹¤íŒ¨ ì‹œ ê¸°ë³¸ê°’ ë°˜í™˜
                result = {
                    'document_type': 'unknown',
                    'layout_info': {'column_count': 1, 'text_density': 'medium'},
                    'content_structure': {'question_format': 'unknown', 'question_count': 0},
                    'preprocessing_suggestions': {},
                    'metadata': {'subject': 'unknown'},
                    'ocr_suggestions': {}
                }
            
            result['api_key_used'] = 'gpt-4o'
            result['tokens_used'] = response.usage.total_tokens
            result['cost'] = response.usage.total_tokens * 0.00001  # GPT-4O ëŒ€ëµ ë¹„ìš©
            
            return result
            
        except Exception as e:
            logger.error(f"GPT combined document analysis error: {str(e)}")
            return {
                'document_type': 'unknown',
                'layout_info': {},
                'content_structure': {},
                'preprocessing_suggestions': {},
                'metadata': {},
                'ocr_suggestions': {},
                'error': str(e)
            }
    
    async def _execute_preprocessing_on_combined(self, doc_analysis: Dict, db: Session) -> Dict[str, Any]:
        """2ë‹¨ê³„: í†µí•© ì´ë¯¸ì§€ì— ëŒ€í•œ ì „ì²˜ë¦¬ ì‹¤í–‰"""
        try:
            upload_id = doc_analysis.get('upload_id') or doc_analysis['analysis_result'].get('upload_id')
            await self._log_processing_step(upload_id, "image_preprocessing", "processing", {}, db)
            
            print("Starting image preprocessing with OpenCV...")
            
            image_path = doc_analysis['image_path']
            analysis_result = doc_analysis['analysis_result']
            preprocessing_suggestions = analysis_result.get('preprocessing_suggestions', {})
            
            # OpenCVë¡œ ì´ë¯¸ì§€ ì½ê¸°
            image = cv2.imread(image_path)
            if image is None:
                raise Exception(f"Failed to load image: {image_path}")
            
            print(f"Original image shape: {image.shape}")
            applied_operations = []
            
            # 1. ë°ê¸° ì¡°ì •
            if preprocessing_suggestions.get('brightness_adjustment', False):
                print("Applying brightness adjustment...")
                # íˆìŠ¤í† ê·¸ë¨ í‰ê· ì„ ê¸°ì¤€ìœ¼ë¡œ ë°ê¸° ì¡°ì •
                gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)
                mean_brightness = np.mean(gray)
                
                if mean_brightness < 100:  # ë„ˆë¬´ ì–´ë‘ì›€
                    brightness_factor = 130 - mean_brightness
                    image = cv2.convertScaleAbs(image, alpha=1.0, beta=brightness_factor)
                    applied_operations.append(f"brightness_adjustment:+{brightness_factor}")
                elif mean_brightness > 200:  # ë„ˆë¬´ ë°ìŒ
                    brightness_factor = mean_brightness - 180
                    image = cv2.convertScaleAbs(image, alpha=1.0, beta=-brightness_factor)
                    applied_operations.append(f"brightness_adjustment:-{brightness_factor}")
            
            # 2. ëŒ€ë¹„ ê°•í™”
            if preprocessing_suggestions.get('contrast_enhancement', False):
                print("Applying contrast enhancement...")
                # CLAHE (Contrast Limited Adaptive Histogram Equalization) ì ìš©
                lab = cv2.cvtColor(image, cv2.COLOR_BGR2LAB)
                clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8, 8))
                lab[:, :, 0] = clahe.apply(lab[:, :, 0])
                image = cv2.cvtColor(lab, cv2.COLOR_LAB2BGR)
                applied_operations.append("contrast_enhancement:CLAHE")
            
            # 3. ë…¸ì´ì¦ˆ ì œê±°
            if preprocessing_suggestions.get('noise_reduction', False):
                print("Applying noise reduction...")
                # Non-local means denoising
                image = cv2.fastNlMeansDenoisingColored(image, None, 10, 10, 7, 21)
                applied_operations.append("noise_reduction:fastNlMeans")
            
            # 4. ê¸°ìš¸ê¸° ë³´ì •
            if preprocessing_suggestions.get('deskew_needed', False):
                print("Applying deskew correction...")
                gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)
                
                # Hough Line Transformìœ¼ë¡œ ê¸°ìš¸ê¸° ì°¾ê¸°
                edges = cv2.Canny(gray, 50, 150, apertureSize=3)
                lines = cv2.HoughLines(edges, 1, np.pi/180, threshold=200)
                
                if lines is not None:
                    angles = []
                    for rho, theta in lines[:20]:  # ìƒìœ„ 20ê°œ ì„ ë¶„ë§Œ ì‚¬ìš©
                        angle = theta * 180 / np.pi - 90
                        if abs(angle) < 45:  # 45ë„ ì´ë‚´ì˜ ê°ë„ë§Œ ê³ ë ¤
                            angles.append(angle)
                    
                    if angles:
                        median_angle = np.median(angles)
                        if abs(median_angle) > 0.5:  # 0.5ë„ ì´ìƒ ê¸°ìš¸ì–´ì§„ ê²½ìš°ë§Œ ë³´ì •
                            center = (image.shape[1] // 2, image.shape[0] // 2)
                            rotation_matrix = cv2.getRotationMatrix2D(center, -median_angle, 1.0)
                            image = cv2.warpAffine(image, rotation_matrix, (image.shape[1], image.shape[0]), 
                                                 flags=cv2.INTER_CUBIC, borderMode=cv2.BORDER_REPLICATE)
                            applied_operations.append(f"deskew_correction:{median_angle:.2f}degrees")
            
            # 5. ì—¬ë°± í¬ë¡­
            if preprocessing_suggestions.get('crop_margins', False):
                print("Applying margin cropping...")
                gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)
                
                # ì´ì§„í™”
                _, thresh = cv2.threshold(gray, 0, 255, cv2.THRESH_BINARY_INV + cv2.THRESH_OTSU)
                
                # ì»¨í…ì¸  ì˜ì—­ ì°¾ê¸°
                coords = np.column_stack(np.where(thresh > 0))
                if len(coords) > 0:
                    y_min, x_min = coords.min(axis=0)
                    y_max, x_max = coords.max(axis=0)
                    
                    # ì•½ê°„ì˜ íŒ¨ë”© ì¶”ê°€
                    padding = 20
                    y_min = max(0, y_min - padding)
                    x_min = max(0, x_min - padding)
                    y_max = min(image.shape[0], y_max + padding)
                    x_max = min(image.shape[1], x_max + padding)
                    
                    # í¬ë¡­
                    image = image[y_min:y_max, x_min:x_max]
                    applied_operations.append(f"margin_crop:({x_min},{y_min},{x_max},{y_max})")
            
            # 6. OCRì„ ìœ„í•œ ì „ìš© ì „ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸
            print("Applying OCR-optimized preprocessing...")
            
            # ê·¸ë ˆì´ìŠ¤ì¼€ì¼ ë³€í™˜
            if len(image.shape) == 3:
                gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)
            else:
                gray = image.copy()
            
            # ë…¸ì´ì¦ˆ ì œê±° (ë” ê°•ë ¥í•œ í•„í„°)
            gray = cv2.bilateralFilter(gray, 9, 75, 75)
            
            # ì ì‘í˜• ì´ì§„í™” (í…ìŠ¤íŠ¸ ì¶”ì¶œì— ìµœì )
            binary = cv2.adaptiveThreshold(gray, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C, 
                                          cv2.THRESH_BINARY, 15, 4)
            
            # ëª¨í´ë¡œì§€ ì—°ì‚°ìœ¼ë¡œ í…ìŠ¤íŠ¸ ê°œì„ 
            kernel = np.ones((1,1), np.uint8)
            binary = cv2.morphologyEx(binary, cv2.MORPH_CLOSE, kernel)
            
            # ìµœì¢…ì ìœ¼ë¡œ 3ì±„ë„ë¡œ ë³€í™˜ (EasyOCR ìš”êµ¬ì‚¬í•­)
            image = cv2.cvtColor(binary, cv2.COLOR_GRAY2BGR)
            
            applied_operations.append("OCR-optimized:bilateral_filter+adaptive_threshold+morphology")
            
            # ì „ì²˜ë¦¬ëœ ì´ë¯¸ì§€ ì €ì¥
            preprocessed_path = self.temp_dir / f"preprocessed_{doc_analysis['analysis_result'].get('upload_id')}.png"
            cv2.imwrite(str(preprocessed_path), image)
            
            print(f"Preprocessed image shape: {image.shape}")
            print(f"Applied operations: {applied_operations}")
            
            await self._log_processing_step(doc_analysis['analysis_result'].get('upload_id'), 
                                          "image_preprocessing", "completed", {
                'operations_applied': applied_operations,
                'original_size': f"{cv2.imread(image_path).shape}",
                'preprocessed_size': f"{image.shape}",
                'preprocessed_path': str(preprocessed_path)
            }, db)
            
            return {
                'success': True,
                'preprocessed_image_path': str(preprocessed_path),
                'operations_applied': applied_operations,
                'original_path': image_path
            }
            
        except Exception as e:
            logger.error(f"Image preprocessing error: {str(e)}")
            await self._log_processing_step(doc_analysis['analysis_result'].get('upload_id'), 
                                          "image_preprocessing", "failed", {"error": str(e)}, db)
            return {'success': False, 'error': str(e)}
    
    async def _process_ocr_on_combined(self, preprocessing_result: Dict, db: Session) -> Dict[str, Any]:
        """3ë‹¨ê³„: í†µí•© ì´ë¯¸ì§€ì— ëŒ€í•œ OCR ì²˜ë¦¬"""
        try:
            upload_id = preprocessing_result.get('upload_id', 'unknown')
            await self._log_processing_step(upload_id, "ocr_processing", "processing", {}, db)
            
            print("Starting OCR processing...")
            
            image_path = preprocessing_result['preprocessed_image_path']
            
            # EasyOCR ì‚¬ìš© (í•œêµ­ì–´ + ì˜ì–´) - ìµœì í™”ëœ ì„¤ì •
            import easyocr
            reader = easyocr.Reader(['ko', 'en'], gpu=False, verbose=False)  
            
            print("Performing EasyOCR text extraction with optimized settings...")
            
            # OCR ì‹¤í–‰ (í•œêµ­ì–´ ë¬¸ì„œì— ìµœì í™”ëœ íŒŒë¼ë¯¸í„° - ì•ˆì •ì ì¸ ê¸°ë³¸ ëª¨ë“œ)
            results = reader.readtext(image_path, 
                                    detail=1,
                                    paragraph=False,  # ê¸°ë³¸ ë¼ì¸ ë‹¨ìœ„ ì¸ì‹ (ë” ì•ˆì •ì )
                                    width_ths=0.5,   # í…ìŠ¤íŠ¸ ë¼ì¸ ë„ˆë¹„ ì„ê³„ê°’
                                    height_ths=0.5,  # í…ìŠ¤íŠ¸ ë¼ì¸ ë†’ì´ ì„ê³„ê°’  
                                    text_threshold=0.3,  # í…ìŠ¤íŠ¸ ì‹ ë¢°ë„ ì„ê³„ê°’
                                    low_text=0.2,    # ì €í’ˆì§ˆ í…ìŠ¤íŠ¸ë„ í¬í•¨
                                    link_threshold=0.2)  # í…ìŠ¤íŠ¸ ì—°ê²° ì„ê³„ê°’
            
            # ê²°ê³¼ ì •ë¦¬
            extracted_text_blocks = []
            full_text = ""
            total_confidence = 0
            
            for (bbox, text, confidence) in results:
                if confidence > 0.1:  # ì‹ ë¢°ë„ 10% ì´ìƒë§Œ ì‚¬ìš© (ë” ë§ì€ í…ìŠ¤íŠ¸ í¬í•¨)
                    # bbox ì •ë³´ ì •ë¦¬
                    x_coords = [point[0] for point in bbox]
                    y_coords = [point[1] for point in bbox]
                    
                    text_block = {
                        'text': text.strip(),
                        'confidence': confidence,
                        'bbox': {
                            'x_min': min(x_coords),
                            'y_min': min(y_coords),
                            'x_max': max(x_coords),
                            'y_max': max(y_coords)
                        }
                    }
                    
                    extracted_text_blocks.append(text_block)
                    full_text += text + " "
                    total_confidence += confidence
            
            # Y ì¢Œí‘œ ê¸°ì¤€ìœ¼ë¡œ ì •ë ¬ (ìœ„ì—ì„œ ì•„ë˜ë¡œ)
            extracted_text_blocks.sort(key=lambda x: x['bbox']['y_min'])
            
            # í‰ê·  ì‹ ë¢°ë„ ê³„ì‚°
            average_confidence = total_confidence / len(extracted_text_blocks) if extracted_text_blocks else 0
            
            # ì¤„ë°”ê¿ˆìœ¼ë¡œ êµ¬ë¶„ëœ ì „ì²´ í…ìŠ¤íŠ¸ ìƒì„±
            organized_text = ""
            current_y = 0
            line_threshold = 30  # ê°™ì€ ì¤„ë¡œ ê°„ì£¼í•  Y ì¢Œí‘œ ì°¨ì´
            
            for block in extracted_text_blocks:
                block_y = block['bbox']['y_min']
                
                # ìƒˆë¡œìš´ ì¤„ì¸ì§€ íŒë‹¨
                if abs(block_y - current_y) > line_threshold:
                    organized_text += "\n" + block['text']
                    current_y = block_y
                else:
                    organized_text += " " + block['text']
            
            print(f"OCR completed: {len(extracted_text_blocks)} text blocks extracted")
            print(f"Average confidence: {average_confidence:.2f}")
            print(f"Total characters: {len(organized_text)}")
            print(f"=== ì‹¤ì œ ì¶”ì¶œëœ í…ìŠ¤íŠ¸ (ì²˜ìŒ 500ì) ===")
            print(f"'{organized_text[:500]}...'")
            print("=== ì¶”ì¶œëœ í…ìŠ¤íŠ¸ ë ===")
            
            # ì¶”ê°€ë¡œ Tesseract OCRë„ ì‹œë„ (ë¹„êµìš©)
            tesseract_text = ""
            try:
                import pytesseract
                # Tesseract ì„¤ì • (í•œêµ­ì–´ + ì˜ì–´)
                config = '--oem 3 --psm 6 -l kor+eng'
                tesseract_text = pytesseract.image_to_string(image_path, config=config)
                print(f"Tesseract also extracted {len(tesseract_text)} characters")
            except Exception as tesseract_error:
                print(f"Tesseract OCR failed: {tesseract_error}")
            
            # ê²°ê³¼ ì €ì¥
            await self._log_processing_step(upload_id, "ocr_processing", "completed", {
                'text_blocks_count': len(extracted_text_blocks),
                'average_confidence': average_confidence,
                'total_characters': len(organized_text),
                'ocr_engines_used': ['easyocr', 'tesseract'] if tesseract_text else ['easyocr']
            }, db)
            
            return {
                'success': True,
                'ocr_method': 'easyocr',
                'extracted_text_blocks': extracted_text_blocks,
                'organized_text': organized_text.strip(),
                'tesseract_text': tesseract_text,
                'average_confidence': average_confidence,
                'total_text_blocks': len(extracted_text_blocks),
                'preprocessed_image_path': image_path
            }
            
        except Exception as e:
            logger.error(f"OCR processing error: {str(e)}")
            await self._log_processing_step(upload_id, "ocr_processing", "failed", {"error": str(e)}, db)
            return {'success': False, 'error': str(e)}

    async def _analyze_pages_with_gpt(self, doc_analysis_id: int, pdf_path: str, db: Session) -> Dict[str, Any]:
        """2ë‹¨ê³„: í˜ì´ì§€ë³„ GPT ë¶„ì„"""
        try:
            # êµ¬í˜„ ì˜ˆì • - í˜ì´ì§€ë³„ë¡œ ì¸ë„¤ì¼ ìƒì„±í•˜ê³  GPTë¡œ ë¶„ì„
            # ê° í˜ì´ì§€ì˜ ë ˆì´ì•„ì›ƒ, ë¬¸ì œ ì˜ì—­, ì „ì²˜ë¦¬ ë ˆì‹œí”¼ ìƒì„±
            pass
        except Exception as e:
            return {'success': False, 'error': str(e)}
    
    async def _execute_preprocessing(self, page_analyses: List[Dict], db: Session) -> Dict[str, Any]:
        """3ë‹¨ê³„: ì „ì²˜ë¦¬ ì‹¤í–‰"""
        try:
            # êµ¬í˜„ ì˜ˆì • - OpenCV ë“±ì„ ì‚¬ìš©í•œ ì´ë¯¸ì§€ ì „ì²˜ë¦¬
            pass
        except Exception as e:
            return {'success': False, 'error': str(e)}
    
    async def _process_ocr(self, preprocessing_results: List[Dict], db: Session) -> Dict[str, Any]:
        """4ë‹¨ê³„: OCR ì²˜ë¦¬"""
        try:
            # êµ¬í˜„ ì˜ˆì • - Tesseract, PaddleOCR ë“±ì„ ì‚¬ìš©í•œ OCR
            pass
        except Exception as e:
            return {'success': False, 'error': str(e)}
    
    async def _postprocess_and_structure(self, upload_id: int, ocr_result: Dict, db: Session) -> Dict[str, Any]:
        """4ë‹¨ê³„: í›„ì²˜ë¦¬ ë° êµ¬ì¡°í™” - Claudeë¡œ ê³ ê¸‰ êµìœ¡ ì½˜í…ì¸  ë¶„ë¥˜"""
        try:
            await self._log_processing_step(upload_id, "advanced_classification", "processing", {}, db)
            
            print("Starting advanced text classification with Claude...")
            
            if not self.claude_client:
                print("ERROR: Claude client not available for advanced classification")
                return {
                    'questions': [],
                    'materials': [],
                    'chapters': [],
                    'summary': {'error': 'Claude API not available'}
                }
            
            organized_text = ocr_result.get('organized_text', '')
            text_blocks = ocr_result.get('extracted_text_blocks', [])
            
            if not organized_text:
                print("No text extracted from OCR")
                return {
                    'questions': [],
                    'materials': [],
                    'chapters': [],
                    'summary': {'error': 'No text extracted'}
                }
            
            print(f"Processing {len(organized_text)} characters of extracted text with Claude...")
            print(f"=== Claudeì—ê²Œ ì „ë‹¬í•˜ëŠ” í…ìŠ¤íŠ¸ ===")
            print(f"'{organized_text[:1000]}...'")
            print("=== ì „ë‹¬ í…ìŠ¤íŠ¸ ë ===")
            
            # Claudeë¡œ ê³ ê¸‰ êµìœ¡ ì½˜í…ì¸  ë¶„ë¥˜ ë° êµ¬ì¡°í™”
            structuring_prompt = f"""
ë‹¤ìŒì€ PDF ë¬¸ì„œì—ì„œ OCRë¡œ ì¶”ì¶œí•œ êµìœ¡/ì‹œí—˜ ê´€ë ¨ í…ìŠ¤íŠ¸ì…ë‹ˆë‹¤. ì´ í…ìŠ¤íŠ¸ë¥¼ ì •ë°€í•˜ê²Œ ë¶„ì„í•˜ì—¬ êµìœ¡ ì½˜í…ì¸ ë¡œ êµ¬ì¡°í™”í•´ì£¼ì„¸ìš”.

ì¶”ì¶œëœ í…ìŠ¤íŠ¸:
{organized_text[:15000]}

ë‹¤ìŒ JSON í˜•íƒœë¡œ ì •í™•íˆ ë¶„ì„í•´ì£¼ì„¸ìš”:

{{
  "document_analysis": {{
    "title": "ë¬¸ì„œ ì œëª©",
    "subject": "ê³¼ëª©ëª…", 
    "exam_type": "ê¸°ì¶œë¬¸ì œ/í•´ì„¤ì§‘/ìš”ì•½ì„œ/êµì¬/ë¬¸ì œì§‘",
    "academic_level": "ì´ˆë“±/ì¤‘ë“±/ê³ ë“±/ëŒ€í•™/ìê²©ì¦",
    "total_pages_analyzed": "ë¶„ì„ëœ í˜ì´ì§€ ìˆ˜"
  }},
  
  "chapters": [
    {{
      "chapter_number": 1,
      "chapter_title": "ë‹¨ì›/ì±•í„° ì œëª©",
      "main_topics": ["ì£¼ìš” ê°œë…1", "ì£¼ìš” ê°œë…2"],
      "learning_objectives": ["í•™ìŠµëª©í‘œ1", "í•™ìŠµëª©í‘œ2"],
      "page_range": "ì‹œì‘í˜ì´ì§€-ëí˜ì´ì§€"
    }}
  ],
  
  "questions": [
    {{
      "question_id": "Q001",
      "chapter_id": 1,
      "question_number": 1,
      "question_text": "ë¬¸ì œ ë‚´ìš©",
      "question_type": "ê°ê´€ì‹/ì£¼ê´€ì‹/ì„œìˆ í˜•/ê³„ì‚°í˜•",
      "options": ["ì„ íƒì§€1", "ì„ íƒì§€2", "ì„ íƒì§€3", "ì„ íƒì§€4", "ì„ íƒì§€5"],
      "correct_answer": "ì •ë‹µ",
      "explanation": "ìƒì„¸ í•´ì„¤",
      "difficulty_level": "ìƒ/ì¤‘/í•˜",
      "bloom_taxonomy": "ì§€ì‹/ì´í•´/ì ìš©/ë¶„ì„/ì¢…í•©/í‰ê°€",
      "topic_tags": ["ì„¸ë¶€ì£¼ì œ1", "ì„¸ë¶€ì£¼ì œ2"],
      "estimated_time": "í’€ì´ ì˜ˆìƒ ì†Œìš”ì‹œê°„(ë¶„)",
      "keywords": ["í•µì‹¬í‚¤ì›Œë“œ1", "í•µì‹¬í‚¤ì›Œë“œ2"]
    }}
  ],
  
  "study_materials": [
    {{
      "material_id": "M001", 
      "chapter_id": 1,
      "material_type": "ê°œë…ì„¤ëª…/ê³µì‹/ì •ë¦¬/ì˜ˆì œ/ìš”ì•½/ë„í‘œ/ê·¸ë˜í”„",
      "title": "í•™ìŠµìë£Œ ì œëª©",
      "content": "ìƒì„¸ ë‚´ìš©",
      "importance_level": "í•µì‹¬/ì¤‘ìš”/ì°¸ê³ ",
      "related_questions": ["Q001", "Q002"],
      "prerequisites": ["ì„ ìˆ˜í•™ìŠµìš”ì†Œ1", "ì„ ìˆ˜í•™ìŠµìš”ì†Œ2"]
    }}
  ],
  
  "learning_analytics": {{
    "total_questions": 0,
    "question_distribution": {{
      "ê°ê´€ì‹": 0, "ì£¼ê´€ì‹": 0, "ì„œìˆ í˜•": 0, "ê³„ì‚°í˜•": 0
    }},
    "difficulty_distribution": {{
      "ìƒ": 0, "ì¤‘": 0, "í•˜": 0
    }},
    "bloom_taxonomy_distribution": {{
      "ì§€ì‹": 0, "ì´í•´": 0, "ì ìš©": 0, "ë¶„ì„": 0, "ì¢…í•©": 0, "í‰ê°€": 0
    }},
    "chapter_coverage": "ì±•í„°ë³„ ë¬¸ì œ/ìë£Œ ë¶„í¬",
    "main_topics": ["ì „ì²´ ì£¼ìš” ì£¼ì œë“¤"],
    "estimated_study_time": "ì „ì²´ í•™ìŠµ ì˜ˆìƒ ì†Œìš”ì‹œê°„(ì‹œê°„)"
  }}
}}

ë°˜ë“œì‹œ JSON í˜•íƒœë¡œë§Œ ë‹µë³€í•˜ê³ , ì¶”ê°€ ì„¤ëª…ì€ í•˜ì§€ ë§ˆì„¸ìš”. ëª¨ë“  ë¶„ë¥˜ëŠ” êµìœ¡í•™ì  ê´€ì ì—ì„œ ì •í™•í•˜ê²Œ í•´ì£¼ì„¸ìš”.
            """.strip()
            
            print("Sending text to Claude for advanced classification...")
            
            response = await self.claude_client.messages.create(
                model="claude-3-5-sonnet-20241022",
                max_tokens=8000,
                temperature=0.1,
                messages=[
                    {
                        "role": "user",
                        "content": structuring_prompt
                    }
                ]
            )
            
            result_text = response.content[0].text.strip()
            print(f"Claude response length: {len(result_text)} characters")
            
            # JSON íŒŒì‹±
            try:
                structured_result = json.loads(result_text)
            except json.JSONDecodeError as json_error:
                print(f"JSON parsing failed: {json_error}")
                print(f"Raw response: {result_text[:500]}...")
                
                # JSON íŒŒì‹± ì‹¤íŒ¨ ì‹œ ê¸°ë³¸ êµ¬ì¡° ë°˜í™˜
                structured_result = {
                    'document_analysis': {
                        'title': 'Unknown Document',
                        'subject': 'Unknown',
                        'exam_type': 'Unknown',
                        'academic_level': 'Unknown',
                        'total_pages_analyzed': 0
                    },
                    'chapters': [],
                    'questions': [],
                    'study_materials': [],
                    'learning_analytics': {
                        'total_questions': 0,
                        'parsing_error': str(json_error)
                    }
                }
            
            # ìƒˆë¡œìš´ ê³ ê¸‰ ë¶„ë¥˜ ê²°ê³¼ ì²˜ë¦¬
            document_analysis = structured_result.get('document_analysis', {})
            chapters = structured_result.get('chapters', [])
            questions = structured_result.get('questions', [])
            materials = structured_result.get('study_materials', [])
            analytics = structured_result.get('learning_analytics', {})
            
            print(f"Claude advanced classification result:")
            print(f"  - {len(chapters)} chapters identified")
            print(f"  - {len(questions)} questions extracted")
            print(f"  - {len(materials)} learning materials created")
            
            # ê³ ê¸‰ ë°ì´í„°ë² ì´ìŠ¤ ì €ì¥ - ì±•í„° ì •ë³´ ë¨¼ì € ì €ì¥
            chapter_ids = {}
            if chapters:
                for chapter in chapters:
                    try:
                        result = db.execute(text("""
                            INSERT INTO chapters
                            (pdf_upload_id, chapter_number, title, main_topics, learning_objectives, page_range)
                            VALUES (:upload_id, :chapter_number, :title, :main_topics, :learning_objectives, :page_range)
                        """), {
                            'upload_id': upload_id,
                            'chapter_number': chapter.get('chapter_number', 0),
                            'title': chapter.get('chapter_title', ''),
                            'main_topics': json.dumps(chapter.get('main_topics', [])),
                            'learning_objectives': json.dumps(chapter.get('learning_objectives', [])),
                            'page_range': chapter.get('page_range', '')
                        })
                        chapter_ids[chapter.get('chapter_number', 0)] = result.lastrowid
                        print(f"Saved chapter: {chapter.get('chapter_title', 'Untitled')}")
                    except Exception as db_error:
                        print(f"Failed to save chapter to DB: {db_error}")
            
            # ê³ ê¸‰ ë¬¸ì œ ì €ì¥ (Claude ë¶„ë¥˜ ê¸°ì¤€)
            if questions:
                for question in questions:
                    try:
                        chapter_id = chapter_ids.get(question.get('chapter_id', 0))
                        db.execute(text("""
                            INSERT INTO extracted_questions 
                            (pdf_upload_id, chapter_id, question_id, question_text, question_type, 
                             difficulty_level, correct_answer, options, explanation, bloom_taxonomy,
                             topic_tags, estimated_time, keywords)
                            VALUES (:upload_id, :chapter_id, :question_id, :question_text, :question_type,
                                    :difficulty, :correct_answer, :options, :explanation, :bloom_taxonomy,
                                    :topic_tags, :estimated_time, :keywords)
                        """), {
                            'upload_id': upload_id,
                            'chapter_id': chapter_id,
                            'question_id': question.get('question_id', ''),
                            'question_text': question.get('question_text', ''),
                            'question_type': question.get('question_type', ''),
                            'difficulty': question.get('difficulty_level', ''),
                            'correct_answer': question.get('correct_answer', ''),
                            'options': json.dumps(question.get('options', [])),
                            'explanation': question.get('explanation', ''),
                            'bloom_taxonomy': question.get('bloom_taxonomy', ''),
                            'topic_tags': json.dumps(question.get('topic_tags', [])),
                            'estimated_time': question.get('estimated_time', ''),
                            'keywords': json.dumps(question.get('keywords', []))
                        })
                        print(f"Saved advanced question: {question.get('question_id', 'Unknown ID')}")
                    except Exception as db_error:
                        print(f"Failed to save question to DB: {db_error}")
            
            # ê³ ê¸‰ í•™ìŠµ ìë£Œ ì €ì¥
            if materials:
                for material in materials:
                    try:
                        # Dummy certificate_id ê°€ì ¸ì˜¤ê¸°
                        dummy_cert = db.execute(text("SELECT id FROM certificates_info LIMIT 1")).fetchone()
                        cert_id = dummy_cert[0] if dummy_cert else 1
                        
                        chapter_id = chapter_ids.get(material.get('chapter_id', 0))
                        db.execute(text("""
                            INSERT INTO study_materials 
                            (certificate_id, pdf_upload_id, chapter_id, material_id, material_type, title, content,
                             importance_level, related_questions, prerequisites)
                            VALUES (:cert_id, :upload_id, :chapter_id, :material_id, :material_type, :title, :content,
                                    :importance_level, :related_questions, :prerequisites)
                        """), {
                            'cert_id': cert_id,
                            'upload_id': upload_id,
                            'chapter_id': chapter_id,
                            'material_id': material.get('material_id', ''),
                            'material_type': material.get('material_type', ''),
                            'title': material.get('title', ''),
                            'content': material.get('content', ''),
                            'importance_level': material.get('importance_level', ''),
                            'related_questions': json.dumps(material.get('related_questions', [])),
                            'prerequisites': json.dumps(material.get('prerequisites', []))
                        })
                        print(f"Saved advanced material: {material.get('title', 'Untitled')}")
                    except Exception as db_error:
                        print(f"Failed to save material to DB: {db_error}")
            
            db.commit()
            
            await self._log_processing_step(upload_id, "advanced_classification", "completed", {
                'chapters_identified': len(chapters),
                'questions_extracted': len(questions),
                'materials_created': len(materials),
                'claude_tokens_used': response.usage.input_tokens + response.usage.output_tokens if hasattr(response, 'usage') else 0,
                'document_analysis': document_analysis,
                'learning_analytics': analytics
            }, db)
            
            return {
                'questions': questions,
                'materials': materials,
                'chapters': chapters,
                'document_analysis': document_analysis,
                'learning_analytics': analytics,
                'tokens_used': response.usage.input_tokens + response.usage.output_tokens if hasattr(response, 'usage') else 0
            }
            
        except Exception as e:
            logger.error(f"Advanced classification error: {str(e)}")
            await self._log_processing_step(upload_id, "advanced_classification", "failed", {"error": str(e)}, db)
            return {
                'questions': [],
                'materials': [],
                'chapters': [],
                'document_analysis': {'error': str(e)},
                'learning_analytics': {'error': str(e)}
            }
    
    async def _log_processing_step(self, upload_id: int, step_name: str, status: str, 
                                 result_data: Dict, db: Session, step_order: int = 0):
        """ì²˜ë¦¬ ë‹¨ê³„ ë¡œê·¸ ê¸°ë¡"""
        try:
            # upload_idê°€ Noneì¸ ê²½ìš° ë¡œê·¸ ê¸°ë¡ì„ ê±´ë„ˆëœ€
            if upload_id is None:
                print(f"Warning: upload_id is None for step {step_name}, skipping log")
                return
                
            now = datetime.now()
            
            if status == "processing":
                db.execute(text("""
                    INSERT INTO processing_steps 
                    (pdf_upload_id, step_name, step_order, status, started_at, result_data)
                    VALUES (:upload_id, :step_name, :step_order, :status, :started_at, :result_data)
                """), {
                    'upload_id': upload_id,
                    'step_name': step_name,
                    'step_order': step_order,
                    'status': status,
                    'started_at': now,
                    'result_data': json.dumps(result_data)
                })
            else:
                db.execute(text("""
                    UPDATE processing_steps 
                    SET status = :status, completed_at = :completed_at, 
                        result_data = :result_data, progress_percent = :progress
                    WHERE pdf_upload_id = :upload_id AND step_name = :step_name
                """), {
                    'upload_id': upload_id,
                    'step_name': step_name,
                    'status': status,
                    'completed_at': now,
                    'result_data': json.dumps(result_data),
                    'progress': 100 if status == "completed" else 0
                })
            
            db.commit()
            
        except Exception as e:
            logger.error(f"ì²˜ë¦¬ ë‹¨ê³„ ë¡œê·¸ ê¸°ë¡ ì˜¤ë¥˜: {str(e)}")    
    async def _post_process_special_content(self, questions, chunk_idx):
        """ğŸ”¥ íŠ¹ìˆ˜ ì½˜í…ì¸  (í‘œ/ê·¸ë¦¼/ì½”ë“œ) í›„ì²˜ë¦¬"""
        enhanced_questions = []
        
        for question in questions:
            passage = question.get('passage', '')
            question_text = question.get('question_text', '')
            combined_text = question_text + ' ' + passage
            
            # í‘œ ì²˜ë¦¬
            if ('|' in passage or 'í‘œ' in combined_text):
                print(f'ğŸ“Š Table detected in Q{question.get("question_number")}')
                question['has_table'] = True
            
            # ì½”ë“œ ì²˜ë¦¬  
            if any(code_word in combined_text.lower() for code_word in 
                   ['class', 'function', 'public', 'int', 'string']):
                print(f'ğŸ’» Code detected in Q{question.get("question_number")}')
                question['has_code'] = True
                
            enhanced_questions.append(question)
        
        return enhanced_questions
    
    async def _detect_and_merge_orphaned_choices(self, all_questions: List[Dict]) -> List[Dict]:
        """ğŸ” ê³ ì•„ ì„ íƒì§€ íƒì§€ ë° ë³‘í•© ì‹œìŠ¤í…œ - ì‹œí—˜ì§€ íë¦„ ê¸°ë°˜ ì²˜ë¦¬"""
        import re
        
        print(f"ğŸ” ê³ ì•„ ì„ íƒì§€ íƒì§€ ì‹œì‘: {len(all_questions)}ê°œ ë¬¸ì œ ë¶„ì„")
        
        # 1. ëª¨ë“  ë¬¸ì œë¥¼ í˜ì´ì§€/ìœ„ì¹˜ ìˆœì„œë¡œ ì •ë ¬ (ì •í™•í•œ íë¦„ íŒŒì•…ì„ ìœ„í•´)
        def get_sort_key(question):
            page = question.get('source_page', 0)
            location = question.get('page_location', '')
            # í˜ì´ì§€ ë²ˆí˜¸ ìš°ì„ , ìœ„ì¹˜ëŠ” ìƒë‹¨/ì¤‘ë‹¨/í•˜ë‹¨ ìˆœ
            location_score = 0
            if 'ìƒë‹¨' in location: location_score = 1
            elif 'ì¤‘ë‹¨' in location: location_score = 2  
            elif 'í•˜ë‹¨' in location: location_score = 3
            return (page, location_score)
        
        sorted_questions = sorted(all_questions, key=get_sort_key)
        
        # 2. ê³ ì•„ ì„ íƒì§€ íŒ¨í„´ íƒì§€ 
        orphaned_indices = []
        incomplete_questions = []
        
        for i, question in enumerate(sorted_questions):
            question_text = question.get('question_text', '').strip()
            options = question.get('options', [])
            passage = question.get('passage', '').strip()
            
            # ğŸ” ê³ ì•„ ì„ íƒì§€ íŒ¨í„´ íƒì§€
            is_orphaned_choices = self._is_orphaned_choice_pattern(question_text, options, passage)
            
            # ğŸ” ë¶ˆì™„ì „í•œ ë¬¸ì œ íƒì§€ (ì„ íƒì§€ê°€ 2ê°œ ì´í•˜ì´ê±°ë‚˜ ì—°ì†ë˜ì§€ ì•ŠëŠ” ê²½ìš°)
            is_incomplete_question = self._is_incomplete_question(options)
            
            if is_orphaned_choices:
                orphaned_indices.append(i)
                print(f"ğŸ” ê³ ì•„ ì„ íƒì§€ ë°œê²¬ #{i}: '{question_text[:50]}...' + {len(options)}ê°œ ì„ íƒì§€")
                
            if is_incomplete_question:
                incomplete_questions.append(i)
                print(f"ğŸ” ë¶ˆì™„ì „í•œ ë¬¸ì œ ë°œê²¬ #{i}: Q{question.get('question_number', 'N/A')} - {len(options)}ê°œ ì„ íƒì§€")
        
        # 3. ê³ ì•„ ì„ íƒì§€ì™€ ë¶ˆì™„ì „í•œ ë¬¸ì œ ë³‘í•© ì‹¤í–‰
        if orphaned_indices and incomplete_questions:
            print(f"ğŸ”— ë³‘í•© ëŒ€ìƒ: {len(incomplete_questions)}ê°œ ë¶ˆì™„ì „í•œ ë¬¸ì œ + {len(orphaned_indices)}ê°œ ê³ ì•„ ì„ íƒì§€")
            sorted_questions = self._merge_orphaned_with_incomplete(sorted_questions, orphaned_indices, incomplete_questions)
        
        # 4. íŠ¹ì • ë¬¸ì œ ë²ˆí˜¸ì˜ ì„ íƒì§€ ë³µêµ¬ (ì˜ˆ: 11ë²ˆ ë¬¸ì œ â‘¢â‘£â‘¤ ë³µêµ¬)
        sorted_questions = self._recover_specific_missing_choices(sorted_questions)
        
        print(f"âœ… ê³ ì•„ ì„ íƒì§€ íƒì§€ ì™„ë£Œ: {len(all_questions)} â†’ {len(sorted_questions)}ê°œ ë¬¸ì œ")
        return sorted_questions
    
    def _is_orphaned_choice_pattern(self, question_text: str, options: List[str], passage: str) -> bool:
        """ğŸ” ê°•í™”ëœ ì„ íƒì§€ë§Œ ìˆê³  ì‹¤ì œ ì§ˆë¬¸ì´ ì—†ëŠ” ê³ ì•„ íŒ¨í„´ íŒë‹¨"""
        import re
        
        # ğŸ” ê°•í™”ëœ ì‹¤ì œ ì§ˆë¬¸ íŒ¨í„´ ì¸ì‹
        question_patterns = [
            r'ë‹¤ìŒ\s*ì¤‘', r'ë¬´ì—‡ì¸ê°€\?', r'ì–´ë–¤\s*ê²ƒ', r'ì˜³ì€\s*ê²ƒ', r'í‹€ë¦°\s*ê²ƒ',
            r'í•´ë‹¹í•˜ëŠ”\s*ê²ƒ', r'ì•„ë‹Œ\s*ê²ƒ', r'ì„¤ëª…ìœ¼ë¡œ', r'ì—\s*ëŒ€í•œ',
            r'\?$', r'êµ¬í•˜ì‹œì˜¤', r'ê³ ë¥´ì‹œì˜¤', r'í•˜ì‹œì˜¤$', r'ì€\?', r'ëŠ”\?',
            r'ë°©ë²•ì€', r'ê²°ê³¼ëŠ”', r'ì´ìœ ëŠ”', r'ê°€ì¥\s*ì ì ˆí•œ',
            r'ì•Œê³ ë¦¬ì¦˜', r'í•¨ìˆ˜', r'ê°’ì„', r'ì˜ˆìƒ\s*ê²°ê³¼',
            r'ë‹¤ìŒ\s*ì½”ë“œ', r'ë‹¤ìŒ\s*í”„ë¡œê·¸ë¨', r'ë‹¤ìŒ\s*í‘œ'
        ]
        
        has_real_question = any(re.search(pattern, question_text, re.IGNORECASE) for pattern in question_patterns)
        
        # ğŸ” ê°•í™”ëœ ì„ íƒì§€ë§Œ ìˆëŠ” íŒ¨í„´ í™•ì¸
        only_choices_patterns = [
            r'^\s*[â‘ â‘¡â‘¢â‘£â‘¤]\s*[^ì„œì‚­ì´ì˜ê·¸ë¦¬ì¶”]',  # ì„ íƒì§€ë¡œ ì‹œì‘í•˜ëŠ” ë‹¨ìˆœ ë‚´ìš©
            r'^[â‘ â‘¡â‘¢â‘£â‘¤]\s*\w+$',  # ì„ íƒì§€ + ë‹¨ì–´ í•˜ë‚˜
            r'^[â‘ â‘¡â‘¢â‘£â‘¤]\s*[A-Za-z0-9]+',  # ì„ íƒì§€ + ì˜ìˆ«ì
            r'^\s*â‘¢\s*[^ì˜ˆìì´ë‹¤]',  # â‘¢ë²ˆë¶€í„° ì‹œì‘ (ì˜ˆì/ì´ë‹¤ ì œì™¸)
            r'^\s*â‘£\s*[^ì˜ˆìì´ë‹¤]',  # â‘£ë²ˆë¶€í„° ì‹œì‘ 
            r'^\s*â‘¤\s*[^ì˜ˆìì´ë‹¤]'   # â‘¤ë²ˆë¶€í„° ì‹œì‘
        ]
        
        looks_like_only_choices = any(re.search(pattern, question_text, re.IGNORECASE) for pattern in only_choices_patterns)
        
        # ğŸ” ì§ˆë¬¸ ë‹¨ë…ì„± ê²€ì‚¬ - ì§ˆë¬¸ì´ ë„ˆë¬´ ì§§ê³  ì˜ë¯¸ê°€ ì—†ëŠ” ê²½ìš°
        is_meaningless_text = (
            len(question_text.strip()) < 25 and 
            not has_real_question and
            not any(word in question_text for word in ['í‘œ', 'ê·¸ë¦¼', 'ì½”ë“œ', 'ìƒí™©', 'ë¬¸ì œ'])
        )
        
        # ğŸ” ì„ íƒì§€ ë²ˆí˜¸ ì—°ì†ì„± ê²€ì‚¬ - 3ë²ˆë¶€í„° ì‹œì‘í•˜ë©´ ê³ ì•„ ì„ íƒì§€ì¼ ê°€ëŠ¥ì„± ë†’ìŒ
        starts_with_late_choices = any(re.search(rf'^\s*{choice}', question_text) for choice in ['â‘¢', 'â‘£', 'â‘¤'])
        
        # ğŸ¯ ê³ ì•„ ì„ íƒì§€ íŒì • ì¡°ê±´ (ê°•í™”ëœ ë²„ì „)
        is_orphaned = (
            len(options) >= 1 and  # ì„ íƒì§€ê°€ 1ê°œ ì´ìƒ ìˆê³ 
            (
                starts_with_late_choices or  # 3,4,5ë²ˆ ì„ íƒì§€ë¡œ ì‹œì‘í•˜ê±°ë‚˜
                (not has_real_question and looks_like_only_choices) or  # ì‹¤ì œ ì§ˆë¬¸ ì—†ì´ ì„ íƒì§€ë§Œ ìˆê±°ë‚˜
                is_meaningless_text  # ì˜ë¯¸ì—†ëŠ” ì§§ì€ í…ìŠ¤íŠ¸ì¸ ê²½ìš°
            )
        )
        
        if is_orphaned:
            print(f"ğŸ” ê³ ì•„ ì„ íƒ­ì§€ íŒ¨í„´ ê°ì§€: '{question_text[:50]}...' | ì„ íƒì§€: {len(options)}ê°œ")
        
        return is_orphaned
    
    def _is_incomplete_question(self, options: List[str]) -> bool:
        """ë¶ˆì™„ì „í•œ ë¬¸ì œì¸ì§€ íŒë‹¨ (ì„ íƒì§€ ê°œìˆ˜ë‚˜ ì—°ì†ì„± ë¶€ì¡±)"""
        if len(options) < 3:
            return True
            
        # ì„ íƒì§€ ë²ˆí˜¸ ì—°ì†ì„± í™•ì¸
        import re
        choice_numbers = []
        for option in options:
            # â‘ â‘¡â‘¢â‘£â‘¤ ë˜ëŠ” 1)2)3)4)5) íŒ¨í„´ì—ì„œ ë²ˆí˜¸ ì¶”ì¶œ
            match = re.search(r'[â‘ â‘¡â‘¢â‘£â‘¤]|(\d+)\)', option)
            if match:
                if 'â‘ ' in option: choice_numbers.append(1)
                elif 'â‘¡' in option: choice_numbers.append(2)
                elif 'â‘¢' in option: choice_numbers.append(3)
                elif 'â‘£' in option: choice_numbers.append(4)
                elif 'â‘¤' in option: choice_numbers.append(5)
                elif match.group(1): choice_numbers.append(int(match.group(1)))
        
        # ì—°ì†ì„± í™•ì¸ - 1ë¶€í„° ì‹œì‘í•´ì„œ ì—°ì†ë˜ì–´ì•¼ í•¨
        if choice_numbers and min(choice_numbers) != 1:
            return True  # 1ë²ˆë¶€í„° ì‹œì‘í•˜ì§€ ì•ŠìŒ
            
        # ì¤‘ê°„ì— ë¹ ì§„ ë²ˆí˜¸ê°€ ìˆëŠ”ì§€ í™•ì¸
        if choice_numbers:
            expected = list(range(1, max(choice_numbers) + 1))
            if sorted(choice_numbers) != expected:
                return True  # ì¤‘ê°„ì— ë¹ ì§„ ë²ˆí˜¸ê°€ ìˆìŒ
        
        return False
    
    def _merge_orphaned_with_incomplete(self, questions: List[Dict], orphaned_indices: List[int], incomplete_indices: List[int]) -> List[Dict]:
        """ê³ ì•„ ì„ íƒì§€ë¥¼ ë¶ˆì™„ì „í•œ ë¬¸ì œì™€ ë³‘í•©"""
        
        merged_questions = []
        used_orphans = set()
        
        for i, question in enumerate(questions):
            if i in orphaned_indices:
                # ê³ ì•„ ì„ íƒì§€ëŠ” ë³„ë„ ì²˜ë¦¬í•˜ë¯€ë¡œ ìŠ¤í‚µ
                continue
                
            if i in incomplete_indices:
                # ë¶ˆì™„ì „í•œ ë¬¸ì œì˜ ê²½ìš° ë‹¤ìŒ ê³ ì•„ ì„ íƒì§€ì™€ ë³‘í•© ì‹œë„
                merged_question = self._try_merge_with_next_orphan(questions, i, orphaned_indices, used_orphans)
                if merged_question != question:
                    print(f"ğŸ”— Q{question.get('question_number', 'N/A')} ë³‘í•© ì„±ê³µ: {len(question.get('options', []))} â†’ {len(merged_question.get('options', []))}ê°œ ì„ íƒì§€")
                merged_questions.append(merged_question)
            else:
                # ì™„ì „í•œ ë¬¸ì œëŠ” ê·¸ëŒ€ë¡œ ì¶”ê°€
                merged_questions.append(question)
        
        print(f"ğŸ”— ë³‘í•© ì™„ë£Œ: {len(used_orphans)}ê°œ ê³ ì•„ ì„ íƒì§€ ë³‘í•©ë¨")
        return merged_questions
    
    def _try_merge_with_next_orphan(self, questions: List[Dict], incomplete_idx: int, orphaned_indices: List[int], used_orphans: set) -> Dict:
        """ë¶ˆì™„ì „í•œ ë¬¸ì œì™€ ë‹¤ìŒ ê³ ì•„ ì„ íƒì§€ ë³‘í•© ì‹œë„"""
        
        incomplete_question = questions[incomplete_idx].copy()
        
        # ë°”ë¡œ ë‹¤ìŒì— ìˆëŠ” ê³ ì•„ ì„ íƒì§€ ì°¾ê¸°
        for orphan_idx in orphaned_indices:
            if orphan_idx > incomplete_idx and orphan_idx not in used_orphans:
                # ë„ˆë¬´ ë©€ë¦¬ ë–¨ì–´ì§„ ê²½ìš°ëŠ” ë³‘í•©í•˜ì§€ ì•ŠìŒ
                if orphan_idx - incomplete_idx > 3:
                    break
                    
                orphan_question = questions[orphan_idx]
                orphan_options = orphan_question.get('options', [])
                
                if orphan_options:
                    # ê¸°ì¡´ ì„ íƒì§€ì™€ ê³ ì•„ ì„ íƒì§€ ë³‘í•©
                    current_options = incomplete_question.get('options', [])
                    merged_options = current_options + orphan_options
                    
                    # ì„ íƒì§€ ë²ˆí˜¸ ìˆœì„œ ì •ë ¬
                    merged_options = self._sort_options_by_number(merged_options)
                    
                    incomplete_question['options'] = merged_options
                    incomplete_question['cross_page_resolved'] = True
                    used_orphans.add(orphan_idx)
                    
                    return incomplete_question
        
        return incomplete_question
    
    def _sort_options_by_number(self, options: List[str]) -> List[str]:
        """ì„ íƒì§€ë¥¼ ë²ˆí˜¸ ìˆœì„œë¡œ ì •ë ¬"""
        import re
        
        def get_option_number(option):
            if 'â‘ ' in option: return 1
            elif 'â‘¡' in option: return 2
            elif 'â‘¢' in option: return 3
            elif 'â‘£' in option: return 4
            elif 'â‘¤' in option: return 5
            else:
                match = re.search(r'(\d+)\)', option)
                return int(match.group(1)) if match else 999
        
        return sorted(options, key=get_option_number)
    
    def _recover_specific_missing_choices(self, questions: List[Dict]) -> List[Dict]:
        """íŠ¹ì • ë¬¸ì œ ë²ˆí˜¸ì˜ ëˆ„ë½ëœ ì„ íƒì§€ ë³µêµ¬ (ì˜ˆ: 11ë²ˆ ë¬¸ì œ)"""
        
        # ë¬¸ì œë³„ ì„ íƒì§€ í˜„í™© ë¶„ì„
        for question in questions:
            q_num = question.get('question_number')
            options = question.get('options', [])
            
            if isinstance(q_num, (int, str)) and str(q_num) in ['11', '6']:  # íŠ¹íˆ ë¬¸ì œê°€ ë§ì€ ë²ˆí˜¸ë“¤
                if len(options) < 4:  # ì¼ë°˜ì ìœ¼ë¡œ 4-5ê°œ ì„ íƒì§€ê°€ ì •ìƒ
                    print(f"ğŸ” Q{q_num} ì„ íƒì§€ ë¶€ì¡± ê°ì§€: {len(options)}ê°œ (ë³µêµ¬ ëŒ€ìƒ)")
                    
                    # ë‹¤ë¥¸ ë¬¸ì œë“¤ì—ì„œ ê°™ì€ ë²ˆí˜¸ì˜ ì¶”ê°€ ì„ íƒì§€ ì°¾ê¸°
                    additional_options = self._find_additional_options_for_question(questions, q_num)
                    if additional_options:
                        merged_options = options + additional_options
                        merged_options = self._sort_options_by_number(merged_options)
                        question['options'] = merged_options
                        question['cross_page_resolved'] = True
                        print(f"âœ… Q{q_num} ì„ íƒì§€ ë³µêµ¬: {len(options)} â†’ {len(merged_options)}ê°œ")
        
        return questions
    
    def _attempt_immediate_choice_recovery(self, question_number: int, current_options: List[str], chunk_questions: List[Dict]) -> List[str]:
        """ì¦‰ì‹œ ì„ íƒì§€ ë³µêµ¬ ì‹œë„ - í˜„ì¬ ì²­í¬ì—ì„œ ì—°ê²°ëœ ì„ íƒì§€ ì°¾ê¸°"""
        import re
        
        print(f"ğŸ” Q{question_number} ì¦‰ì‹œ ì„ íƒì§€ ë³µêµ¬ ì‹œë„...")
        
        # í˜„ì¬ ì„ íƒì§€ì—ì„œ ë§ˆì§€ë§‰ ë²ˆí˜¸ ì°¾ê¸°
        last_choice_num = 0
        for option in current_options:
            if 'â‘ ' in option: last_choice_num = max(last_choice_num, 1)
            elif 'â‘¡' in option: last_choice_num = max(last_choice_num, 2)
            elif 'â‘¢' in option: last_choice_num = max(last_choice_num, 3)
            elif 'â‘£' in option: last_choice_num = max(last_choice_num, 4)
            elif 'â‘¤' in option: last_choice_num = max(last_choice_num, 5)
            else:
                # 1), 2), 3) í˜•íƒœ ì²´í¬
                match = re.search(r'(\d+)\)', option)
                if match:
                    last_choice_num = max(last_choice_num, int(match.group(1)))
        
        print(f"í˜„ì¬ ì„ íƒì§€ ë§ˆì§€ë§‰ ë²ˆí˜¸: {last_choice_num}")
        
        if last_choice_num == 0:
            return current_options  # ë²ˆí˜¸ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ
        
        # ë‹¤ìŒ ë²ˆí˜¸ë“¤ (â‘¢â‘£â‘¤ ë˜ëŠ” 3)4)5)) ì°¾ê¸°
        needed_numbers = list(range(last_choice_num + 1, 6))  # ë‹¤ìŒ ë²ˆí˜¸ë¶€í„° 5ê¹Œì§€
        recovered_options = current_options.copy()
        
        print(f"ì°¾ì•„ì•¼ í•  ì„ íƒì§€ ë²ˆí˜¸: {needed_numbers}")
        
        # í˜„ì¬ ì²­í¬ì˜ ëª¨ë“  ë¬¸ì œì—ì„œ í•„ìš”í•œ ì„ íƒì§€ ì°¾ê¸°
        for question in chunk_questions:
            q_options = question.get('options', [])
            q_text = question.get('question_text', '')
            
            # ì´ ë¬¸ì œê°€ ê³ ì•„ ì„ íƒì§€ì¼ ê°€ëŠ¥ì„± ì²´í¬
            is_likely_orphan = (
                len(q_text.strip()) < 20 and  # ì§§ì€ í…ìŠ¤íŠ¸
                len(q_options) > 0 and        # ì„ íƒì§€ëŠ” ìˆìŒ
                not any(pattern in q_text.lower() for pattern in ['ë¬¸ì œ', 'ë‹¤ìŒ', 'ì•„ë˜', 'ìœ„ì—ì„œ', 'ë³´ê¸°'])  # ì§ˆë¬¸ íŒ¨í„´ ì—†ìŒ
            )
            
            if is_likely_orphan:
                print(f"ê³ ì•„ ì„ íƒì§€ í›„ë³´ ë°œê²¬: '{q_text[:30]}...' with {len(q_options)} options")
                
                for option in q_options:
                    for num in needed_numbers:
                        # ì›í•˜ëŠ” ë²ˆí˜¸ì˜ ì„ íƒì§€ì¸ì§€ ì²´í¬
                        if (f'{"â‘ â‘¡â‘¢â‘£â‘¤"[num-1]}' in option or f'{num})' in option):
                            print(f"âœ… ë°œê²¬: ì„ íƒì§€ {num}ë²ˆ - '{option[:40]}...'")
                            recovered_options.append(option)
                            if num in needed_numbers:
                                needed_numbers.remove(num)
        
        print(f"ë³µêµ¬ ê²°ê³¼: {len(current_options)}ê°œ â†’ {len(recovered_options)}ê°œ (ë¶€ì¡±: {needed_numbers})")
        return recovered_options
    
    def _is_non_question_pattern(self, text: str) -> bool:
        """ê°•í™”ëœ ë¹„-ì§ˆë¬¸ íŒ¨í„´ ê°ì§€ - ì„¤ëª…ë¬¸, í•´ì„¤, ë³´ê¸° êµ¬ë¶„"""
        text_lower = text.lower().strip()
        
        # 1. ì„¤ëª…ë¬¸ ì–´ë¯¸ íŒ¨í„´ (í•´ì„¤ì´ë‚˜ ì„¤ëª… í…ìŠ¤íŠ¸)
        explanation_endings = [
            'ì…ë‹ˆë‹¤', 'í•©ë‹ˆë‹¤', 'ë‹¤.', 'ì´ë‹¤.', 'ëœë‹¤.', 'í•œë‹¤.', 'ì´ë©°', 'í•˜ì—¬',
            'ë‹¤ìŒê³¼ ê°™ë‹¤', 'í•  ìˆ˜ ìˆë‹¤', 'ë³¼ ìˆ˜ ìˆë‹¤', 'ì—¬ê²¨ì§„ë‹¤', 'ê²ƒì´ë‹¤',
            'ì„¤ëª…í•˜ë©´', 'ì•Œì•„ë³´ë©´', 'ì‚´í´ë³´ë©´', 'ì˜ë¯¸í•œë‹¤', 'ëœ»í•œë‹¤'
        ]
        
        if any(text.endswith(ending) for ending in explanation_endings):
            return True
        
        # 2. í•´ì„¤/ì •ë‹µ í˜ì´ì§€ í‚¤ì›Œë“œ
        explanation_keywords = [
            'í•´ì„¤', 'ì •ë‹µ', 'í’€ì´', 'ë‹µì•ˆ', 'í•´ë‹µ', 'ì„¤ëª…', 'í•´ì„', 'ë‹µ:', 'ì •ë‹µì€',
            'í•´ë‹µì§€', 'ì±„ì ê¸°ì¤€', 'ì ìˆ˜', 'ì •ë¦¬', 'ìš”ì•½', 'ë³µìŠµ'
        ]
        
        if any(keyword in text_lower for keyword in explanation_keywords):
            return True
        
        # 3. ë‹¨ìˆœ ì„¤ëª…ë¬¸ íŒ¨í„´ (ì§ˆë¬¸ì´ ì•„ë‹Œ ì„œìˆ )
        description_patterns = [
            'ì˜ ì£¼ìš” ìƒíƒœëŠ”', 'ì˜ íŠ¹ì§•ì€', 'ì€ ë‹¤ìŒê³¼ ê°™ë‹¤', 'ë¼ê³  í•  ìˆ˜ ìˆë‹¤', 
            'ë¡œ ì •ì˜ëœë‹¤', 'ì— ëŒ€í•´ ì•Œì•„ë³´', 'ë¥¼ ì‚´í´ë³´', 'ëŠ” ì˜ë¯¸í•œë‹¤'
        ]
        
        if any(pattern in text for pattern in description_patterns):
            return True
        
        # 4. ë³´ê¸°/ì§€ë¬¸ íŒ¨í„´ (ë¬¸ì œì˜ ì¼ë¶€ì´ì§€ë§Œ ì§ˆë¬¸ ìì²´ê°€ ì•„ë‹˜)
        passage_indicators = [
            'ë³´ê¸°:', '[ë³´ê¸°]', 'ì§€ë¬¸:', '[ì§€ë¬¸]', 'ë‹¤ìŒì„ ì½ê³ ', 'ì•„ë˜ ì½”ë“œë¥¼', 
            'ë‹¤ìŒ í”„ë¡œê·¸ë¨ì„', 'ì•„ë˜ í‘œë¥¼', 'ë‹¤ìŒ ê·¸ë¦¼ì„'
        ]
        
        if any(indicator in text for indicator in passage_indicators):
            return True
        
        # 5. ì§ˆë¬¸í˜• íŒ¨í„´ì´ ì „í˜€ ì—†ëŠ” ê²½ìš° (ê¸ì •ì  ê²€ì¦ ì‹¤íŒ¨)
        question_patterns = [
            '?', 'ì€?', 'ëŠ”?', 'ì¸ê°€?', 'í• ê¹Œ?', 'ë‹¤ë©´?', 'ë¬´ì—‡ì¸ê°€?', 
            'êµ¬í•˜ì‹œì˜¤', 'ê³ ë¥´ì‹œì˜¤', 'í•˜ì‹œì˜¤', 'ë‹¤ìŒ ì¤‘', 'ì•„ë˜ì—ì„œ', 'ìœ„ì—ì„œ',
            'ë³´ê¸°ì—ì„œ', 'ì•Œë§ì€', 'ì˜³ì€', 'í‹€ë¦°', 'ì ì ˆí•œ'
        ]
        
        has_question_pattern = any(pattern in text for pattern in question_patterns)
        
        # ì§ˆë¬¸ íŒ¨í„´ì´ ì—†ê³  ê¸¸ì´ê°€ ê¸´ ê²½ìš° ì„¤ëª…ë¬¸ìœ¼ë¡œ ê°„ì£¼
        if not has_question_pattern and len(text) > 50:
            return True
        
        # 6. ì½”ë“œ ë¸”ë¡ë§Œ ìˆëŠ” ê²½ìš° (ì§ˆë¬¸ì´ ì•„ë‹Œ ë³´ê¸°)
        if text.startswith('[CODE_START]') or 'class ' in text or 'public ' in text:
            if not has_question_pattern:
                return True
        
        return False
    
    def _is_clearly_non_question(self, text: str) -> bool:
        """ë³´ìˆ˜ì ì¸ ë¹„-ì§ˆë¬¸ ê°ì§€ - ëª…ë°±íˆ ì§ˆë¬¸ì´ ì•„ë‹Œ ê²½ìš°ë§Œ ì œì™¸"""
        text_lower = text.lower().strip()
        
        # 1. ëª…ë°±í•œ í•´ì„¤/ì •ë‹µ í‚¤ì›Œë“œë§Œ ì œì™¸
        clear_explanation_keywords = ['ì •ë‹µì€', 'í•´ì„¤:', 'í’€ì´:', 'ì •ë‹µ:', 'ë‹µì•ˆ:']
        if any(keyword in text_lower for keyword in clear_explanation_keywords):
            return True
        
        # 2. ëª…ë°±í•œ ì„¤ëª…ë¬¸ ì–´ë¯¸ (ë§¤ìš° ì œí•œì )
        clear_explanation_endings = ['ë‹¤ìŒê³¼ ê°™ë‹¤.', 'ì •ì˜ëœë‹¤.', 'ì„¤ëª…í•˜ë©´ ë‹¤ìŒê³¼ ê°™ë‹¤.']
        if any(text.endswith(ending) for ending in clear_explanation_endings):
            return True
        
        # 3. ë©”íƒ€ë°ì´í„°ì„± í…ìŠ¤íŠ¸
        meta_patterns = ['í˜ì´ì§€ ', 'ëª©ì°¨', 'í‘œì§€', 'ì°¸ê³ ìë£Œ', 'ë¶€ë¡']
        if any(pattern in text_lower for pattern in meta_patterns) and len(text) < 30:
            return True
        
        # ë‚˜ë¨¸ì§€ëŠ” ëª¨ë‘ í†µê³¼ - ì• ë§¤í•œ ê²½ìš° ì§ˆë¬¸ìœ¼ë¡œ ì²˜ë¦¬í•˜ì—¬ ëˆ„ë½ ë°©ì§€
        return False
    
    def _find_additional_options_for_question(self, questions: List[Dict], target_question_number) -> List[str]:
        """íŠ¹ì • ë¬¸ì œ ë²ˆí˜¸ì— ëŒ€í•œ ì¶”ê°€ ì„ íƒì§€ ì°¾ê¸°"""
        additional_options = []
        
        for question in questions:
            q_num = question.get('question_number')
            if q_num == target_question_number:
                options = question.get('options', [])
                # ì´ë¯¸ ê°€ì§„ ì„ íƒì§€ì™€ ì¤‘ë³µë˜ì§€ ì•ŠëŠ” ì¶”ê°€ ì„ íƒì§€ ì¶”ì¶œ
                for option in options:
                    if option not in additional_options:
                        # â‘¢â‘£â‘¤ íŒ¨í„´ì´ ìˆëŠ”ì§€ í™•ì¸
                        if any(choice in option for choice in ['â‘¢', 'â‘£', 'â‘¤']):
                            additional_options.append(option)
        
        return additional_options
    
    def _preprocess_page_connections(self, page_content: str, page_num: int, previous_pages: List[str]) -> str:
        """ğŸ”— ê°•í™”ëœ í˜ì´ì§€ê°„ ì„ íƒì§€ ì—°ê²° ì „ì²˜ë¦¬ - í…ìŠ¤íŠ¸í™” ë‹¨ê³„ì—ì„œ ì²˜ë¦¬"""
        import re
        
        print(f"ğŸ”— í˜ì´ì§€ {page_num} ê°•í™”ëœ ì—°ê²° ì „ì²˜ë¦¬ ì‹œì‘...")
        
        # ğŸˆ 1ë‹¨ê³„: í˜ì´ì§€ ê²½ê³„ ë§ˆì»¤ ê°ì§€
        page_boundary_issues = self._detect_page_boundary_markers(page_content)
        
        # ğŸˆ 2ë‹¨ê³„: ê³ ì•„ ì„ íƒì§€ íŒ¨í„´ ê°ì§€ (ê°•í™”ëœ ë²„ì „)
        orphaned_choices = self._detect_orphaned_choices_in_text(page_content)
        
        # ğŸˆ 3ë‹¨ê³„: ì½”ë“œ ë¸”ë¡ ê²½ê³„ ê°ì§€
        code_boundary_issues = self._detect_code_boundary_issues(page_content)
        
        if page_boundary_issues:
            print(f"âš ï¸ í˜ì´ì§€ {page_num} ê²½ê³„ ë§ˆì»¤ ê°ì§€: {page_boundary_issues}")
        
        if orphaned_choices and previous_pages:
            print(f"ğŸ” í˜ì´ì§€ {page_num}ì—ì„œ ê³ ì•„ ì„ íƒì§€ ê°ì§€: {len(orphaned_choices)}ê°œ")
            
            # 4. ì´ì „ í˜ì´ì§€ì—ì„œ ë¶ˆì™„ì „í•œ ë¬¸ì œ ì°¾ê¸° (ê°•í™”ëœ ë²„ì „)
            last_page_content = previous_pages[-1] if previous_pages else ""
            incomplete_question = self._find_incomplete_question_in_text(last_page_content)
            
            if incomplete_question:
                print(f"ğŸ”— ì´ì „ í˜ì´ì§€ ë¶ˆì™„ì „ ë¬¸ì œ ë°œê²¬: {incomplete_question[:70]}...")
                
                # 5. ìŠ¤ë§ˆíŠ¸ ì„ íƒì§€ ë³‘í•© ìˆ˜í–‰
                merged_content = self._merge_orphaned_choices_with_previous(
                    page_content, orphaned_choices, incomplete_question, previous_pages
                )
                
                if merged_content != page_content:
                    print(f"âœ… í˜ì´ì§€ {page_num} ìŠ¤ë§ˆíŠ¸ ì„ íƒì§€ ë³‘í•© ì™„ë£Œ")
                    return merged_content
        
        # ğŸˆ 6ë‹¨ê³„: ì½”ë“œ ì—°ì†ì„± ë° ë“¤ì—¬ì“°ê¸° ë³µêµ¬
        if code_boundary_issues and previous_pages:
            merged_content = self._merge_code_blocks_across_pages(page_content, previous_pages)
            if merged_content != page_content:
                print(f"âœ… í˜ì´ì§€ {page_num} ì½”ë“œ ë¸”ë¡ ë³‘í•© ì™„ë£Œ")
                return merged_content
        
        return page_content
    
    def _detect_page_boundary_markers(self, content: str) -> List[str]:
        """ğŸ” í˜ì´ì§€ ê²½ê³„ ë§ˆì»¤ ê°ì§€"""
        import re
        
        boundary_patterns = [
            r'\(â‘¢â‘£â‘¤ ë‹¤ìŒ í˜ì´ì§€\)',
            r'\(â‘£â‘¤â‘¥ ë‹¤ìŒ í˜ì´ì§€\)', 
            r'\(â‘¤â‘¥ ë‹¤ìŒ í˜ì´ì§€\)',
            r'\(â‘¥ ë‹¤ìŒ í˜ì´ì§€\)',
            r'\(í˜ì´ì§€ ê²½ê³„ë¡œ ì˜ë¦¼\)',
            r'\[ì—°ì†\]', r'\[ì´ì–´ì§\]'
        ]
        
        found_markers = []
        for pattern in boundary_patterns:
            if re.search(pattern, content):
                found_markers.append(pattern)
        
        return found_markers
    
    def _detect_code_boundary_issues(self, content: str) -> bool:
        """ğŸ” ì½”ë“œ ë¸”ë¡ ê²½ê³„ ë¬¸ì œ ê°ì§€"""
        import re
        
        # ë¹ˆ ì½”ë“œ ë¸”ë¡ ë˜ëŠ” ë“¤ì—¬ì“°ê¸°ê°€ ë§ê°€ì§„ ì½”ë“œ ê°ì§€
        code_patterns = [
            r'\{\s*$',  # ì—´ë¦° ì¤‘ê´„í˜¸ë§Œ ìˆìŒ
            r'^\s*\}',  # ë‹«íŒ ì¤‘ê´„í˜¸ë¡œ ì‹œì‘
            r'if\s*\(.*\)\s*$',  # if ë¬¸ ë¯¸ì™„ì„±
            r'for\s*\(.*\)\s*$',  # for ë¬¸ ë¯¸ì™„ì„±
            r'while\s*\(.*\)\s*$',  # while ë¬¸ ë¯¸ì™„ì„±
        ]
        
        for pattern in code_patterns:
            if re.search(pattern, content, re.MULTILINE):
                return True
        return False
    
    def _merge_code_blocks_across_pages(self, current_content: str, previous_pages: List[str]) -> str:
        """ğŸ”— í˜ì´ì§€ ê°„ ì½”ë“œ ë¸”ë¡ ë³‘í•© ë° ë“¤ì—¬ì“°ê¸° ë³µêµ¬"""
        import re
        
        if not previous_pages:
            return current_content
        
        last_page = previous_pages[-1]
        
        # ì´ì „ í˜ì´ì§€ì˜ ë§ˆì§€ë§‰ ì½”ë“œ ë¸”ë¡ ì°¾ê¸°
        last_code_match = None
        for match in re.finditer(r'```[\w]*\n(.*?)```', last_page, re.DOTALL):
            last_code_match = match
        
        if last_code_match:
            last_code = last_code_match.group(1)
            # ë¯¸ì™„ì„±ëœ ì½”ë“œ ë¸”ë¡ì¸ì§€ ê²€ì‚¬ (ì¤‘ê´„í˜¸ ë§¤ì¹­ ë“±)
            open_braces = last_code.count('{')
            close_braces = last_code.count('}')
            
            if open_braces > close_braces:
                # í˜„ì¬ í˜ì´ì§€ì—ì„œ ì½”ë“œ ì—°ì† ì°¾ê¸°
                current_code_match = re.search(r'^(.*?)```', current_content, re.DOTALL)
                if current_code_match:
                    current_code_part = current_code_match.group(1)
                    # ì½”ë“œ ë³‘í•© ë° ë“¤ì—¬ì“°ê¸° ë³µêµ¬
                    merged_code = self._restore_code_indentation(last_code + "\n" + current_code_part)
                    
                    # ì´ì „ í˜ì´ì§€ ì—…ë°ì´íŠ¸
                    updated_last_page = last_page.replace(last_code_match.group(0), 
                                                        f"```\n{merged_code}\n```")
                    previous_pages[-1] = updated_last_page
                    
                    # í˜„ì¬ í˜ì´ì§€ì—ì„œ ì½”ë“œ ë¶€ë¶„ ì œê±°
                    remaining_content = current_content.replace(current_code_match.group(0), '').strip()
                    return remaining_content
        
        return current_content
    
    def _restore_code_indentation(self, code_block: str) -> str:
        """ğŸ¯ ì½”ë“œ ë¸”ë¡ ë“¤ì—¬ì“°ê¸° ë³µêµ¬"""
        lines = code_block.split('\n')
        restored_lines = []
        current_indent = 0
        
        for line in lines:
            stripped = line.strip()
            if not stripped:
                restored_lines.append('')
                continue
            
            # ë“¤ì—¬ì“°ê¸° ë ˆë²¨ ì¡°ì •
            if stripped.endswith('{'):
                restored_lines.append('    ' * current_indent + stripped)
                current_indent += 1
            elif stripped.startswith('}'):
                current_indent = max(0, current_indent - 1)
                restored_lines.append('    ' * current_indent + stripped)
            else:
                restored_lines.append('    ' * current_indent + stripped)
        
        return '\n'.join(restored_lines)
    
    def _detect_orphaned_choices_in_text(self, content: str) -> List[str]:
        """ğŸ” ê°•í™”ëœ í…ìŠ¤íŠ¸ì—ì„œ ê³ ì•„ ì„ íƒì§€ íŒ¨í„´ íƒì§€"""
        import re
        
        lines = content.strip().split('\n')
        orphaned_choices = []
        
        # ğŸ” ê°•í™”ëœ ì„ íƒì§€ íŒ¨í„´ë“¤ - ë” ë§ì€ íŒ¨í„´ ì¸ì‹
        choice_patterns = [
            r'^ì„ íƒì§€\s*[3-5]:\s*(.+)$',  # "ì„ íƒì§€ 3:", "ì„ íƒì§€ 4:", "ì„ íƒì§€ 5:"
            r'^[â‘¢â‘£â‘¤]\s*(.+)$',  # â‘¢, â‘£, â‘¤ë¡œ ì‹œì‘
            r'^[3-5]\)\s*(.+)$',  # 3), 4), 5)ë¡œ ì‹œì‘
            r'^\s*[â‘¢â‘£â‘¤]\s*(.+)$',  # ê³µë°± + â‘¢â‘£â‘¤ íŒ¨í„´
            r'^\s*[3-5]\)\s*(.+)$',  # ê³µë°± + 3)4)5) íŒ¨í„„
            r'^\s*â‘¢',  # ë‹¨ìˆœíˆ â‘¢ìœ¼ë¡œ ì‹œì‘ (ì§§ì€ ì„ íƒì§€)
            r'^\s*â‘£',  # ë‹¨ìˆœíˆ â‘£ìœ¼ë¡œ ì‹œì‘
            r'^\s*â‘¤',  # ë‹¨ìˆœíˆ â‘¤ìœ¼ë¡œ ì‹œì‘
        ]
        
        # ğŸ” ê°•í™”ëœ ì§ˆë¬¸ íŒ¨í„´ ì¸ì‹
        question_patterns = [
            r'ë¬¸ì œ\s*\d+ë²ˆ',  # "ë¬¸ì œ 1ë²ˆ"
            r'\d+\.\s*[ê°€-í£].+\?',  # "1. ë‹¤ìŒ ì¤‘...?"
            r'ë‹¤ìŒ\s*ì¤‘',  # "ë‹¤ìŒ ì¤‘"
            r'ë¬´ì—‡ì¸ê°€\?', r'ì–´ë–¤\s*ê²ƒ', r'ì˜³ì€\s*ê²ƒ', r'í‹€ë¦°\s*ê²ƒ',
            r'í•´ë‹¹í•˜ëŠ”\s*ê²ƒ', r'ì•„ë‹Œ\s*ê²ƒ', r'ì„¤ëª…ìœ¼ë¡œ', r'ì—\s*ëŒ€í•œ',
            r'êµ¬í•˜ì‹œì˜¤', r'ê³ ë¥´ì‹œì˜¤', r'í•˜ì‹œì˜¤$'
        ]
        
        has_question = any(re.search(pattern, content, re.IGNORECASE) for pattern in question_patterns)
        
        # ì§ˆë¬¸ ì—†ì´ ì„ íƒì§€ë§Œ ìˆëŠ” ê²½ìš°
        if not has_question:
            for line in lines:
                line = line.strip()
                for pattern in choice_patterns:
                    match = re.search(pattern, line, re.IGNORECASE)
                    if match:
                        orphaned_choices.append(line)
                        break
        
        return orphaned_choices
    
    def _find_incomplete_question_in_text(self, content: str) -> str:
        """ğŸ” ê°•í™”ëœ í…ìŠ¤íŠ¸ì—ì„œ ë¶ˆì™„ì „í•œ ë¬¸ì œ ì°¾ê¸° (ì„ íƒì§€ê°€ ë¶€ì¡±í•œ)"""
        import re
        
        # ğŸ” ê°•í™”ëœ ë¬¸ì œ íŒ¨í„´ - ë‹¤ì–‘í•œ ë¬¸ì œ ë²ˆí˜¸ í‘œê¸°ë²• ì¸ì‹
        question_patterns = [
            r'(ë¬¸ì œ\s*\d+ë²ˆ.*?)(?=ë¬¸ì œ\s*\d+ë²ˆ|$)',  # "ë¬¸ì œ 1ë²ˆ"
            r'(\d+\.\s*[ê°€-í£].*?)(?=\d+\.|$)',  # "1. ë‹¤ìŒ ì¤‘"
            r'(\d+\)\s*[ê°€-í£].*?)(?=\d+\)|$)',  # "1) ë‹¤ìŒ ì¤‘"
            r'([â‘ â‘¡â‘¢â‘£â‘¤â‘¥â‘¦â‘§â‘¨â‘©].*?)(?=[â‘ â‘¡â‘¢â‘£â‘¤â‘¥â‘¦â‘§â‘¨â‘©]|$)'  # ì›í˜• ë²ˆí˜¸
        ]
        
        for pattern in question_patterns:
            question_match = re.search(pattern, content, re.DOTALL)
            if question_match:
                question_block = question_match.group(1)
                
                # ğŸ” ê°•í™”ëœ ì„ íƒì§€ ê°œìˆ˜ í™•ì¸ - ë” ì •í™•í•œ íŒ¨í„´ ë§¤ì¹­
                choice_patterns = [
                    r'ì„ íƒì§€\s*[1-5]:',
                    r'[â‘ â‘¡â‘¢â‘£â‘¤]',
                    r'[1-5]\)',
                    r'\s*â‘ \s', r'\s*â‘¡\s', r'\s*â‘¢\s', r'\s*â‘£\s', r'\s*â‘¤\s'
                ]
                
                choice_count = 0
                for choice_pattern in choice_patterns:
                    matches = re.findall(choice_pattern, question_block)
                    choice_count = max(choice_count, len(matches))
                
                # ğŸ¯ ì„ íƒì§€ ì—°ì†ì„± ê²€ì‚¬
                found_numbers = []
                if 'â‘ ' in question_block: found_numbers.append(1)
                if 'â‘¡' in question_block: found_numbers.append(2)
                if 'â‘¢' in question_block: found_numbers.append(3)
                if 'â‘£' in question_block: found_numbers.append(4)
                if 'â‘¤' in question_block: found_numbers.append(5)
                
                # 1,2ë§Œ ìˆê±°ë‚˜ 1,2,3ë§Œ ìˆëŠ” ê²½ìš° = ë¶ˆì™„ì „
                if found_numbers and max(found_numbers) <= 3:
                    print(f"ğŸ” ë¶ˆì™„ì „í•œ ë¬¸ì œ ë°œê²¬: ì„ íƒì§€ {found_numbers}ë²ˆë§Œ ìˆìŒ")
                    return question_block
                
                # ë³´í†µ 4-5ê°œ ì„ íƒì§€ê°€ ì •ìƒì´ë¯€ë¡œ 3ê°œ ì´í•˜ë©´ ë¶ˆì™„ì „
                if choice_count <= 3:
                    print(f"ğŸ” ë¶ˆì™„ì „í•œ ë¬¸ì œ ë°œê²¬: {choice_count}ê°œ ì„ íƒì§€ë§Œ ìˆìŒ")
                    return question_block
        
        return ""
    
    def _merge_orphaned_choices_with_previous(self, current_content: str, orphaned_choices: List[str], 
                                            incomplete_question: str, previous_pages: List[str]) -> str:
        """ğŸ”— ê°•í™”ëœ ê³ ì•„ ì„ íƒì§€ë¥¼ ì´ì „ í˜ì´ì§€ì˜ ë¶ˆì™„ì „í•œ ë¬¸ì œì™€ ë³‘í•©"""
        
        if not orphaned_choices or not incomplete_question:
            return current_content
        
        print(f"ğŸ”— ì„ íƒì§€ ë³‘í•© ì‹œì‘: {len(orphaned_choices)}ê°œ ê³ ì•„ ì„ íƒì§€ â†’ ì´ì „ í˜ì´ì§€ ë¶ˆì™„ì „ ë¬¸ì œ")
        
        # ì´ì „ í˜ì´ì§€ ë‚´ìš©ì—ì„œ ë¶ˆì™„ì „í•œ ë¬¸ì œë¥¼ ì™„ì „í•œ ë¬¸ì œë¡œ êµì²´
        if previous_pages:
            last_page_idx = len(previous_pages) - 1
            updated_last_page = previous_pages[last_page_idx]
            
            # ğŸ”— ìŠ¤ë§ˆíŠ¸ ì„ íƒì§€ ë³‘í•© - ê¸°ì¡´ ì„ íƒì§€ì™€ ë²ˆí˜¸ ì—°ì†ì„± ë§ì¶”ê¸°
            enhanced_choices = self._smart_merge_choices(incomplete_question, orphaned_choices)
            enhanced_question = incomplete_question + "\n" + "\n".join(enhanced_choices)
            
            # ì´ì „ í˜ì´ì§€ ì—…ë°ì´íŠ¸
            updated_last_page = updated_last_page.replace(incomplete_question, enhanced_question)
            previous_pages[last_page_idx] = updated_last_page
            
            print(f"âœ… ì´ì „ í˜ì´ì§€ ì„ íƒì§€ ë³‘í•© ì™„ë£Œ: {len(enhanced_choices)}ê°œ ì„ íƒì§€ ì¶”ê°€")
            
            # í˜„ì¬ í˜ì´ì§€ì—ì„œ ê³ ì•„ ì„ íƒì§€ ì œê±° - ë” ì •í™•í•œ ì œê±°
            remaining_content = current_content
            for choice in orphaned_choices:
                # ì •í™•í•œ ë¼ì¸ ë‹¨ìœ„ë¡œ ì œê±°
                lines = remaining_content.split('\n')
                filtered_lines = []
                for line in lines:
                    if not any(choice.strip() in line.strip() for choice in orphaned_choices):
                        filtered_lines.append(line)
                remaining_content = '\n'.join(filtered_lines)
            
            # ë¹ˆ ì¤„ ì •ë¦¬  
            import re
            remaining_content = re.sub(r'\n\s*\n', '\n', remaining_content).strip()
            
            print(f"âœ… í˜„ì¬ í˜ì´ì§€ ê³ ì•„ ì„ íƒì§€ ì œê±° ì™„ë£Œ")
            return remaining_content
        
        return current_content
    
    def _smart_merge_choices(self, incomplete_question: str, orphaned_choices: List[str]) -> List[str]:
        """ğŸ¯ ìŠ¤ë§ˆíŠ¸ ì„ íƒì§€ ë³‘í•© - ë²ˆí˜¸ ì—°ì†ì„± ë³´ì¥"""
        import re
        
        # ê¸°ì¡´ ë¬¸ì œì—ì„œ ì´ë¯¸ ìˆëŠ” ì„ íƒì§€ ë²ˆí˜¸ ì°¾ê¸°
        existing_numbers = []
        if 'â‘ ' in incomplete_question: existing_numbers.append(1)
        if 'â‘¡' in incomplete_question: existing_numbers.append(2)
        if 'â‘¢' in incomplete_question: existing_numbers.append(3)
        
        print(f"ğŸ” ê¸°ì¡´ ì„ íƒì§€: {existing_numbers}ë²ˆ")
        
        # ê³ ì•„ ì„ íƒì§€ì˜ ë²ˆí˜¸ ì¬ì¡°ì •
        adjusted_choices = []
        expected_next = max(existing_numbers) + 1 if existing_numbers else 3
        
        for choice in orphaned_choices:
            # ê¸°ì¡´ ë²ˆí˜¸ ì œê±°í•˜ê³  ì˜¬ë°”ë¥¸ ë²ˆí˜¸ë¡œ êµì²´
            clean_choice = re.sub(r'^\s*[â‘¢â‘£â‘¤â‘ â‘¡]\s*', '', choice.strip())
            clean_choice = re.sub(r'^\s*[3-5]\)\s*', '', clean_choice)
            
            # ì˜¬ë°”ë¥¸ ë²ˆí˜¸ ì¶”ê°€
            if expected_next == 3:
                adjusted_choice = f"â‘¢ {clean_choice}"
            elif expected_next == 4:
                adjusted_choice = f"â‘£ {clean_choice}"
            elif expected_next == 5:
                adjusted_choice = f"â‘¤ {clean_choice}"
            else:
                adjusted_choice = f"{expected_next}) {clean_choice}"
            
            adjusted_choices.append(adjusted_choice)
            expected_next += 1
        
        print(f"âœ… ì„ íƒì§€ ë²ˆí˜¸ ì¬ì¡°ì •: {len(adjusted_choices)}ê°œ â†’ {expected_next-1}ë²ˆê¹Œì§€")
        return adjusted_choices
    
    
    async def _handle_cross_page_questions(self, all_questions: List[Dict], full_connected_content: str) -> List[Dict]:
        """í˜ì´ì§€ ê²½ê³„ ë¬¸ì œ í•´ê²° ë° ë¬¸ì œ ë²ˆí˜¸ ì •ê·œí™” + ğŸ†• ê³ ì•„ ì„ íƒì§€ ë³‘í•© ì‹œìŠ¤í…œ"""
        print(f"\nğŸ”— í˜ì´ì§€ ê²½ê³„ ë¬¸ì œ í•´ê²° ë° ê³ ì•„ ì„ íƒì§€ íƒì§€ ì¤‘...")
        
        # ğŸ†• STEP 0: ê³ ì•„ ì„ íƒì§€ íƒì§€ ë° ë³‘í•© ì‹œìŠ¤í…œ
        print(f"ğŸ” STEP 0: ê³ ì•„ ì„ íƒì§€ íƒì§€ ì‹œì‘...")
        orphaned_choices_merged = await self._detect_and_merge_orphaned_choices(all_questions)
        all_questions = orphaned_choices_merged
        
        # 1. ì „ì²´ ì—°ê²° ì´ë¯¸ì§€ì—ì„œ ë°œê²¬ëœ í˜ì´ì§€ ê²½ê³„ ë¬¸ì œë“¤ íŒŒì‹±
        cross_page_fixes = {}
        if full_connected_content and "í˜ì´ì§€ ê²½ê³„ ë¬¸ì œ" in full_connected_content:
            # í˜ì´ì§€ ê²½ê³„ ë¬¸ì œ ë°œê²¬ëœ ê²½ìš° íŒŒì‹±
            import re
            problem_patterns = re.findall(r'### í˜ì´ì§€ ê²½ê³„ ë¬¸ì œ (\d+)ë²ˆ ë°œê²¬!(.*?)(?=### í˜ì´ì§€ ê²½ê³„ ë¬¸ì œ|\Z)', 
                                        full_connected_content, re.DOTALL)
            
            for problem_num, problem_content in problem_patterns:
                try:
                    question_match = re.search(r'\*\*ì™„ì „í•œ ì§ˆë¬¸\*\*: (.*?)(?=\*\*)', problem_content, re.DOTALL)
                    options_match = re.search(r'\*\*ì™„ì „í•œ ì„ íƒì§€\*\*: (.*?)(?=\*\*)', problem_content, re.DOTALL)
                    table_match = re.search(r'\*\*ì™„ì „í•œ í‘œ ë°ì´í„°\*\*.*?\n((?:\|.*?\|\n)+)', problem_content, re.DOTALL)
                    
                    if question_match or options_match or table_match:
                        cross_page_fixes[int(problem_num)] = {
                            'question': question_match.group(1).strip() if question_match else None,
                            'options': options_match.group(1).strip() if options_match else None,
                            'table': table_match.group(1).strip() if table_match else None
                        }
                        print(f"âœ… í˜ì´ì§€ ê²½ê³„ ë¬¸ì œ {problem_num}ë²ˆ ìˆ˜ì • ë°ì´í„° ì¤€ë¹„ë¨")
                except Exception as e:
                    print(f"âš ï¸ í˜ì´ì§€ ê²½ê³„ ë¬¸ì œ {problem_num}ë²ˆ íŒŒì‹± ì‹¤íŒ¨: {e}")
        
        # 2. ë¬¸ì œ ë²ˆí˜¸ ì¤‘ë³µ í•´ê²° ë° ì •ê·œí™”
        question_by_number = {}
        fixed_questions = []
        
        for question in all_questions:
            q_num = question.get('question_number', 'N/A')
            
            # ë¬¸ì œ ë²ˆí˜¸ ì •ê·œí™”
            if isinstance(q_num, str) and q_num != 'N/A':
                import re
                number_match = re.search(r'(\d+)$', str(q_num))
                if number_match:
                    q_num = int(number_match.group(1))
            
            if q_num == 'N/A' or not isinstance(q_num, int):
                continue
                
            # 3. í˜ì´ì§€ ê²½ê³„ ë¬¸ì œ ìˆ˜ì • ì ìš©
            if q_num in cross_page_fixes:
                fix_data = cross_page_fixes[q_num]
                
                # ì§ˆë¬¸ ìˆ˜ì •
                if fix_data['question']:
                    question['question_text'] = fix_data['question']
                    print(f"ğŸ”§ Q{q_num}: ì§ˆë¬¸ ìˆ˜ì • ì ìš©")
                
                # ì„ íƒì§€ ìˆ˜ì •
                if fix_data['options']:
                    # ì„ íƒì§€ íŒŒì‹± (â‘ , â‘¡, â‘¢, â‘£ í˜•íƒœ)
                    import re
                    options = re.findall(r'[â‘ â‘¡â‘¢â‘£â‘¤]\s*[^â‘ â‘¡â‘¢â‘£â‘¤]+', fix_data['options'])
                    if options:
                        question['options'] = [opt.strip() for opt in options]
                        print(f"ğŸ”§ Q{q_num}: ì„ íƒì§€ ìˆ˜ì • ì ìš© ({len(options)}ê°œ)")
                
                # í‘œ ë°ì´í„° ìˆ˜ì •
                if fix_data['table']:
                    current_passage = question.get('passage', '')
                    if fix_data['table'] not in current_passage:
                        question['passage'] = current_passage + '\n\n' + fix_data['table']
                        question['has_table'] = True
                        print(f"ğŸ”§ Q{q_num}: í‘œ ë°ì´í„° ìˆ˜ì • ì ìš©")
            
            # 4. ì¤‘ë³µ ë¬¸ì œ ë³‘í•© ì²˜ë¦¬
            if q_num in question_by_number:
                existing = question_by_number[q_num]
                current = question
                
                # ë” ì™„ì „í•œ ë²„ì „ ì„ íƒ (ì„ íƒì§€ ìˆ˜, ì§€ë¬¸ ê¸¸ì´ ê¸°ì¤€)
                existing_score = len(existing.get('options', [])) * 10 + len(existing.get('passage', ''))
                current_score = len(current.get('options', [])) * 10 + len(current.get('passage', ''))
                
                if current_score > existing_score:
                    question_by_number[q_num] = current
                    print(f"ğŸ”„ Q{q_num}: ë” ì™„ì „í•œ ë²„ì „ìœ¼ë¡œ êµì²´ (ì ìˆ˜: {existing_score} â†’ {current_score})")
            else:
                question_by_number[q_num] = question
        
        # 5. ë¬¸ì œ ë²ˆí˜¸ ìˆœì„œë¡œ ì •ë ¬í•˜ì—¬ ë°˜í™˜
        sorted_questions = []
        for q_num in sorted(question_by_number.keys()):
            question = question_by_number[q_num]
            question['question_number'] = q_num
            sorted_questions.append(question)
        
        print(f"ğŸ”— í˜ì´ì§€ ê²½ê³„ ë¬¸ì œ í•´ê²° ì™„ë£Œ:")
        print(f"  - ìˆ˜ì • ì ìš©: {len(cross_page_fixes)}ê°œ ë¬¸ì œ")
        print(f"  - ì¤‘ë³µ ì œê±°: {len(all_questions)} â†’ {len(sorted_questions)}ê°œ ë¬¸ì œ")
        print(f"  - ìµœì¢… ë²”ìœ„: 1~{max(question_by_number.keys()) if question_by_number else 0}ë²ˆ")
        
        return sorted_questions
    
    async def _post_process_special_content(self, questions: List[Dict], chunk_idx: int) -> List[Dict]:
        """íŠ¹ìˆ˜ ì½˜í…ì¸  (í‘œ/ê·¸ë¦¼/ì½”ë“œ) í›„ì²˜ë¦¬"""
        processed = []
        
        for question in questions:
            passage = question.get('passage', '')
            options = question.get('options', [])
            
            # í‘œ ë°ì´í„° í›„ì²˜ë¦¬
            if '|' in passage and passage.count('|') > 2:
                question['has_table'] = True
                # í‘œ í—¤ë” ëˆ„ë½ ê²€ì‚¬ ë° ìˆ˜ì •
                lines = passage.split('\n')
                table_lines = [line for line in lines if '|' in line]
                if table_lines and not any('í”„ë¡œì„¸ìŠ¤' in line or 'ë„ì°©ì‹œê°„' in line or 'ì‹¤í–‰ì‹œê°„' in line for line in table_lines[:1]):
                    # í‘œ í—¤ë”ê°€ ëˆ„ë½ëœ ê²½ìš° ì¶”ì •í•˜ì—¬ ì¶”ê°€
                    if len(table_lines[0].split('|')) == 4:  # 3ì»¬ëŸ¼ + ë¹ˆì¹¸
                        header = "| í”„ë¡œì„¸ìŠ¤ | ë„ì°©ì‹œê°„ | ì‹¤í–‰ì‹œê°„ |"
                        question['passage'] = header + '\n|-------|-------|-------|\n' + '\n'.join(table_lines)
                        print(f"ğŸ”§ Q{question.get('question_number')}: í‘œ í—¤ë” ë³´ì™„ ì ìš©")
            
            # ê·¸ë¦¼ ì„ íƒì§€ í›„ì²˜ë¦¬  
            has_image_options = any('[ê·¸ë¦¼:' in opt or '[ìˆ˜ì‹:' in opt or len(opt.strip()) <= 5 for opt in options)
            if has_image_options:
                question['has_figure'] = True
                print(f"ğŸ“Š Q{question.get('question_number')}: ê·¸ë¦¼ ì„ íƒì§€ ê°ì§€ë¨")
            
            processed.append(question)
        
        return processed
    
    def _estimate_question_count_from_markdown(self, markdown_content: str) -> int:
        """ë§ˆí¬ë‹¤ìš´ í…ìŠ¤íŠ¸ì—ì„œ ì‹¤ì œ ë¬¸ì œ ìˆ˜ ì¶”ì •"""
        import re
        
        # ë‹¤ì–‘í•œ ë¬¸ì œ ë²ˆí˜¸ íŒ¨í„´ ê²€ìƒ‰
        patterns = [
            r'### ë¬¸ì œ (\d+)ë²ˆ',  # GPT Visionì´ ìƒì„±í•œ êµ¬ì¡°í™”ëœ íŒ¨í„´
            r'## (\d+)ë²ˆ',       # ëŒ€ì•ˆ ë²ˆí˜¸ íŒ¨í„´
            r'(\d+)\.\s*[ê°€-í£]',  # 1. ë¡œ ì‹œì‘í•˜ëŠ” ë¬¸ì œ
            r'(\d+)\)\s*[ê°€-í£]',  # 1) ë¡œ ì‹œì‘í•˜ëŠ” ë¬¸ì œ
            r'ë¬¸ì œ\s*(\d+)',      # "ë¬¸ì œ 1" íŒ¨í„´
            r'Q\.?(\d+)',        # Q1, Q.1 íŒ¨í„´
        ]
        
        question_numbers = set()
        
        for pattern in patterns:
            matches = re.findall(pattern, markdown_content)
            for match in matches:
                try:
                    num = int(match)
                    if 1 <= num <= 150:  # í•©ë¦¬ì ì¸ ë¬¸ì œ ë²ˆí˜¸ ë²”ìœ„
                        question_numbers.add(num)
                except ValueError:
                    continue
        
        # ìµœê³  ë¬¸ì œ ë²ˆí˜¸ë¥¼ ê¸°ì¤€ìœ¼ë¡œ ì‹¤ì œ ë¬¸ì œ ìˆ˜ ì¶”ì •
        if question_numbers:
            max_question = max(question_numbers)
            estimated_count = max_question
            print(f"  ğŸ“Š ë¬¸ì œ ë²ˆí˜¸ ë²”ìœ„: 1~{max_question} (ì¶”ì •: {estimated_count}ê°œ)")
            return estimated_count
        
        # ë¬¸ì œ ë²ˆí˜¸ê°€ ì—†ëŠ” ê²½ìš° ì§ˆë¬¸ íŒ¨í„´ìœ¼ë¡œ ì¶”ì •
        question_patterns = [
            r'[ê°€-í£]{3,}ì€\?',     # ~ì€?
            r'[ê°€-í£]{3,}ëŠ”\?',     # ~ëŠ”?
            r'[ê°€-í£]{3,}ì¸ê°€\?',   # ~ì¸ê°€?
            r'[ê°€-í£]{3,}í• ê¹Œ\?',   # ~í• ê¹Œ?
            r'[ê°€-í£]{3,}ë‹¤ë©´\?',   # ~ë‹¤ë©´?
            r'ë¬´ì—‡ì¸ê°€\?',          # ë¬´ì—‡ì¸ê°€?
            r'êµ¬í•˜ì‹œì˜¤',           # êµ¬í•˜ì‹œì˜¤
            r'ê³ ë¥´ì‹œì˜¤',           # ê³ ë¥´ì‹œì˜¤
            r'ë‹¤ìŒ ì¤‘',            # ë‹¤ìŒ ì¤‘
            r'ì•„ë˜ì—ì„œ',           # ì•„ë˜ì—ì„œ
        ]
        
        question_count = 0
        for pattern in question_patterns:
            matches = re.findall(pattern, markdown_content)
            question_count += len(matches)
        
        # ì¤‘ë³µ ì œê±°ë¥¼ ìœ„í•´ ì ˆë°˜ìœ¼ë¡œ ì¶”ì •
        estimated_from_patterns = max(1, question_count // 2)
        print(f"  ğŸ“Š ì§ˆë¬¸ íŒ¨í„´ ê¸°ë°˜ ì¶”ì •: {estimated_from_patterns}ê°œ")
        
        return estimated_from_patterns
    
    def _count_questions_in_page(self, page_content: str) -> int:
        """ê°œë³„ í˜ì´ì§€ì—ì„œ ë¬¸ì œ ìˆ˜ ì¹´ìš´íŠ¸"""
        import re
        
        # í˜ì´ì§€ ë‚´ ë¬¸ì œ ë²ˆí˜¸ íŒ¨í„´ ê²€ìƒ‰
        patterns = [
            r'### ë¬¸ì œ (\d+)ë²ˆ',
            r'## (\d+)ë²ˆ', 
            r'(\d+)\.\s*[ê°€-í£]',
            r'(\d+)\)\s*[ê°€-í£]',
            r'ë¬¸ì œ\s*(\d+)',
            r'Q\.?(\d+)',
        ]
        
        question_numbers_in_page = set()
        
        for pattern in patterns:
            matches = re.findall(pattern, page_content)
            for match in matches:
                try:
                    num = int(match)
                    if 1 <= num <= 150:
                        question_numbers_in_page.add(num)
                except ValueError:
                    continue
        
        if question_numbers_in_page:
            return len(question_numbers_in_page)
        
        # ë²ˆí˜¸ê°€ ì—†ëŠ” ê²½ìš° ì§ˆë¬¸ íŒ¨í„´ìœ¼ë¡œ ì¶”ì •
        question_indicators = [
            r'[â‘ â‘¡â‘¢â‘£â‘¤]',  # ì„ íƒì§€ íŒ¨í„´
            r'1\)\s*[ê°€-í£]', r'2\)\s*[ê°€-í£]', r'3\)\s*[ê°€-í£]',  # ë²ˆí˜¸í˜• ì„ íƒì§€
            r'[ê°€-í£]{5,}ì€\?', r'[ê°€-í£]{5,}ëŠ”\?',  # ì§ˆë¬¸ íŒ¨í„´
            r'ë‹¤ìŒ ì¤‘', r'ì•„ë˜ì—ì„œ', r'ìœ„ì—ì„œ',  # ë¬¸ì œ ì‹œì‘ í‘œí˜„
        ]
        
        indicators_found = 0
        for pattern in question_indicators:
            if re.search(pattern, page_content):
                indicators_found += 1
        
        # ì—¬ëŸ¬ ì§€í‘œê°€ ìˆìœ¼ë©´ ë¬¸ì œê°€ ìˆì„ ê°€ëŠ¥ì„±ì´ ë†’ìŒ
        if indicators_found >= 3:
            return 1
        elif indicators_found >= 1:
            return max(1, indicators_found // 3)  # ë³´ìˆ˜ì  ì¶”ì •
        else:
            return 0
    
    async def _detect_tables_in_pages(self, upload_id: int, page_files: List[str], db: Session) -> Dict[str, Any]:
        """1ë‹¨ê³„: ëª¨ë“  í˜ì´ì§€ì—ì„œ í‘œ ìœ„ì¹˜ ê°ì§€ (ì˜ˆë¹„ ìŠ¤ìº”)"""
        try:
            print(f"\nğŸ” í‘œ ê°ì§€ ì˜ˆë¹„ ìŠ¤ìº” ì‹œì‘ ({len(page_files)}í˜ì´ì§€)...")
            
            table_detection_prompt = """
ğŸ” **í‘œ ê°ì§€ ì „ë¬¸ ì‹œìŠ¤í…œ** - ë¹ ë¥¸ ìŠ¤ìº” ëª¨ë“œ

ì´ í˜ì´ì§€ì—ì„œ í‘œ(table)ì˜ ì¡´ì¬ë¥¼ ê°ì§€í•˜ê³  ìœ„ì¹˜ë¥¼ íŒŒì•…í•´ì£¼ì„¸ìš”.

**ê°ì§€ ëŒ€ìƒ**:
1. ê²©ì í˜•íƒœì˜ í‘œ (ì„ ìœ¼ë¡œ êµ¬ë¶„ëœ ë°ì´í„°)
2. ì •ë ¬ëœ ë°ì´í„° í‘œ (íƒ­ì´ë‚˜ ê³µë°±ìœ¼ë¡œ ì •ë ¬)
3. í”„ë¡œì„¸ìŠ¤ ìŠ¤ì¼€ì¤„ë§ í‘œ (P1, P2, P3 ë“± í¬í•¨)
4. ì‹œê°„í‘œ, ë°ì´í„°í‘œ, ë¹„êµí‘œ ë“±

**ì¶œë ¥ í˜•ì‹**:
í‘œ ê°ì§€ë¨: [ì˜ˆ/ì•„ë‹ˆì˜¤]
í‘œ ê°œìˆ˜: [ìˆ«ì]
í‘œ ìœ í˜•: [í”„ë¡œì„¸ìŠ¤ìŠ¤ì¼€ì¤„ë§/ì‹œê°„í‘œ/ë¹„êµí‘œ/ë°ì´í„°í‘œ/ê¸°íƒ€]
í‘œ ìœ„ì¹˜: [ìƒë‹¨/ì¤‘ë‹¨/í•˜ë‹¨]
ë°ì´í„° í–‰ ì˜ˆìƒ: [P1,P2,P3 ë“± ë°ì´í„° í–‰ì´ ë³´ì´ëŠ”ì§€]

ë§Œì•½ í‘œê°€ ì—†ë‹¤ë©´: "í‘œ ì—†ìŒ"
"""

            table_pages = []
            table_details = {}
            
            for i, page_file in enumerate(page_files):
                try:
                    with open(page_file, "rb") as f:
                        page_base64 = base64.b64encode(f.read()).decode('utf-8')
                    
                    response = await self.openai_client.chat.completions.create(
                        model="gpt-4o",
                        messages=[{
                            "role": "user", 
                            "content": [
                                {"type": "text", "text": table_detection_prompt},
                                {"type": "image_url", "image_url": {"url": f"data:image/png;base64,{page_base64}"}}
                            ]
                        }],
                        max_tokens=500,
                        temperature=0.1
                    )
                    
                    detection_result = response.choices[0].message.content.strip()
                    
                    if "í‘œ ê°ì§€ë¨: ì˜ˆ" in detection_result or "í‘œ ê°œìˆ˜:" in detection_result:
                        table_pages.append(i + 1)
                        table_details[i + 1] = detection_result
                        print(f"ğŸ“Š í˜ì´ì§€ {i+1}: í‘œ ê°ì§€ë¨")
                    else:
                        print(f"ğŸ“„ í˜ì´ì§€ {i+1}: í‘œ ì—†ìŒ")
                    
                    # API ê³¼ë¶€í•˜ ë°©ì§€ ê°•í™”
                    await asyncio.sleep(3)
                    
                except Exception as e:
                    print(f"âš ï¸ í˜ì´ì§€ {i+1} í‘œ ê°ì§€ ì‹¤íŒ¨: {e}")
            
            result = {
                'success': True,
                'table_pages': table_pages,
                'table_details': table_details,
                'total_table_pages': len(table_pages)
            }
            
            print(f"âœ… í‘œ ê°ì§€ ì™„ë£Œ: {len(table_pages)}ê°œ í˜ì´ì§€ì—ì„œ í‘œ ë°œê²¬")
            return result
            
        except Exception as e:
            print(f"âŒ í‘œ ê°ì§€ ì‹œìŠ¤í…œ ì˜¤ë¥˜: {e}")
            return {'success': False, 'error': str(e), 'table_pages': [], 'table_details': {}}
    
    async def _process_page_with_dedicated_table_extraction(self, page_file: str, page_num: int, upload_id: int, table_info: Dict) -> Dict[str, Any]:
        """2ë‹¨ê³„: í‘œê°€ ìˆëŠ” í˜ì´ì§€ ì „ìš© ê³ í•´ìƒë„ í‘œ ì¶”ì¶œ ì‹œìŠ¤í…œ"""
        try:
            print(f"ğŸ”¥ í˜ì´ì§€ {page_num} ì „ìš© í‘œ ì¶”ì¶œ ì‹œìŠ¤í…œ ê°€ë™...")
            
            # ë” ë†’ì€ í•´ìƒë„ë¡œ í˜ì´ì§€ ì¬ìƒì„± (í‘œ ì „ìš©)
            high_res_path = await self._create_ultra_high_res_page(page_file, page_num)
            
            with open(high_res_path, "rb") as f:
                ultra_high_base64 = base64.b64encode(f.read()).decode('utf-8')
            
            table_extraction_prompt = f"""
ğŸ”¥ **ULTRA í‘œ ì¶”ì¶œ ì‹œìŠ¤í…œ** - í˜ì´ì§€ {page_num}

ì´ í˜ì´ì§€ì—ëŠ” í‘œê°€ ê°ì§€ë˜ì—ˆìŠµë‹ˆë‹¤: {table_info.get('table_details', {}).get(page_num, 'ì •ë³´ì—†ìŒ')}

**ğŸ¯ ìµœìš°ì„  ë¯¸ì…˜: P1, P2, P3 ë°ì´í„° í–‰ ì™„ì „ ì¶”ì¶œ**

âš¡ **í‘œ ì¶”ì¶œ ê¸´ê¸‰ í”„ë¡œí† ì½œ**:
1. **í—¤ë” í–‰ ì‹ë³„**: ì²« ë²ˆì§¸ í–‰ì˜ ì»¬ëŸ¼ëª…ë“¤ (í”„ë¡œì„¸ìŠ¤, ë„ì°©ì‹œê°„, ì‹¤í–‰ì‹œê°„ ë“±)
2. **ğŸš¨ ë°ì´í„° í–‰ í•„ìˆ˜ ì¶”ì¶œ**: P1, P2, P3, P4, P5... ëª¨ë“  í”„ë¡œì„¸ìŠ¤ ë°ì´í„°
3. **ëª¨ë“  ì…€ ê°’**: ìˆ«ì ë°ì´í„° (0, 1, 2, 3, 4...) ì •í™•íˆ ì¶”ì¶œ
4. **í‘œ êµ¬ì¡°**: ì •í™•í•œ í–‰Ã—ì—´ ìˆ˜ íŒŒì•…
5. **í‘œ ì™„ì „ì„±**: ëˆ„ë½ëœ í–‰ì´ë‚˜ ì—´ì´ ì—†ëŠ”ì§€ ê²€ì¦

**ğŸ“Š í•„ìˆ˜ ì¶”ì¶œ í˜•ì‹**:
```
**í‘œ ê°ì§€ ê²°ê³¼**: âœ… ë°œê²¬ë¨
**í‘œ ìœ í˜•**: [í”„ë¡œì„¸ìŠ¤ìŠ¤ì¼€ì¤„ë§/ë°ì´í„°í‘œ/ì‹œê°„í‘œ]
**í‘œ í¬ê¸°**: [í–‰ìˆ˜]Ã—[ì—´ìˆ˜] (í—¤ë” í¬í•¨)

| ì»¬ëŸ¼1 | ì»¬ëŸ¼2 | ì»¬ëŸ¼3 | ì»¬ëŸ¼4 |
|--------|--------|--------|--------|
| P1     | 0      | 3      | 1      |
| P2     | 1      | 4      | 2      |
| P3     | 2      | 2      | 3      |
| P4     | 3      | 5      | 4      |

**ë°ì´í„° í–‰ ê²€ì¦**: âœ… P1,P2,P3,P4 ëª¨ë“  ë°ì´í„° ì¶”ì¶œë¨
**ëˆ„ë½ ë°ì´í„°**: [ì—†ìŒ/P5ëˆ„ë½/ê¸°íƒ€]
```

ğŸš¨ **ê¸´ê¸‰ ê²€ì¦ ì²´í¬ë¦¬ìŠ¤íŠ¸**:
- [ ] í‘œ í—¤ë” ì™„ì „ ì¶”ì¶œ (í”„ë¡œì„¸ìŠ¤, ë„ì°©ì‹œê°„, ì‹¤í–‰ì‹œê°„...)
- [ ] P1 ë°ì´í„° í–‰ ì¶”ì¶œ (ëª¨ë“  ì…€ ê°’)  
- [ ] P2 ë°ì´í„° í–‰ ì¶”ì¶œ (ëª¨ë“  ì…€ ê°’)
- [ ] P3 ë°ì´í„° í–‰ ì¶”ì¶œ (ëª¨ë“  ì…€ ê°’)
- [ ] P4, P5... ì¶”ê°€ ë°ì´í„° í–‰ë“¤
- [ ] ëª¨ë“  ìˆ«ì ê°’ ì •í™•ì„± (0, 1, 2, 3, 4...)

**ì‹¤íŒ¨ ì‹œ ëª…ì‹œ**: "â›” í‘œ ë°ì´í„° ë¶ˆì™„ì „ - [êµ¬ì²´ì  ëˆ„ë½ ë‚´ìš©]"

ì´ì œ ì´ í˜ì´ì§€ì˜ ëª¨ë“  ë‚´ìš©ì„ ë¶„ì„í•˜ê³ , íŠ¹íˆ í‘œ ë°ì´í„°ë¥¼ ì™„ë²½í•˜ê²Œ ì¶”ì¶œí•´ì£¼ì„¸ìš”.
"""

            response = await self.openai_client.chat.completions.create(
                model="gpt-4o",
                messages=[{
                    "role": "user",
                    "content": [
                        {"type": "text", "text": table_extraction_prompt},
                        {"type": "image_url", "image_url": {"url": f"data:image/png;base64,{ultra_high_base64}"}}
                    ]
                }],
                max_tokens=4000,
                temperature=0.1
            )
            
            markdown_content = response.choices[0].message.content.strip()
            
            # ì„ì‹œ ê³ í•´ìƒë„ íŒŒì¼ ì‚­ì œ
            if os.path.exists(high_res_path):
                os.unlink(high_res_path)
            
            print(f"ğŸ”¥ í˜ì´ì§€ {page_num} ì „ìš© í‘œ ì¶”ì¶œ ì™„ë£Œ: {len(markdown_content)}chars")
            
            return {
                'success': True,
                'markdown': f"# í˜ì´ì§€ {page_num} (ì „ìš© í‘œ ì¶”ì¶œ)\n\n{markdown_content}",
                'extraction_method': 'dedicated_table_system',
                'table_focused': True
            }
            
        except Exception as e:
            print(f"âŒ í˜ì´ì§€ {page_num} ì „ìš© í‘œ ì¶”ì¶œ ì‹¤íŒ¨: {e}")
            # ì‹¤íŒ¨ ì‹œ ì¼ë°˜ ì²˜ë¦¬ë¡œ fallback
            return await self._process_single_page_with_gpt_vision(page_file, page_num, upload_id)
    
    async def _create_ultra_high_res_page(self, original_page_file: str, page_num: int) -> str:
        """í‘œ ì¶”ì¶œìš© ì´ˆê³ í•´ìƒë„ í˜ì´ì§€ ìƒì„±"""
        try:
            from PIL import Image
            
            # ì›ë³¸ ì´ë¯¸ì§€ ë¡œë“œ
            original_image = Image.open(original_page_file)
            
            # í‘œ ì¶”ì¶œ ìµœì  í•´ìƒë„: 3000px ì´ìƒ
            target_width = 3000
            aspect_ratio = original_image.height / original_image.width
            target_height = int(target_width * aspect_ratio)
            
            # ê³ í•´ìƒë„ë¡œ ë¦¬ì‚¬ì´ì¦ˆ (LANCZOS ì•Œê³ ë¦¬ì¦˜ ì‚¬ìš©)
            ultra_high_image = original_image.resize((target_width, target_height), Image.Resampling.LANCZOS)
            
            # ì„ì‹œ íŒŒì¼ë¡œ ì €ì¥
            ultra_high_path = self.temp_dir / f"ultra_high_page_{page_num}.png"
            ultra_high_image.save(str(ultra_high_path), 'PNG', optimize=True)
            
            print(f"ğŸ“ í˜ì´ì§€ {page_num} ì´ˆê³ í•´ìƒë„ ìƒì„±: {target_width}x{target_height}")
            
            return str(ultra_high_path)
            
        except Exception as e:
            print(f"âš ï¸ ì´ˆê³ í•´ìƒë„ ìƒì„± ì‹¤íŒ¨: {e}, ì›ë³¸ ì‚¬ìš©")
            return original_page_file
    
    async def _extract_tables_directly_from_pdf(self, pdf_path: str, upload_id: int) -> Dict[str, Any]:
        """ğŸš€ ê·¼ë³¸ í•´ê²°ì±…: PDFì—ì„œ ì§ì ‘ í‘œ ì¶”ì¶œ (ì´ë¯¸ì§€ ë³€í™˜ ìš°íšŒ)"""
        try:
            print(f"\nğŸ”„ PDF ì§ì ‘ í‘œ ì¶”ì¶œ ì‹œìŠ¤í…œ ê°€ë™...")
            
            tables_found = []
            extraction_methods_used = []
            
            # ë°©ë²• 1: pdfplumberë¡œ í‘œ êµ¬ì¡° ì§ì ‘ ì¶”ì¶œ
            try:
                import pdfplumber
                
                print("ğŸ“Š ë°©ë²• 1: pdfplumberë¡œ í‘œ êµ¬ì¡° ë¶„ì„...")
                with pdfplumber.open(pdf_path) as pdf:
                    for page_num, page in enumerate(pdf.pages, 1):
                        print(f"   í˜ì´ì§€ {page_num} í‘œ ê²€ìƒ‰...")
                        
                        # í‘œ ìë™ ê°ì§€ ë° ì¶”ì¶œ
                        tables = page.extract_tables()
                        
                        if tables:
                            print(f"   âœ… í˜ì´ì§€ {page_num}: {len(tables)}ê°œ í‘œ ë°œê²¬")
                            
                            for table_idx, table in enumerate(tables):
                                if table and len(table) > 1:  # í—¤ë” + ìµœì†Œ 1ê°œ ë°ì´í„° í–‰
                                    table_data = {
                                        'page_number': page_num,
                                        'table_index': table_idx + 1,
                                        'extraction_method': 'pdfplumber',
                                        'headers': table[0] if table else [],
                                        'data_rows': table[1:] if len(table) > 1 else [],
                                        'total_rows': len(table),
                                        'total_columns': len(table[0]) if table and table[0] else 0,
                                        'raw_table': table
                                    }
                                    
                                    # ğŸ¯ ëª¨ë“  ë°ì´í„° í–‰ ê²€ì¦ (P1,P2,P3ì— êµ­í•œë˜ì§€ ì•ŠìŒ)
                                    data_rows_count = len(table_data['data_rows'])
                                    has_meaningful_data = any(
                                        any(str(cell).strip() for cell in row if cell) 
                                        for row in table_data['data_rows']
                                    )
                                    
                                    if has_meaningful_data:
                                        table_data['completeness_check'] = {
                                            'has_headers': bool(table_data['headers']),
                                            'data_rows_count': data_rows_count,
                                            'has_data': has_meaningful_data,
                                            'status': 'âœ… ì™„ì „í•œ í‘œ (í—¤ë” + ë°ì´í„°)'
                                        }
                                        
                                        tables_found.append(table_data)
                                        print(f"      í‘œ {table_idx+1}: {len(table_data['headers'])}ê°œ ì»¬ëŸ¼, {data_rows_count}ê°œ ë°ì´í„° í–‰")
                                    else:
                                        print(f"      í‘œ {table_idx+1}: ë¹ˆ ë°ì´í„° - ìŠ¤í‚µ")
                        else:
                            print(f"   ğŸ“„ í˜ì´ì§€ {page_num}: í‘œ ì—†ìŒ")
                
                extraction_methods_used.append('pdfplumber')
                print(f"âœ… pdfplumber ì™„ë£Œ: {len([t for t in tables_found if t['extraction_method'] == 'pdfplumber'])}ê°œ í‘œ ì¶”ì¶œ")
                
            except ImportError:
                print("âš ï¸ pdfplumber ì—†ìŒ - pip install pdfplumber í•„ìš”")
            except Exception as e:
                print(f"âš ï¸ pdfplumber ì‹¤íŒ¨: {e}")
            
            # ë°©ë²• 2: PyMuPDFë¡œ í…ìŠ¤íŠ¸ ê¸°ë°˜ í‘œ ì¶”ì¶œ ì‹œë„
            try:
                print("\nğŸ“Š ë°©ë²• 2: PyMuPDF í…ìŠ¤íŠ¸ ê¸°ë°˜ í‘œ ì¶”ì¶œ...")
                doc = fitz.open(pdf_path)
                
                for page_num in range(len(doc)):
                    page = doc.load_page(page_num)
                    text = page.get_text()
                    
                    # í…ìŠ¤íŠ¸ì—ì„œ í‘œ íŒ¨í„´ ê°ì§€
                    lines = text.split('\n')
                    potential_table_lines = []
                    
                    for line in lines:
                        # í‘œ ê°™ì€ êµ¬ì¡° ê°ì§€ (ì—¬ëŸ¬ ê°œì˜ íƒ­/ê³µë°±ìœ¼ë¡œ êµ¬ë¶„ëœ ë°ì´í„°)
                        if '\t' in line or len(line.split()) >= 3:
                            # ìˆ«ìë‚˜ íŠ¹ì • íŒ¨í„´ì´ í¬í•¨ëœ ì¤„
                            if any(char.isdigit() for char in line):
                                potential_table_lines.append(line.strip())
                    
                    if len(potential_table_lines) >= 2:  # ìµœì†Œ í—¤ë” + 1ê°œ ë°ì´í„° í–‰
                        print(f"   âœ… í˜ì´ì§€ {page_num+1}: í…ìŠ¤íŠ¸ ê¸°ë°˜ í‘œ íŒ¨í„´ {len(potential_table_lines)}ê°œ í–‰ ë°œê²¬")
                        
                        # ê°„ë‹¨í•œ í‘œ êµ¬ì¡°í™”
                        table_rows = []
                        for line in potential_table_lines:
                            if '\t' in line:
                                row = line.split('\t')
                            else:
                                row = line.split()
                            table_rows.append([cell.strip() for cell in row if cell.strip()])
                        
                        if table_rows and len(table_rows) > 1:
                            table_data = {
                                'page_number': page_num + 1,
                                'table_index': 1,
                                'extraction_method': 'pymupdf_text',
                                'headers': table_rows[0] if table_rows else [],
                                'data_rows': table_rows[1:] if len(table_rows) > 1 else [],
                                'total_rows': len(table_rows),
                                'total_columns': len(table_rows[0]) if table_rows and table_rows[0] else 0,
                                'raw_table': table_rows,
                                'completeness_check': {
                                    'has_headers': bool(table_rows[0]) if table_rows else False,
                                    'data_rows_count': len(table_rows) - 1 if len(table_rows) > 1 else 0,
                                    'has_data': len(table_rows) > 1,
                                    'status': 'âœ… í…ìŠ¤íŠ¸ ê¸°ë°˜ í‘œ ì¶”ì¶œ'
                                }
                            }
                            
                            # ì¤‘ë³µ ë°©ì§€ (ì´ë¯¸ pdfplumberë¡œ ì¶”ì¶œëœ ê²ƒê³¼ ë¹„êµ)
                            is_duplicate = any(
                                t['page_number'] == table_data['page_number'] and 
                                t['extraction_method'] == 'pdfplumber'
                                for t in tables_found
                            )
                            
                            if not is_duplicate:
                                tables_found.append(table_data)
                
                extraction_methods_used.append('pymupdf_text')
                print(f"âœ… PyMuPDF í…ìŠ¤íŠ¸ ì™„ë£Œ: {len([t for t in tables_found if t['extraction_method'] == 'pymupdf_text'])}ê°œ ì¶”ê°€ í‘œ")
                
                doc.close()
                
            except Exception as e:
                print(f"âš ï¸ PyMuPDF í…ìŠ¤íŠ¸ ì¶”ì¶œ ì‹¤íŒ¨: {e}")
            
            # ê²°ê³¼ ì •ë¦¬
            result = {
                'success': len(tables_found) > 0,
                'tables_count': len(tables_found),
                'tables_data': tables_found,
                'extraction_methods': extraction_methods_used,
                'summary': {
                    'total_tables': len(tables_found),
                    'pages_with_tables': len(set(t['page_number'] for t in tables_found)),
                    'by_method': {method: len([t for t in tables_found if t['extraction_method'] == method]) for method in set(t['extraction_method'] for t in tables_found)}
                }
            }
            
            print(f"\nğŸ¯ PDF ì§ì ‘ í‘œ ì¶”ì¶œ ê²°ê³¼:")
            print(f"   ì´ {result['tables_count']}ê°œ í‘œ ì¶”ì¶œ")
            print(f"   {result['summary']['pages_with_tables']}ê°œ í˜ì´ì§€ì—ì„œ ë°œê²¬")
            print(f"   ì‚¬ìš©ëœ ë°©ë²•: {', '.join(extraction_methods_used)}")
            
            for table in tables_found:
                print(f"   ğŸ“Š í˜ì´ì§€ {table['page_number']} í‘œ {table['table_index']}: "
                      f"{table['total_columns']}ì»¬ëŸ¼ Ã— {len(table['data_rows'])}ë°ì´í„°í–‰ "
                      f"({table['extraction_method']})")
            
            return result
            
        except Exception as e:
            print(f"âŒ PDF ì§ì ‘ í‘œ ì¶”ì¶œ ì‹¤íŒ¨: {e}")
            return {
                'success': False,
                'error': str(e),
                'tables_count': 0,
                'tables_data': [],
                'extraction_methods': []
            }
    
    def _validate_table_completeness(self, passage: str, question_number) -> Dict[str, Any]:
        """í‘œ ë°ì´í„° ì™„ì „ì„± ê²€ì¦ - P1, P2, P3 ë“± ë°ì´í„° í–‰ í™•ì¸"""
        import re
        
        validation_result = {
            'is_complete': True,
            'issues': [],
            'data_rows_found': [],
            'header_found': False
        }
        
        # í‘œê°€ í¬í•¨ëœ ì§€ë¬¸ì¸ì§€ í™•ì¸
        if '|' not in passage and 'í‘œ' not in passage:
            return validation_result
        
        lines = passage.split('\n')
        table_lines = [line.strip() for line in lines if '|' in line]
        
        if not table_lines:
            validation_result['is_complete'] = False
            validation_result['issues'].append('í‘œ í˜•ì‹ì´ ê°ì§€ë˜ì§€ ì•ŠìŒ')
            return validation_result
        
        # í—¤ë” í–‰ ê°ì§€ (ì²« ë²ˆì§¸ í‘œ ë¼ì¸)
        header_patterns = [
            r'í”„ë¡œì„¸ìŠ¤|process',
            r'ë„ì°©ì‹œê°„|arrival',
            r'ì‹¤í–‰ì‹œê°„|burst',
            r'ìš°ì„ ìˆœìœ„|priority',
            r'í—¤ë”|header',
            r'í•­ëª©|item',
            r'êµ¬ë¶„|classification'
        ]
        
        first_table_line = table_lines[0].lower()
        validation_result['header_found'] = any(re.search(pattern, first_table_line) for pattern in header_patterns)
        
        # ë°ì´í„° í–‰ íŒ¨í„´ ê²€ì¦ (ì¼ë°˜ì ì¸ íŒ¨í„´)
        data_row_patterns = [
            r'[a-zA-Z]+[0-9]+',    # A1, P1, Task1 ë“±
            r'[ê°€-í£]+\s*[0-9]+',   # í”„ë¡œì„¸ìŠ¤1, ì‘ì—…1 ë“±
            r'[0-9]+[.]?',         # 1, 2, 3... ë˜ëŠ” 1., 2., 3...
            r'[ê°€-í£]{1,3}[0-9]+', # ê°€1, ë‚˜1, í•­ëª©1 ë“±
        ]
        
        data_rows_found = []
        for line in table_lines[1:]:  # í—¤ë” ì œì™¸í•œ ë‚˜ë¨¸ì§€ ë¼ì¸
            line_lower = line.lower()
            for pattern in data_row_patterns:
                matches = re.findall(pattern, line_lower)
                for match in matches:
                    if match not in data_rows_found:
                        data_rows_found.append(match)
        
        validation_result['data_rows_found'] = data_rows_found
        
        # ì™„ì „ì„± ê²€ì¦
        if validation_result['header_found'] and len(data_rows_found) == 0:
            validation_result['is_complete'] = False
            validation_result['issues'].append('í—¤ë”ë§Œ ìˆê³  ë°ì´í„° í–‰ ì—†ìŒ')
        elif len(data_rows_found) < 2:
            validation_result['is_complete'] = False
            validation_result['issues'].append(f'ë°ì´í„° í–‰ ë¶€ì¡±: {len(data_rows_found)}ê°œ ë°œê²¬')
        elif not validation_result['header_found']:
            validation_result['issues'].append('í‘œ í—¤ë” ë¶ˆë¶„ëª…')
        
        return validation_result
    
    async def _ocr_based_enhancement_pipeline(self, upload_id: int, basic_questions: List[Dict], 
                                            pdf_path: str, db: Session) -> List[Dict]:
        """ğŸ” OCR ê¸°ë°˜ ë³´ì™„ ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸"""
        try:
            print(f"\nğŸ” OCR ê¸°ë°˜ ë³´ì™„ íŒŒì´í”„ë¼ì¸ ì‹œì‘...")
            
            # 1ë‹¨ê³„: PDF ì „ì²´ OCR ì²˜ë¦¬
            ocr_data = await self._process_pdf_with_ocr(pdf_path, upload_id, db)
            if not ocr_data['success']:
                print("âš ï¸ OCR ì²˜ë¦¬ ì‹¤íŒ¨, ê¸°ë³¸ ê²°ê³¼ ìœ ì§€")
                return basic_questions
            
            print(f"âœ… OCR ì²˜ë¦¬ ì™„ë£Œ: {len(ocr_data['pages'])}í˜ì´ì§€")
            
            # 2ë‹¨ê³„: OCR vs ê¸°ì¡´ ê²°ê³¼ ë¹„êµ ë¶„ì„
            gap_analysis = await self._analyze_content_gaps(basic_questions, ocr_data, upload_id, db)
            print(f"ğŸ“Š ê²©ì°¨ ë¶„ì„ ì™„ë£Œ: {len(gap_analysis['missing_questions'])}ê°œ ëˆ„ë½ ë¬¸ì œ ë°œê²¬")
            
            # 3ë‹¨ê³„: ëˆ„ë½ëœ ë¬¸ì œ ë³µêµ¬
            recovered_questions = await self._recover_missing_questions_from_ocr(
                gap_analysis['missing_questions'], ocr_data, upload_id, db
            )
            print(f"ğŸ”§ OCR ê¸°ë°˜ ë³µêµ¬: {len(recovered_questions)}ê°œ ë¬¸ì œ ë³µêµ¬")
            
            # 4ë‹¨ê³„: í˜ì´ì§€ ê²½ê³„ ë¬¸ì œ OCR ê¸°ë°˜ í•´ê²°
            boundary_fixed_questions = await self._fix_boundary_issues_with_ocr(
                basic_questions, ocr_data, upload_id, db
            )
            print(f"ğŸ”— í˜ì´ì§€ ê²½ê³„ ë¬¸ì œ ìˆ˜ì •: {len([q for q in boundary_fixed_questions if q.get('boundary_fixed')])}ê°œ")
            
            # 5ë‹¨ê³„: ìµœì¢… í†µí•© ë° í’ˆì§ˆ ê²€ì¦
            final_questions = boundary_fixed_questions + recovered_questions
            final_questions = await self._final_ocr_quality_validation(final_questions, ocr_data, db)
            
            print(f"âœ… OCR ê¸°ë°˜ ë³´ì™„ ì™„ë£Œ: {len(final_questions)}ê°œ ë¬¸ì œ (ê¸°ì¡´: {len(basic_questions)})")
            return final_questions
            
        except Exception as e:
            print(f"âš ï¸ OCR ë³´ì™„ íŒŒì´í”„ë¼ì¸ ì˜¤ë¥˜: {e}")
            return basic_questions
    
    async def _process_pdf_with_ocr(self, pdf_path: str, upload_id: int, db: Session) -> Dict:
        """PDF ì „ì²´ OCR ì²˜ë¦¬"""
        try:
            import fitz  # PyMuPDF
            import pytesseract
            from PIL import Image
            import io
            
            print(f"ğŸ“„ PDF OCR ì²˜ë¦¬ ì‹œì‘: {pdf_path}")
            
            # PDF ì—´ê¸°
            doc = fitz.open(pdf_path)
            ocr_pages = []
            
            for page_num in range(len(doc)):
                page = doc.load_page(page_num)
                
                # ê³ í•´ìƒë„ ì´ë¯¸ì§€ë¡œ ë³€í™˜
                mat = fitz.Matrix(3.0, 3.0)  # 3x í™•ëŒ€
                pix = page.get_pixmap(matrix=mat)
                img_data = pix.tobytes("png")
                
                # PIL Imageë¡œ ë³€í™˜
                pil_image = Image.open(io.BytesIO(img_data))
                
                # Tesseract OCR ì‹¤í–‰
                print(f"   ğŸ” í˜ì´ì§€ {page_num + 1} OCR ì²˜ë¦¬ ì¤‘...")
                try:
                    ocr_text = pytesseract.image_to_string(pil_image, lang='kor+eng', config='--psm 6')
                    
                    # ì¢Œí‘œ ì •ë³´ì™€ í•¨ê»˜ ì¶”ì¶œ
                    ocr_data = pytesseract.image_to_data(pil_image, lang='kor+eng', output_type=pytesseract.Output.DICT)
                    
                    # êµ¬ì¡°í™”ëœ ë°ì´í„° ìƒì„±
                    structured_text = self._structure_ocr_data(ocr_data, ocr_text)
                    
                    ocr_pages.append({
                        'page_number': page_num + 1,
                        'raw_text': ocr_text,
                        'structured_data': structured_text,
                        'confidence_avg': self._calculate_ocr_confidence(ocr_data)
                    })
                    
                except Exception as ocr_error:
                    print(f"   âš ï¸ í˜ì´ì§€ {page_num + 1} OCR ì‹¤íŒ¨: {ocr_error}")
                    ocr_pages.append({
                        'page_number': page_num + 1,
                        'raw_text': '',
                        'structured_data': {},
                        'confidence_avg': 0
                    })
            
            doc.close()
            
            return {
                'success': True,
                'pages': ocr_pages,
                'total_pages': len(ocr_pages)
            }
            
        except ImportError:
            print("âš ï¸ OCR ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ì„¤ì¹˜ë˜ì§€ ì•ŠìŒ (pytesseract, Pillow í•„ìš”)")
            return {'success': False, 'error': 'OCR libraries not installed'}
        except Exception as e:
            print(f"âŒ OCR ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
            return {'success': False, 'error': str(e)}
    
    def _structure_ocr_data(self, ocr_data: Dict, raw_text: str) -> Dict:
        """OCR ë°ì´í„°ë¥¼ êµ¬ì¡°í™”"""
        import re
        
        # ë¬¸ì œ íŒ¨í„´ ê°ì§€
        questions = []
        question_patterns = [
            r'(\d+)\s*[.)]\s*([^\n]+)',  # ìˆ«ì. ë˜ëŠ” ìˆ«ì) í˜•íƒœ
            r'ë¬¸ì œ\s*(\d+)\s*ë²ˆ\s*([^\n]+)',  # ë¬¸ì œ Xë²ˆ í˜•íƒœ
        ]
        
        for pattern in question_patterns:
            matches = re.finditer(pattern, raw_text, re.MULTILINE)
            for match in matches:
                questions.append({
                    'number': int(match.group(1)),
                    'text': match.group(2).strip(),
                    'position': match.span()
                })
        
        # ì„ íƒì§€ íŒ¨í„´ ê°ì§€
        choices = []
        choice_patterns = [
            r'[â‘ â‘¡â‘¢â‘£â‘¤]\s*([^\nâ‘ â‘¡â‘¢â‘£â‘¤]+)',
            r'[1-5][.)]\s*([^\n1-5]+)',
            r'[ê°€ë‚˜ë‹¤ë¼ë§ˆ]\s*[.)]\s*([^\nê°€ë‚˜ë‹¤ë¼ë§ˆ]+)'
        ]
        
        for pattern in choice_patterns:
            matches = re.finditer(pattern, raw_text, re.MULTILINE)
            for match in matches:
                choices.append({
                    'text': match.group(1).strip(),
                    'position': match.span()
                })
        
        # í‘œ íŒ¨í„´ ê°ì§€
        tables = []
        table_indicators = ['|', 'â”€', 'â”Œ', 'â”œ', 'â””', 'â”¬', 'â”¤', 'â”', 'â”˜', 'â”´']
        lines = raw_text.split('\n')
        
        for i, line in enumerate(lines):
            if any(indicator in line for indicator in table_indicators):
                # í‘œ ì˜ì—­ í™•ì¥ ê°ì§€
                table_start = max(0, i - 2)
                table_end = min(len(lines), i + 5)
                table_text = '\n'.join(lines[table_start:table_end])
                
                if len(table_text.strip()) > 10:  # ìµœì†Œ ê¸¸ì´ í•„í„°
                    tables.append({
                        'text': table_text,
                        'line_range': (table_start, table_end),
                        'confidence': 'high' if line.count('|') > 2 else 'medium'
                    })
        
        return {
            'questions': questions,
            'choices': choices,
            'tables': tables,
            'total_text_length': len(raw_text)
        }
    
    def _calculate_ocr_confidence(self, ocr_data: Dict) -> float:
        """OCR ì‹ ë¢°ë„ ê³„ì‚°"""
        confidences = [int(conf) for conf in ocr_data.get('conf', []) if int(conf) > 0]
        return sum(confidences) / len(confidences) if confidences else 0.0
    
    async def _analyze_content_gaps(self, basic_questions: List[Dict], ocr_data: Dict, upload_id: int, db: Session) -> Dict:
        """OCR ê²°ê³¼ì™€ ê¸°ë³¸ ì¶”ì¶œ ê²°ê³¼ ë¹„êµ ë¶„ì„"""
        try:
            print(f"ğŸ“Š ì½˜í…ì¸  ê²©ì°¨ ë¶„ì„ ì‹œì‘...")
            
            # ê¸°ë³¸ ì¶”ì¶œì—ì„œ ì°¾ì€ ë¬¸ì œ ë²ˆí˜¸ë“¤
            extracted_numbers = set()
            for question in basic_questions:
                if 'question_number' in question:
                    extracted_numbers.add(question['question_number'])
                elif 'id' in question:
                    extracted_numbers.add(question['id'])
            
            print(f"   ê¸°ë³¸ ì¶”ì¶œ ë¬¸ì œ ë²ˆí˜¸: {sorted(extracted_numbers)}")
            
            # OCRì—ì„œ ì°¾ì€ ëª¨ë“  ë¬¸ì œ ë²ˆí˜¸ë“¤
            ocr_numbers = set()
            all_ocr_questions = []
            
            for page_data in ocr_data.get('pages', []):
                structured = page_data.get('structured_data', {})
                questions = structured.get('questions', [])
                
                for q in questions:
                    number = q.get('number')
                    if number and isinstance(number, int) and 1 <= number <= 200:
                        ocr_numbers.add(number)
                        all_ocr_questions.append({
                            'number': number,
                            'text': q.get('text', ''),
                            'page': page_data.get('page_number', 0),
                            'confidence': page_data.get('confidence_avg', 0)
                        })
            
            print(f"   OCR ë°œê²¬ ë¬¸ì œ ë²ˆí˜¸: {sorted(ocr_numbers)}")
            
            # ëˆ„ë½ëœ ë¬¸ì œë“¤ ì‹ë³„
            missing_numbers = ocr_numbers - extracted_numbers
            extra_numbers = extracted_numbers - ocr_numbers
            
            missing_questions = []
            for q in all_ocr_questions:
                if q['number'] in missing_numbers and q['confidence'] > 30:  # ì‹ ë¢°ë„ í•„í„°
                    missing_questions.append(q)
            
            print(f"   ğŸ“‹ ë¶„ì„ ê²°ê³¼:")
            print(f"      - ëˆ„ë½ëœ ë¬¸ì œ: {sorted(missing_numbers)} ({len(missing_questions)}ê°œ ë³µêµ¬ ê°€ëŠ¥)")
            print(f"      - ì¶”ê°€ ë°œê²¬: {sorted(extra_numbers)}")
            
            return {
                'missing_questions': missing_questions,
                'extra_numbers': extra_numbers,
                'ocr_total': len(ocr_numbers),
                'extracted_total': len(extracted_numbers),
                'coverage_ratio': len(extracted_numbers) / len(ocr_numbers) if ocr_numbers else 0
            }
            
        except Exception as e:
            print(f"âŒ ê²©ì°¨ ë¶„ì„ ì˜¤ë¥˜: {e}")
            return {'missing_questions': [], 'extra_numbers': set(), 'coverage_ratio': 0}
    
    async def _recover_missing_questions_from_ocr(self, missing_questions: List[Dict], ocr_data: Dict, upload_id: int, db: Session) -> List[Dict]:
        """OCR ë°ì´í„°ì—ì„œ ëˆ„ë½ëœ ë¬¸ì œë“¤ ë³µêµ¬"""
        try:
            if not missing_questions:
                return []
                
            print(f"ğŸ”§ OCR ê¸°ë°˜ ë¬¸ì œ ë³µêµ¬ ì‹œì‘: {len(missing_questions)}ê°œ ë¬¸ì œ")
            
            recovered_questions = []
            
            for missing_q in missing_questions:
                question_number = missing_q['number']
                page_number = missing_q['page']
                
                # í•´ë‹¹ í˜ì´ì§€ì˜ OCR ë°ì´í„° ì°¾ê¸°
                page_data = None
                for page in ocr_data.get('pages', []):
                    if page.get('page_number') == page_number:
                        page_data = page
                        break
                
                if not page_data:
                    continue
                
                # ë¬¸ì œ í…ìŠ¤íŠ¸ ì£¼ë³€ ì»¨í…ìŠ¤íŠ¸ ì¶”ì¶œ
                raw_text = page_data.get('raw_text', '')
                question_text = missing_q.get('text', '')
                
                # ì„ íƒì§€ ì°¾ê¸° (ë¬¸ì œ í…ìŠ¤íŠ¸ ë’¤ì˜ ë‚´ìš©ì—ì„œ)
                choices = self._extract_choices_from_context(raw_text, question_text, question_number)
                
                # í…Œì´ë¸”/ì´ë¯¸ì§€ ì»¨í…ìŠ¤íŠ¸ í™•ì¸
                has_table = self._detect_table_context(raw_text, question_text)
                
                recovered_question = {
                    'question_number': question_number,
                    'question_text': question_text,
                    'choices': choices,
                    'has_table': has_table,
                    'recovery_source': 'ocr',
                    'confidence': missing_q.get('confidence', 0),
                    'page_number': page_number,
                    'passage': self._extract_passage_context(raw_text, question_text)
                }
                
                recovered_questions.append(recovered_question)
                print(f"   âœ… ë¬¸ì œ {question_number}ë²ˆ ë³µêµ¬ë¨ (ì„ íƒì§€ {len(choices)}ê°œ)")
            
            print(f"ğŸ¯ OCR ë³µêµ¬ ì™„ë£Œ: {len(recovered_questions)}/{len(missing_questions)}ê°œ ì„±ê³µ")
            return recovered_questions
            
        except Exception as e:
            print(f"âŒ OCR ë³µêµ¬ ì˜¤ë¥˜: {e}")
            return []
    
    def _extract_choices_from_context(self, text: str, question_text: str, question_number: int) -> List[Dict]:
        """í…ìŠ¤íŠ¸ì—ì„œ íŠ¹ì • ë¬¸ì œì˜ ì„ íƒì§€ ì¶”ì¶œ"""
        import re
        
        try:
            # ë¬¸ì œ ìœ„ì¹˜ ì°¾ê¸°
            question_pos = text.find(question_text)
            if question_pos == -1:
                # ë¶€ë¶„ ë§¤ì¹­ ì‹œë„
                words = question_text.split()[:5]  # ì²« 5ë‹¨ì–´ë¡œ ì°¾ê¸°
                for word in words:
                    if len(word) > 2:
                        question_pos = text.find(word)
                        if question_pos != -1:
                            break
            
            if question_pos == -1:
                return []
            
            # ë¬¸ì œ ë’¤ì˜ í…ìŠ¤íŠ¸ì—ì„œ ì„ íƒì§€ ì°¾ê¸°
            after_question = text[question_pos + len(question_text):question_pos + len(question_text) + 1000]
            
            choices = []
            choice_patterns = [
                r'â‘ \s*([^\nâ‘¡â‘¢â‘£â‘¤]+)',
                r'â‘¡\s*([^\nâ‘ â‘¢â‘£â‘¤]+)', 
                r'â‘¢\s*([^\nâ‘ â‘¡â‘£â‘¤]+)',
                r'â‘£\s*([^\nâ‘ â‘¡â‘¢â‘¤]+)',
                r'â‘¤\s*([^\nâ‘ â‘¡â‘¢â‘£]+)'
            ]
            
            for i, pattern in enumerate(choice_patterns):
                match = re.search(pattern, after_question)
                if match:
                    choice_text = match.group(1).strip()
                    if len(choice_text) > 1:  # ìµœì†Œ ê¸¸ì´ í•„í„°
                        choices.append({
                            'choice_number': i + 1,
                            'choice_text': choice_text,
                            'has_image': self._detect_image_reference(choice_text)
                        })
            
            return choices[:5]  # ìµœëŒ€ 5ê°œê¹Œì§€
            
        except Exception as e:
            print(f"   âš ï¸ ì„ íƒì§€ ì¶”ì¶œ ì˜¤ë¥˜: {e}")
            return []
    
    def _detect_image_reference(self, text: str) -> bool:
        """í…ìŠ¤íŠ¸ì—ì„œ ì´ë¯¸ì§€ ì°¸ì¡° ê°ì§€"""
        image_keywords = ['ê·¸ë¦¼', 'í‘œ', 'ë„í‘œ', 'ì°¨íŠ¸', 'ê·¸ë˜í”„', 'ì´ë¯¸ì§€', 'ì•„ë˜', 'ìœ„']
        return any(keyword in text for keyword in image_keywords)
    
    def _detect_table_context(self, text: str, question_text: str) -> bool:
        """ë¬¸ì œì— í…Œì´ë¸”/í‘œ ì»¨í…ìŠ¤íŠ¸ê°€ ìˆëŠ”ì§€ ê°ì§€"""
        context_window = 500
        question_pos = text.find(question_text)
        if question_pos == -1:
            return False
        
        context = text[max(0, question_pos-context_window):question_pos+context_window]
        table_indicators = ['|', 'â”€', 'â”Œ', 'â”œ', 'í‘œ', 'í…Œì´ë¸”', 'P1', 'P2', 'P3']
        
        return sum(1 for indicator in table_indicators if indicator in context) >= 2
    
    def _extract_passage_context(self, text: str, question_text: str) -> str:
        """ë¬¸ì œ ì£¼ë³€ ì»¨í…ìŠ¤íŠ¸ ì¶”ì¶œ"""
        question_pos = text.find(question_text)
        if question_pos == -1:
            return ""
        
        # ì•ë’¤ 200ìì”© ì¶”ì¶œ
        start = max(0, question_pos - 200)
        end = min(len(text), question_pos + len(question_text) + 200)
        
        return text[start:end].strip()
    
    async def _fix_boundary_issues_with_ocr(self, questions: List[Dict], ocr_data: Dict, upload_id: int, db: Session) -> List[Dict]:
        """OCRì„ í™œìš©í•˜ì—¬ í˜ì´ì§€ ê²½ê³„ ë¬¸ì œ í•´ê²°"""
        try:
            print(f"ğŸ”— í˜ì´ì§€ ê²½ê³„ ë¬¸ì œ OCR ê¸°ë°˜ ìˆ˜ì • ì‹œì‘...")
            
            fixed_questions = []
            boundary_fixes = 0
            
            for question in questions:
                question_copy = question.copy()
                
                # ë¶ˆì™„ì „í•œ ì„ íƒì§€ ê°ì§€ (options í•„ë“œ ì‚¬ìš©)
                choices = question.get('options', question.get('choices', []))
                if choices and len(choices) < 4:  # ì¼ë°˜ì ìœ¼ë¡œ 4-5ê°œ ì„ íƒì§€ ì˜ˆìƒ
                    
                    question_number = question.get('question_number', question.get('id', 0))
                    
                    # OCRì—ì„œ í•´ë‹¹ ë¬¸ì œì˜ ì™„ì „í•œ ì„ íƒì§€ ì°¾ê¸°
                    complete_choices = self._find_complete_choices_in_ocr(question_number, ocr_data)
                    
                    if len(complete_choices) > len(choices):
                        print(f"   ğŸ”§ ë¬¸ì œ {question_number}ë²ˆ ì„ íƒì§€ ë³´ì™„: {len(choices)} â†’ {len(complete_choices)}ê°œ")
                        question_copy['options'] = complete_choices
                        # í˜¸í™˜ì„±ì„ ìœ„í•´ choicesë„ ì„¤ì •
                        question_copy['choices'] = complete_choices
                        question_copy['boundary_fixed'] = True
                        boundary_fixes += 1
                
                # ë¶ˆì™„ì „í•œ í‘œ ë°ì´í„° ë³´ì™„
                passage = question.get('passage', '')
                if '|' in passage and passage.count('|') < 6:  # ë¶ˆì™„ì „í•œ í‘œ ì¶”ì •
                    
                    question_number = question.get('question_number', question.get('id', 0))
                    complete_table = self._find_complete_table_in_ocr(question_number, ocr_data)
                    
                    if complete_table and len(complete_table) > len(passage):
                        print(f"   ğŸ“Š ë¬¸ì œ {question_number}ë²ˆ í‘œ ë°ì´í„° ë³´ì™„")
                        question_copy['passage'] = complete_table
                        question_copy['boundary_fixed'] = True
                        boundary_fixes += 1
                
                fixed_questions.append(question_copy)
            
            print(f"âœ… í˜ì´ì§€ ê²½ê³„ ë¬¸ì œ ìˆ˜ì • ì™„ë£Œ: {boundary_fixes}ê°œ ë¬¸ì œ ìˆ˜ì •")
            return fixed_questions
            
        except Exception as e:
            print(f"âŒ í˜ì´ì§€ ê²½ê³„ ìˆ˜ì • ì˜¤ë¥˜: {e}")
            return questions
    
    def _find_complete_choices_in_ocr(self, question_number: int, ocr_data: Dict) -> List[Dict]:
        """OCRì—ì„œ íŠ¹ì • ë¬¸ì œì˜ ì™„ì „í•œ ì„ íƒì§€ ì°¾ê¸°"""
        try:
            for page_data in ocr_data.get('pages', []):
                raw_text = page_data.get('raw_text', '')
                
                # ë¬¸ì œ ë²ˆí˜¸ íŒ¨í„´ìœ¼ë¡œ í•´ë‹¹ ë¬¸ì œ ì˜ì—­ ì°¾ê¸°
                import re
                question_patterns = [
                    rf'{question_number}\s*[.)]\s*([^\n]+)',
                    rf'ë¬¸ì œ\s*{question_number}\s*ë²ˆ'
                ]
                
                question_pos = -1
                for pattern in question_patterns:
                    match = re.search(pattern, raw_text)
                    if match:
                        question_pos = match.start()
                        break
                
                if question_pos == -1:
                    continue
                
                # ë¬¸ì œ ë’¤ì˜ í…ìŠ¤íŠ¸ì—ì„œ ëª¨ë“  ì„ íƒì§€ ì¶”ì¶œ
                after_question = raw_text[question_pos:question_pos + 1500]
                choices = []
                
                choice_patterns = [
                    r'â‘ \s*([^\nâ‘¡â‘¢â‘£â‘¤]+)',
                    r'â‘¡\s*([^\nâ‘ â‘¢â‘£â‘¤]+)', 
                    r'â‘¢\s*([^\nâ‘ â‘¡â‘£â‘¤]+)',
                    r'â‘£\s*([^\nâ‘ â‘¡â‘¢â‘¤]+)',
                    r'â‘¤\s*([^\nâ‘ â‘¡â‘¢â‘£]+)'
                ]
                
                for i, pattern in enumerate(choice_patterns):
                    match = re.search(pattern, after_question)
                    if match:
                        choice_text = match.group(1).strip()
                        if len(choice_text) > 1:
                            choices.append({
                                'choice_number': i + 1,
                                'choice_text': choice_text,
                                'has_image': self._detect_image_reference(choice_text)
                            })
                
                if len(choices) >= 4:  # ì¶©ë¶„í•œ ì„ íƒì§€ê°€ ìˆìœ¼ë©´ ë°˜í™˜
                    return choices
            
            return []
            
        except Exception as e:
            print(f"   âš ï¸ OCR ì„ íƒì§€ ì°¾ê¸° ì˜¤ë¥˜: {e}")
            return []
    
    def _find_complete_table_in_ocr(self, question_number: int, ocr_data: Dict) -> str:
        """OCRì—ì„œ íŠ¹ì • ë¬¸ì œì˜ ì™„ì „í•œ í‘œ ë°ì´í„° ì°¾ê¸°"""
        try:
            for page_data in ocr_data.get('pages', []):
                structured = page_data.get('structured_data', {})
                tables = structured.get('tables', [])
                
                # ë†’ì€ ì‹ ë¢°ë„ì˜ í‘œ ì¤‘ì—ì„œ ê°€ì¥ ì™„ì „í•œ ê²ƒ ì„ íƒ
                best_table = ""
                max_score = 0
                
                for table in tables:
                    table_text = table.get('text', '')
                    
                    # í‘œ ì™„ì„±ë„ ì ìˆ˜ ê³„ì‚°
                    score = 0
                    score += table_text.count('|') * 2  # êµ¬ë¶„ì
                    score += table_text.count('P1') + table_text.count('P2') + table_text.count('P3')  # í”„ë¡œì„¸ìŠ¤ ë°ì´í„°
                    score += len(re.findall(r'\d+', table_text))  # ìˆ«ì ë°ì´í„°
                    
                    if score > max_score and len(table_text) > 50:
                        max_score = score
                        best_table = table_text
                
                if best_table and max_score > 10:
                    return best_table
            
            return ""
            
        except Exception as e:
            print(f"   âš ï¸ OCR í‘œ ì°¾ê¸° ì˜¤ë¥˜: {e}")
            return ""
    
    async def _final_ocr_quality_validation(self, questions: List[Dict], ocr_data: Dict, db: Session) -> List[Dict]:
        """ìµœì¢… OCR ê¸°ë°˜ í’ˆì§ˆ ê²€ì¦"""
        try:
            print(f"âœ¨ ìµœì¢… OCR í’ˆì§ˆ ê²€ì¦ ì‹œì‘: {len(questions)}ê°œ ë¬¸ì œ")
            
            validated_questions = []
            
            for question in questions:
                question_copy = question.copy()
                
                # ê¸°ë³¸ ê²€ì¦
                question_number = question.get('question_number', question.get('id', 0))
                question_text = question.get('question_text', question.get('question', ''))
                choices = question.get('options', question.get('choices', []))
                
                if not question_text or len(question_text.strip()) < 5:
                    print(f"   âš ï¸ ë¬¸ì œ {question_number}ë²ˆ: ë¬¸ì œ í…ìŠ¤íŠ¸ ë¶€ì¡±")
                    continue
                
                if len(choices) < 2:
                    print(f"   âš ï¸ ë¬¸ì œ {question_number}ë²ˆ: ì„ íƒì§€ ë¶€ì¡± ({len(choices)}ê°œ)")
                    # OCRì—ì„œ ì„ íƒì§€ ë³´ì™„ ì‹œë„
                    additional_choices = self._find_complete_choices_in_ocr(question_number, ocr_data)
                    if additional_choices:
                        question_copy['options'] = additional_choices
                        question_copy['choices'] = additional_choices
                        print(f"   âœ… ë¬¸ì œ {question_number}ë²ˆ: OCRë¡œ ì„ íƒì§€ ë³´ì™„ë¨")
                
                # ì¤‘ë³µ ì œê±°
                if question_number > 0:
                    # ì´ë¯¸ ê°™ì€ ë²ˆí˜¸ì˜ ë¬¸ì œê°€ ìˆëŠ”ì§€ í™•ì¸
                    duplicate = False
                    for existing in validated_questions:
                        existing_number = existing.get('question_number', existing.get('id', 0))
                        if existing_number == question_number:
                            # ë” ì™„ì „í•œ ë²„ì „ ì„ íƒ (options ìš°ì„  ì‚¬ìš©)
                            existing_choices = len(existing.get('options', existing.get('choices', [])))
                            current_choices = len(question_copy.get('options', question_copy.get('choices', [])))
                            
                            if current_choices > existing_choices:
                                # í˜„ì¬ ë²„ì „ì´ ë” ì™„ì „í•¨ - ê¸°ì¡´ ê²ƒ êµì²´
                                for i, eq in enumerate(validated_questions):
                                    if eq.get('question_number', eq.get('id', 0)) == question_number:
                                        validated_questions[i] = question_copy
                                        print(f"   ğŸ”„ ë¬¸ì œ {question_number}ë²ˆ: ë” ì™„ì „í•œ ë²„ì „ìœ¼ë¡œ êµì²´")
                                        break
                            duplicate = True
                            break
                    
                    if not duplicate:
                        validated_questions.append(question_copy)
                else:
                    validated_questions.append(question_copy)
            
            # ë¬¸ì œ ë²ˆí˜¸ìˆœ ì •ë ¬
            validated_questions.sort(key=lambda x: x.get('question_number', x.get('id', 0)))
            
            print(f"âœ… ìµœì¢… ê²€ì¦ ì™„ë£Œ: {len(validated_questions)}ê°œ ë¬¸ì œ")
            print(f"   í’ˆì§ˆ ì ìˆ˜: {self._calculate_quality_score(validated_questions):.1f}/100")
            
            return validated_questions
            
        except Exception as e:
            print(f"âŒ ìµœì¢… ê²€ì¦ ì˜¤ë¥˜: {e}")
            return questions
    
    def _calculate_quality_score(self, questions: List[Dict]) -> float:
        """ë¬¸ì œ ì„¸íŠ¸ì˜ í’ˆì§ˆ ì ìˆ˜ ê³„ì‚°"""
        if not questions:
            return 0.0
        
        total_score = 0
        for question in questions:
            score = 0
            
            # ê¸°ë³¸ ì ìˆ˜ (20ì )
            question_text = question.get('question_text', question.get('question', ''))
            if question_text and len(question_text.strip()) > 10:
                score += 20
            
            # ì„ íƒì§€ ì ìˆ˜ (40ì ) - options ìš°ì„  ì‚¬ìš©
            choices = question.get('options', question.get('choices', []))
            if len(choices) >= 4:
                score += 40
            elif len(choices) >= 2:
                score += 20
            
            # ì»¨í…ìŠ¤íŠ¸ ì ìˆ˜ (20ì )
            passage = question.get('passage', '')
            if passage and len(passage.strip()) > 20:
                score += 20
            
            # êµ¬ì¡° ì ìˆ˜ (20ì )
            question_number = question.get('question_number', question.get('id', 0))
            if question_number > 0:
                score += 10
                
            if question.get('has_table') or any('|' in str(choice.get('choice_text', '')) for choice in choices):
                score += 10
            
            total_score += min(score, 100)
        
        return total_score / len(questions)
    
    def get_pdf_path(self, upload_id: int, db: Session = None) -> Optional[str]:
        """Upload IDì— í•´ë‹¹í•˜ëŠ” PDF íŒŒì¼ ê²½ë¡œ ë°˜í™˜ - DB ê¸°ë°˜ ê°œì„ """
        try:
            import os
            import glob
            from pathlib import Path
            
            print(f"ğŸ” Upload {upload_id} PDF íŒŒì¼ ê²€ìƒ‰ ì‹œì‘...")
            
            # 1ë‹¨ê³„: ë°ì´í„°ë² ì´ìŠ¤ì—ì„œ ì‹¤ì œ íŒŒì¼ëª… ì¡°íšŒ (ê°€ì¥ ì •í™•í•¨)
            if db:
                try:
                    from app.models import Document
                    document = db.query(Document).filter(Document.id == upload_id).first()
                    if document and hasattr(document, 'filename') and document.filename:
                        # ì‹¤ì œ ì €ì¥ëœ íŒŒì¼ëª…ìœ¼ë¡œ ê²€ìƒ‰
                        stored_filename = document.filename
                        print(f"ğŸ“„ DBì—ì„œ íŒŒì¼ëª… ì¡°íšŒ: {stored_filename}")
                        
                        # PDFê°€ ì €ì¥ë˜ëŠ” ì‹¤ì œ ê²½ë¡œì—ì„œ ê²€ìƒ‰
                        pdf_search_paths = [
                            f"backend/uploads/pdfs/{stored_filename}",
                            f"uploads/pdfs/{stored_filename}",
                            f"static/uploads/pdfs/{stored_filename}"
                        ]
                        
                        for path in pdf_search_paths:
                            if os.path.exists(path):
                                print(f"âœ… DB ê¸°ë°˜ìœ¼ë¡œ PDF ë°œê²¬: {path}")
                                return os.path.abspath(path)
                except Exception as e:
                    print(f"âš ï¸ DB ì¡°íšŒ ì¤‘ ì˜¤ë¥˜: {e}")
            
            # 2ë‹¨ê³„: íƒ€ì„ìŠ¤íƒ¬í”„ ê¸°ë°˜ íŒŒì¼ëª… íŒ¨í„´ ê²€ìƒ‰ (ì‹¤ì œ ì‚¬ìš©ë˜ëŠ” íŒ¨í„´)
            pdf_directories = [
                "backend/uploads/pdfs",
                "uploads/pdfs", 
                "static/uploads/pdfs"
            ]
            
            for pdf_dir in pdf_directories:
                if os.path.exists(pdf_dir):
                    # íƒ€ì„ìŠ¤íƒ¬í”„ íŒ¨í„´ìœ¼ë¡œ ê²€ìƒ‰: 20250826_*_*_*.pdf
                    pattern_files = glob.glob(os.path.join(pdf_dir, "*.pdf"))
                    for file_path in pattern_files:
                        if self._validate_pdf_file(file_path):
                            file_name = os.path.basename(file_path)
                            # íŒŒì¼ëª…ì— upload_id íŒ¨í„´ì´ ìˆëŠ”ì§€ í™•ì¸í•˜ê±°ë‚˜ ë‚ ì§œìˆœìœ¼ë¡œ ë§¤ì¹­
                            print(f"ğŸ“‹ ë°œê²¬ëœ PDF: {file_name}")
                            # ì—¬ê¸°ì„œëŠ” ê°€ì¥ ìµœê·¼ íŒŒì¼ì„ ë°˜í™˜ (ì„ì‹œë°©í¸)
                            if upload_id >= 10:  # ìµœê·¼ ì—…ë¡œë“œë¼ê³  ê°€ì •
                                print(f"âœ… íŒ¨í„´ ê¸°ë°˜ìœ¼ë¡œ PDF ë°œê²¬: {file_path}")
                                return os.path.abspath(file_path)
            
            # 3ë‹¨ê³„: ê¸°ì¡´ ë‹¨ìˆœ íŒ¨í„´ ê²€ìƒ‰ (í˜¸í™˜ì„±)
            legacy_patterns = [
                f"uploads/{upload_id}.pdf",
                f"uploads/upload_{upload_id}.pdf", 
                f"static/uploads/{upload_id}.pdf",
                f"temp_processing/upload_{upload_id}.pdf"
            ]
            
            for path in legacy_patterns:
                if os.path.exists(path):
                    print(f"âœ… ë ˆê±°ì‹œ íŒ¨í„´ìœ¼ë¡œ PDF ë°œê²¬: {path}")
                    return os.path.abspath(path)
            
            # 4ë‹¨ê³„: ì „ì²´ ë””ë ‰í† ë¦¬ ê²€ìƒ‰ (ìµœí›„ ìˆ˜ë‹¨)
            print(f"ğŸ” ì „ì²´ ë””ë ‰í† ë¦¬ ê²€ìƒ‰ ì¤‘...")
            for root, dirs, files in os.walk("."):
                for file in files:
                    if file.endswith('.pdf') and str(upload_id) in file:
                        found_path = os.path.join(root, file)
                        if self._validate_pdf_file(found_path):
                            print(f"âœ… ì „ì²´ ê²€ìƒ‰ìœ¼ë¡œ PDF ë°œê²¬: {found_path}")
                            return os.path.abspath(found_path)
            
            print(f"âŒ Upload {upload_id} PDF íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ")
            print(f"   ê²€ìƒ‰í•œ ë””ë ‰í† ë¦¬: {pdf_directories}")
            print(f"   ì‹œë„í•œ íŒ¨í„´: {legacy_patterns}")
            return None
            
        except Exception as e:
            print(f"âŒ PDF ê²½ë¡œ ê²€ìƒ‰ ì˜¤ë¥˜: {e}")
            return None
    
    def _validate_pdf_file(self, file_path: str) -> bool:
        """PDF íŒŒì¼ ìœ íš¨ì„± ê²€ì‚¬"""
        try:
            import os
            
            if not os.path.exists(file_path) or os.path.getsize(file_path) < 1024:
                return False
            
            # PDF í—¤ë” í™•ì¸
            try:
                with open(file_path, 'rb') as f:
                    header = f.read(8)
                    if not header.startswith(b'%PDF-'):
                        return False
            except:
                return False
            
            # PyMuPDFë¡œ ë¹ ë¥¸ ê²€ì¦
            try:
                import fitz
                doc = fitz.open(file_path)
                if len(doc) == 0:
                    doc.close()
                    return False
                doc.close()
                return True
            except:
                return False
                
        except Exception as e:
            print(f"âŒ PDF ê²€ì¦ ì˜¤ë¥˜: {e}")
            return False
    
    async def _enhanced_image_choice_processing(self, questions: List[Dict], upload_id: int) -> int:
        """í–¥ìƒëœ ì´ë¯¸ì§€ ì„ íƒì§€ ì²˜ë¦¬ - í˜¼í•© ì½˜í…ì¸  ë° ì¸ë¼ì¸ ì´ë¯¸ì§€ ì§€ì›"""
        try:
            import os
            import re
            
            # ì´ë¯¸ì§€ ë””ë ‰í† ë¦¬ í™•ì¸
            image_dir = f"static/images/upload_{upload_id}"
            if not os.path.exists(image_dir):
                print(f"âš ï¸ ì´ë¯¸ì§€ ë””ë ‰í† ë¦¬ ì—†ìŒ: {image_dir}")
                return 0
            
            image_files = [f for f in os.listdir(image_dir) if f.startswith('IMG_')]
            if not image_files:
                print("âš ï¸ ì¶”ì¶œëœ ì´ë¯¸ì§€ íŒŒì¼ì´ ì—†ìŒ")
                return 0
            
            image_list = sorted(image_files)
            processed_count = 0
            
            print(f"ğŸ–¼ï¸ í–¥ìƒëœ ì´ë¯¸ì§€ ì²˜ë¦¬ ì‹œì‘: {len(image_list)}ê°œ ì´ë¯¸ì§€ ì‚¬ìš©")
            
            for question in questions:
                question_number = question.get('question_number', 0)
                question_text = question.get('question_text', '')
                passage = question.get('passage', '')
                current_options = question.get('options', [])
                
                # ë‹¤ì¤‘ ë ˆë²¨ ì´ë¯¸ì§€ ì²˜ë¦¬
                was_processed = False
                
                # 1ë‹¨ê³„: ë¬¸ì œ í…ìŠ¤íŠ¸ ë‚´ ì´ë¯¸ì§€ ì°¸ì¡° ì²˜ë¦¬
                enhanced_question_text = await self._process_inline_image_references(
                    question_text, image_list, upload_id
                )
                if enhanced_question_text != question_text:
                    question['question_text'] = enhanced_question_text
                    was_processed = True
                    print(f"   ğŸ–¼ï¸ Q{question_number}: ë¬¸ì œ í…ìŠ¤íŠ¸ ë‚´ ì´ë¯¸ì§€ ì°¸ì¡° ì²˜ë¦¬ë¨")
                
                # 2ë‹¨ê³„: ì§€ë¬¸ ë‚´ ì´ë¯¸ì§€ ì°¸ì¡° ì²˜ë¦¬
                if passage:
                    enhanced_passage = await self._process_inline_image_references(
                        passage, image_list, upload_id
                    )
                    if enhanced_passage != passage:
                        question['passage'] = enhanced_passage
                        was_processed = True
                        print(f"   ğŸ–¼ï¸ Q{question_number}: ì§€ë¬¸ ë‚´ ì´ë¯¸ì§€ ì°¸ì¡° ì²˜ë¦¬ë¨")
                
                # 3ë‹¨ê³„: ì„ íƒì§€ë³„ ê°œë³„ ì´ë¯¸ì§€ ì²˜ë¦¬ (í˜¼í•© ì½˜í…ì¸  ì§€ì›)
                enhanced_options = []
                for i, option in enumerate(current_options):
                    if isinstance(option, str):
                        # IMG_XXX_IMAGE íŒ¨í„´ì„ ì‹¤ì œ ì´ë¯¸ì§€ë¡œ êµì²´
                        enhanced_option = await self._resolve_image_placeholders(
                            option, image_list, upload_id
                        )
                        
                        # ë¹ˆ ì´ë¯¸ì§€ ì°¸ì¡°ë‚˜ ë¶ˆì™„ì „í•œ ì„ íƒì§€ ì²˜ë¦¬
                        if self._is_incomplete_image_choice(enhanced_option):
                            # ì»¨í…ìŠ¤íŠ¸ ê¸°ë°˜ ì´ë¯¸ì§€ ë§¤ì¹­ ì‹œë„
                            contextual_image = await self._match_contextual_image(
                                question_number, i + 1, question_text, passage, image_list, upload_id
                            )
                            if contextual_image:
                                enhanced_option = contextual_image
                                print(f"   ğŸ¯ Q{question_number} ì„ íƒì§€ {i+1}: ì»¨í…ìŠ¤íŠ¸ ê¸°ë°˜ ì´ë¯¸ì§€ ë§¤ì¹­")
                        
                        enhanced_options.append(enhanced_option)
                    else:
                        enhanced_options.append(option)
                
                # ì„ íƒì§€ê°€ ê°œì„ ë˜ì—ˆëŠ”ì§€ í™•ì¸
                if enhanced_options != current_options:
                    question['options'] = enhanced_options
                    was_processed = True
                    print(f"   âœ… Q{question_number}: ì„ íƒì§€ ì´ë¯¸ì§€ ì²˜ë¦¬ ì™„ë£Œ")
                
                # 4ë‹¨ê³„: ì „ì²´ ì´ë¯¸ì§€ ê¸°ë°˜ ì„ íƒì§€ ì²˜ë¦¬ (ê¸°ì¡´ ë¡œì§)
                if not was_processed:
                    needs_images, confidence = self._analyze_image_requirement(
                        question_text, passage, current_options
                    )
                    
                    if needs_images and confidence > 0.3:  # ì„ê³„ê°’ ë‚®ì¶¤ (30%)
                        matched_images = await self._smart_image_matching(
                            question_number, question_text, passage, image_list, upload_id
                        )
                        
                        if matched_images:
                            current_quality = self._analyze_options_quality(current_options)
                            should_replace = (
                                current_quality < 0.4 or  # í’ˆì§ˆ ì„ê³„ê°’ ë‚®ì¶¤
                                len(current_options) < 2 or  # ìµœì†Œ ì„ íƒì§€ ê¸°ì¤€ ë‚®ì¶¤
                                any('IMG_' in str(opt) and len(str(opt).strip()) < 5 for opt in current_options)
                            )
                            
                            if should_replace:
                                image_options = []
                                for img_info in matched_images[:4]:
                                    img_path = f"/images/upload_{upload_id}/{img_info['filename']}"
                                    image_options.append(f"![IMG_{img_info['number']:03d}]({img_path})")
                                
                                question['options'] = image_options
                                was_processed = True
                                print(f"   ğŸ”„ Q{question_number}: ì „ì²´ ì´ë¯¸ì§€ ì„ íƒì§€ êµì²´ (í’ˆì§ˆ:{current_quality:.2f})")
                
                if was_processed:
                    processed_count += 1
            
            print(f"ğŸ–¼ï¸ ì´ë¯¸ì§€ ì²˜ë¦¬ ì™„ë£Œ: {processed_count}ê°œ ë¬¸ì œ ê°œì„ ")
            return processed_count
            
        except Exception as e:
            print(f"âŒ í–¥ìƒëœ ì´ë¯¸ì§€ ì²˜ë¦¬ ì˜¤ë¥˜: {e}")
            return 0
    
    async def _process_inline_image_references(self, text: str, image_list: List[str], upload_id: int) -> str:
        """í…ìŠ¤íŠ¸ ë‚´ ì´ë¯¸ì§€ ì°¸ì¡° ì²˜ë¦¬"""
        try:
            import re
            
            if not text or not image_list:
                return text
            
            enhanced_text = text
            
            # íŒ¨í„´ 1: "IMG_XXX_IMAGE" í˜•íƒœì˜ ì°¸ì¡°
            img_pattern = r'IMG_(\d{3})_IMAGE'
            matches = re.findall(img_pattern, enhanced_text)
            
            for match in matches:
                img_number = int(match)
                target_filename = f"IMG_{img_number:03d}.png"
                
                # ëŒ€ì‘ë˜ëŠ” ì´ë¯¸ì§€ íŒŒì¼ ì°¾ê¸°
                if target_filename in image_list:
                    img_path = f"/images/upload_{upload_id}/{target_filename}"
                    img_markdown = f"![IMG_{img_number:03d}]({img_path})"
                    enhanced_text = enhanced_text.replace(f"IMG_{img_number:03d}_IMAGE", img_markdown)
                elif f"IMG_{img_number:03d}.jpeg" in image_list:
                    # JPEG í˜•ì‹ë„ ì²´í¬
                    target_filename = f"IMG_{img_number:03d}.jpeg"
                    img_path = f"/images/upload_{upload_id}/{target_filename}"
                    img_markdown = f"![IMG_{img_number:03d}]({img_path})"
                    enhanced_text = enhanced_text.replace(f"IMG_{img_number:03d}_IMAGE", img_markdown)
            
            # íŒ¨í„´ 2: "ê·¸ë¦¼ 1", "ë„í‘œ 2" ë“±ì˜ ì°¸ì¡°
            diagram_patterns = [
                r'(ê·¸ë¦¼\s*(\d+))',
                r'(ë„í‘œ\s*(\d+))',
                r'(ì´ë¯¸ì§€\s*(\d+))',
                r'(ë‹¤ìŒ\s*ê·¸ë¦¼)',
                r'(ì•„ë˜\s*ê·¸ë¦¼)'
            ]
            
            for pattern in diagram_patterns:
                matches = re.finditer(pattern, enhanced_text)
                for match in matches:
                    if len(match.groups()) >= 2 and match.group(2).isdigit():
                        # ìˆ«ìê°€ ìˆëŠ” ê²½ìš° í•´ë‹¹ ì´ë¯¸ì§€ ì°¾ê¸°
                        ref_number = int(match.group(2))
                        target_filename = f"IMG_{ref_number:03d}.png"
                        if target_filename in image_list:
                            img_path = f"/images/upload_{upload_id}/{target_filename}"
                            img_markdown = f"![IMG_{ref_number:03d}]({img_path})"
                            enhanced_text = enhanced_text.replace(match.group(1), f"{match.group(1)} {img_markdown}")
            
            return enhanced_text
            
        except Exception as e:
            print(f"âš ï¸ ì¸ë¼ì¸ ì´ë¯¸ì§€ ì²˜ë¦¬ ì˜¤ë¥˜: {e}")
            return text
    
    async def _resolve_image_placeholders(self, option_text: str, image_list: List[str], upload_id: int) -> str:
        """ì„ íƒì§€ ë‚´ ì´ë¯¸ì§€ í”Œë ˆì´ìŠ¤í™€ë” ì²˜ë¦¬"""
        try:
            import re
            
            if not option_text or not isinstance(option_text, str):
                return str(option_text) if option_text is not None else ''
            
            enhanced_option = option_text
            
            # IMG_XXX_IMAGE íŒ¨í„´ ì²˜ë¦¬
            img_pattern = r'IMG_(\d{3})_IMAGE'
            matches = re.findall(img_pattern, enhanced_option)
            
            for match in matches:
                img_number = int(match)
                target_files = [
                    f"IMG_{img_number:03d}.png",
                    f"IMG_{img_number:03d}.jpeg"
                ]
                
                for target_filename in target_files:
                    if target_filename in image_list:
                        img_path = f"/images/upload_{upload_id}/{target_filename}"
                        img_markdown = f"![IMG_{img_number:03d}]({img_path})"
                        enhanced_option = enhanced_option.replace(f"IMG_{img_number:03d}_IMAGE", img_markdown)
                        break
            
            return enhanced_option
            
        except Exception as e:
            print(f"âš ï¸ ì´ë¯¸ì§€ í”Œë ˆì´ìŠ¤í™€ë” ì²˜ë¦¬ ì˜¤ë¥˜: {e}")
            return str(option_text) if option_text is not None else ''
    
    def _is_incomplete_image_choice(self, option_text: str) -> bool:
        """ë¶ˆì™„ì „í•œ ì´ë¯¸ì§€ ì„ íƒì§€ ê°ì§€"""
        try:
            if not option_text or not isinstance(option_text, str):
                return True
            
            option_str = str(option_text).strip()
            
            # ë¹ˆ ë¬¸ìì—´ì´ë‚˜ ë„ˆë¬´ ì§§ì€ ë¬¸ìì—´
            if len(option_str) < 3:
                return True
            
            # ì²˜ë¦¬ë˜ì§€ ì•Šì€ IMG ì°¸ì¡°
            if 'IMG_' in option_str and '_IMAGE' in option_str:
                return True
            
            # ì˜ë¯¸ì—†ëŠ” í…ìŠ¤íŠ¸
            meaningless_patterns = ['...', 'â€“', 'â€”', '?', 'ë¹„ì–´ìˆìŒ', 'ì—†ìŒ']
            if any(pattern in option_str for pattern in meaningless_patterns):
                return True
            
            return False
            
        except Exception:
            return True
    
    async def _match_contextual_image(self, question_number: int, choice_number: int, 
                                     question_text: str, passage: str, 
                                     image_list: List[str], upload_id: int) -> str:
        """ì»¨í…ìŠ¤íŠ¸ ê¸°ë°˜ ì´ë¯¸ì§€ ë§¤ì¹­"""
        try:
            # ë¬¸ì œ ë²ˆí˜¸ì™€ ì„ íƒì§€ ë²ˆí˜¸ë¥¼ ê³ ë ¤í•œ ì´ë¯¸ì§€ ë§¤ì¹­
            candidate_numbers = [
                question_number,  # ë¬¸ì œ ë²ˆí˜¸ ê¸°ë°˜
                question_number * 10 + choice_number,  # ë¬¸ì œ+ì„ íƒì§€ ì¡°í•©
                (question_number - 1) * 4 + choice_number,  # ìˆœì°¨ì  ë°°ì¹˜
                choice_number,  # ì„ íƒì§€ ë²ˆí˜¸ë§Œ
            ]
            
            for candidate_num in candidate_numbers:
                if 1 <= candidate_num <= 999:
                    target_files = [
                        f"IMG_{candidate_num:03d}.png",
                        f"IMG_{candidate_num:03d}.jpeg"
                    ]
                    
                    for target_filename in target_files:
                        if target_filename in image_list:
                            img_path = f"/images/upload_{upload_id}/{target_filename}"
                            return f"![IMG_{candidate_num:03d}]({img_path})"
            
            return None
            
        except Exception as e:
            print(f"âš ï¸ ì»¨í…ìŠ¤íŠ¸ ì´ë¯¸ì§€ ë§¤ì¹­ ì˜¤ë¥˜: {e}")
            return None
    
    async def _enhance_boundary_chunk_processing(self, chunk_content: str, chunk_info: Dict) -> str:
        """ê²½ê³„ ì²­í¬ ì²˜ë¦¬ ê°•í™” - ì„ íƒì§€ ë³µêµ¬ ì „ìš© í”„ë¡¬í”„íŠ¸"""
        try:
            if chunk_info.get('chunk_type') != 'boundary_overlap':
                return chunk_content
            
            # ê²½ê³„ ì²­í¬ ì „ìš© ê°•í™” í”„ë¡¬í”„íŠ¸ ì¶”ê°€
            enhanced_content = f"""{chunk_content}

ğŸ”§ **ê²½ê³„ ì²­í¬ ì²˜ë¦¬ íŠ¹ë³„ ì§€ì¹¨**:
1. **ë¬¸ì œ ì—°ê²°**: í˜ì´ì§€ ê²½ê³„ì—ì„œ ì˜ë¦° ë¬¸ì œ ë²ˆí˜¸ë¥¼ í•˜ë‚˜ì˜ ì™„ì „í•œ ë¬¸ì œë¡œ ì—°ê²°
2. **ì„ íƒì§€ ë³µêµ¬**: ë¶ˆì™„ì „í•œ ì„ íƒì§€(â‘  ìˆê³  â‘¡â‘¢â‘£ ì—†ëŠ” ê²½ìš°)ë¥¼ ë‹¤ìŒ í˜ì´ì§€ ë‚´ìš©ê³¼ ì—°ê²°í•˜ì—¬ ì™„ì„±
3. **í‘œ/ë‹¤ì´ì–´ê·¸ë¨ í†µí•©**: í˜ì´ì§€ë¥¼ ë„˜ë‚˜ë“œëŠ” í‘œë‚˜ ê·¸ë¦¼ì„ í•˜ë‚˜ì˜ ì™„ì „í•œ ìš”ì†Œë¡œ ì²˜ë¦¬
4. **ë¹ˆ ì»¨í…Œì´ë„ˆ ë°©ì§€**: ì„ íƒì§€ ë¼ë²¨ë§Œ ìˆê³  ë‚´ìš©ì´ ì—†ëŠ” ê²½ìš° ë°˜ë“œì‹œ ì „í›„ ë§¥ë½ì—ì„œ ë‚´ìš© ì°¾ì•„ì„œ ì±„ì›€
5. **ë¬¸ì œ ë²ˆí˜¸ ì¼ê´€ì„±**: ë™ì¼í•œ ë¬¸ì œ ë²ˆí˜¸ê°€ ë‘ í˜ì´ì§€ì— ê±¸ì³ ë‚˜íƒ€ë‚˜ë©´ í•˜ë‚˜ë¡œ í†µí•©

âš ï¸ **ì¤‘ìš”**: ë¹ˆ ì„ íƒì§€ ì»¨í…Œì´ë„ˆ(ì˜ˆ: â‘  ì—†ìŒ, â‘¡ ë¹„ì–´ìˆìŒ)ë¥¼ ë§Œë“¤ì§€ ë§ˆì„¸ìš”. ë°˜ë“œì‹œ ì‹¤ì œ ë‚´ìš©ìœ¼ë¡œ ì±„ì›Œì£¼ì„¸ìš”.
"""
            return enhanced_content
            
        except Exception as e:
            print(f"âš ï¸ ê²½ê³„ ì²­í¬ ê°•í™” ì²˜ë¦¬ ì˜¤ë¥˜: {e}")
            return chunk_content
    
    async def _get_boundary_recovery_prompt(self, chunk_content: str, page_start: int, page_end: int) -> str:
        """ê²½ê³„ ì²­í¬ ì „ìš© ë³µêµ¬ í”„ë¡¬í”„íŠ¸ ìƒì„±"""
        try:
            boundary_prompt = f"""
ğŸ”§ **í˜ì´ì§€ ê²½ê³„ ë¬¸ì œ ë³µêµ¬ ì „ë¬¸ê°€**

ë‹¹ì‹ ì€ í•œêµ­ì–´ ì‹œí—˜ ë¬¸ì œì—ì„œ í˜ì´ì§€ ê²½ê³„ë¡œ ì¸í•´ ì˜ë¦° ë¬¸ì œì™€ ì„ íƒì§€ë¥¼ ë³µêµ¬í•˜ëŠ” ì „ë¬¸ê°€ì…ë‹ˆë‹¤.
ë‹¤ìŒì€ í˜ì´ì§€ {page_start}-{page_end} ê²½ê³„ì—ì„œ ì˜ë¦° ë‚´ìš©ì…ë‹ˆë‹¤.

**í•µì‹¬ ì„ë¬´**:
1. ğŸ”— **ë¬¸ì œ ì—°ê²° ë³µêµ¬**: ì˜ë¦° ë¬¸ì œ ë²ˆí˜¸ë¥¼ ì™„ì „í•œ ë¬¸ì œë¡œ ì—°ê²°
2. ğŸ¯ **ì„ íƒì§€ ì™„ì„±**: ë¶ˆì™„ì „í•œ ì„ íƒì§€(â‘ â‘¡â‘¢â‘£ ì¤‘ ì¼ë¶€ë§Œ ìˆìŒ)ë¥¼ ì™„ì „í•œ ì„¸íŠ¸ë¡œ ë³µêµ¬  
3. ğŸ“Š **í‘œ/ê·¸ë¦¼ í†µí•©**: í˜ì´ì§€ë¥¼ ë„˜ë‚˜ë“œëŠ” í‘œë‚˜ ë‹¤ì´ì–´ê·¸ë¨ì„ í•˜ë‚˜ë¡œ í†µí•©
4. ğŸš« **ë¹ˆ ì»¨í…Œì´ë„ˆ ì œê±°**: ë‚´ìš©ì´ ì—†ëŠ” ì„ íƒì§€ ì»¨í…Œì´ë„ˆë¥¼ ì ˆëŒ€ ë§Œë“¤ì§€ ë§ˆì„¸ìš”

**ë³µêµ¬ ì „ëµ**:
- ì˜ë¦° ë¬¸ì œ í…ìŠ¤íŠ¸ë¥¼ ì´ì–´ë¶™ì—¬ì„œ ì™„ì „í•œ ë¬¸ì œ ë§Œë“¤ê¸°
- ì²« í˜ì´ì§€ ëì˜ ì„ íƒì§€ì™€ ë‹¤ìŒ í˜ì´ì§€ ì‹œì‘ì˜ ì„ íƒì§€ ì—°ê²°
- í‘œì˜ í—¤ë”ì™€ ë°ì´í„° í–‰ì„ ì™„ì „íˆ í†µí•©
- ë¬¸ì œì™€ ê´€ë ¨ëœ ëª¨ë“  ì´ë¯¸ì§€/ë‹¤ì´ì–´ê·¸ë¨ ì°¸ì¡° ìœ ì§€

**ì¶œë ¥ í˜•ì‹** (JSON):
```json
{{
  "questions": [
    {{
      "question_number": ì •ìˆ˜,
      "question_text": "ì™„ì „íˆ ë³µêµ¬ëœ ë¬¸ì œ í…ìŠ¤íŠ¸",
      "options": ["â‘ ", "â‘¡", "â‘¢", "â‘£"], // ë°˜ë“œì‹œ ì™„ì „í•œ ì„ íƒì§€ ì„¸íŠ¸
      "passage": "ì§€ë¬¸ì´ ìˆëŠ” ê²½ìš° (í‘œ/ê·¸ë¦¼ í¬í•¨)",
      "recovery_info": {{
        "was_split": true,
        "merged_content": "ì–´ë–¤ ë‚´ìš©ì´ í•©ì³ì¡ŒëŠ”ì§€ ì„¤ëª…",
        "pages_involved": [{page_start}, {page_end}]
      }}
    }}
  ]
}}
```

**ì ˆëŒ€ ê¸ˆì§€ì‚¬í•­**:
- âŒ "ì„ íƒì§€ ì—†ìŒ", "ë¹ˆ ì„ íƒì§€", "..." ë“±ì˜ ë¹ˆ ì»¨í…Œì´ë„ˆ ìƒì„± ê¸ˆì§€
- âŒ ë¶ˆì™„ì „í•œ ì„ íƒì§€ ì„¸íŠ¸ (â‘ â‘¡ë§Œ ìˆê³  â‘¢â‘£ ì—†ìŒ) ìƒì„± ê¸ˆì§€
- âŒ ë¬¸ì œ ë²ˆí˜¸ ì¤‘ë³µ ìƒì„± ê¸ˆì§€

**ì²˜ë¦¬í•  ë‚´ìš©**:
{chunk_content}
"""
            return boundary_prompt
            
        except Exception as e:
            print(f"âš ï¸ ê²½ê³„ ë³µêµ¬ í”„ë¡¬í”„íŠ¸ ìƒì„± ì˜¤ë¥˜: {e}")
            # ì˜¤ë¥˜ ë°œìƒ ì‹œ ì¼ë°˜ í”„ë¡¬í”„íŠ¸ë¡œ í´ë°±
            return f"""
ğŸ“š **ì‹œí—˜ë¬¸ì œ ì¶”ì¶œ ì „ë¬¸ê°€**

ë‹¤ìŒ êµìœ¡ ì½˜í…ì¸ ì—ì„œ ì‹œí—˜ ë¬¸ì œë§Œ ì¶”ì¶œí•˜ì—¬ JSON í˜•ì‹ìœ¼ë¡œ êµ¬ì¡°í™”í•˜ì„¸ìš”.

**ì²˜ë¦¬í•  ë‚´ìš©**:
{chunk_content}

**ì¶œë ¥ í˜•ì‹**: JSON
```json
{{
  "questions": [
    {{
      "question_number": ì •ìˆ˜,
      "question_text": "ë¬¸ì œ í…ìŠ¤íŠ¸",
      "options": ["â‘ ", "â‘¡", "â‘¢", "â‘£"],
      "passage": "ì§€ë¬¸ (ìˆëŠ” ê²½ìš°ë§Œ)"
    }}
  ]
}}
```
"""
    
    def _analyze_image_requirement(self, question_text: str, passage: str, options: List) -> tuple[bool, float]:
        """ë¬¸ì œê°€ ì´ë¯¸ì§€ ì„ íƒì§€ë¥¼ í•„ìš”ë¡œ í•˜ëŠ”ì§€ ìŠ¤ë§ˆíŠ¸ ë¶„ì„"""
        try:
            combined_text = f"{question_text} {passage}".lower()
            confidence = 0.0
            
            # ëª…í™•í•œ ì´ë¯¸ì§€ í•„ìš” ì‹ í˜¸
            strong_indicators = [
                'í‘œë¥¼ ë³´ê³ ', 'ê·¸ë¦¼ì„ ë³´ê³ ', 'ë‹¤ìŒ í‘œì—ì„œ', 'ìœ„ ê·¸ë¦¼ì—ì„œ', 
                'ì•„ë˜ í‘œëŠ”', 'ë‹¤ìŒ ê·¸ë˜í”„', 'ì°¨íŠ¸ë¥¼ ë³´ê³ ', 'ë„í‘œë¥¼ ì°¸ê³ ',
                'ë‹¤ìŒ ì¤‘ ì˜¬ë°”ë¥¸ ê²ƒ', 'ë‹¤ìŒ ì¤‘ í‹€ë¦° ê²ƒ', 'ê°€ì¥ ì ì ˆí•œ ê²ƒ'
            ]
            
            # í…Œì´ë¸”/ê·¸ë˜í”„ ê´€ë ¨ í‚¤ì›Œë“œ
            table_keywords = [
                'í‘œ', 'table', 'í”„ë¡œì„¸ìŠ¤', 'p1', 'p2', 'p3', 
                'ë„ì°©ì‹œê°„', 'ì‹¤í–‰ì‹œê°„', 'ëŒ€ê¸°ì‹œê°„', 'ì²˜ë¦¬ì‹œê°„',
                'ê·¸ë˜í”„', 'graph', 'chart', 'ì°¨íŠ¸'
            ]
            
            # ì´ë¯¸ì§€ ì„ íƒì§€ íŒ¨í„´
            image_choice_patterns = [
                'img_', 'ì´ë¯¸ì§€', 'ê·¸ë¦¼', 'ë„í˜•', 'ê¸°í˜¸'
            ]
            
            # ì ìˆ˜ ê³„ì‚°
            for indicator in strong_indicators:
                if indicator in combined_text:
                    confidence += 0.4
            
            for keyword in table_keywords:
                if keyword in combined_text:
                    confidence += 0.2
            
            # ê¸°ì¡´ ì„ íƒì§€ì— ì´ë¯¸ì§€ ì°¸ì¡°ê°€ ìˆëŠ” ê²½ìš°
            for option in options:
                option_str = str(option).lower()
                if any(pattern in option_str for pattern in image_choice_patterns):
                    confidence += 0.3
                    
            # ì„ íƒì§€ í’ˆì§ˆì´ ë‚®ì€ ê²½ìš° (í…ìŠ¤íŠ¸ê°€ ê±°ì˜ ì—†ìŒ)
            text_options = [opt for opt in options if opt and len(str(opt).strip()) > 2]
            if len(text_options) < len(options) * 0.5:  # ì ˆë°˜ ì´ìƒì´ ë¶€ì‹¤í•œ ì„ íƒì§€
                confidence += 0.3
            
            # ìµœëŒ€ 1.0ìœ¼ë¡œ ì œí•œ
            confidence = min(confidence, 1.0)
            needs_images = confidence > 0.3
            
            return needs_images, confidence
            
        except Exception as e:
            print(f"âš ï¸ ì´ë¯¸ì§€ ìš”êµ¬ì‚¬í•­ ë¶„ì„ ì˜¤ë¥˜: {e}")
            return False, 0.0
    
    async def _smart_image_matching(self, question_number: int, question_text: str, 
                                   passage: str, image_list: List[str], upload_id: int) -> List[Dict]:
        """ë‹¤ì¤‘ ê¸°ì¤€ ì´ë¯¸ì§€ ë§¤ì¹­ - ê°œì„ ëœ ìŠ¤ì½”ì–´ ì‹œìŠ¤í…œ"""
        try:
            scored_images = []
            combined_text = f"{question_text} {passage}".lower()
            
            for img_file in image_list:
                if 'IMG_' in img_file:
                    try:
                        img_num_str = img_file.split('IMG_')[1].split('.')[0]
                        img_num = int(img_num_str)
                        
                        # ë‹¤ì¤‘ ê¸°ì¤€ ìŠ¤ì½”ì–´ ê³„ì‚°
                        score_components = self._calculate_multi_criteria_score(
                            question_number, img_num, combined_text, img_file
                        )
                        
                        total_score = sum(score_components.values())
                        
                        if total_score > 25:  # ì„ê³„ê°’ ë‚®ì¶¤ (30â†’25)
                            scored_images.append({
                                'filename': img_file,
                                'number': img_num,
                                'score': total_score,
                                'score_breakdown': score_components,
                                'distance': abs(img_num - question_number)
                            })
                            
                    except (ValueError, IndexError):
                        continue
            
            # ë³µí•© ì •ë ¬: ì ìˆ˜ ìš°ì„ , ê±°ë¦¬ ë³´ì¡°
            scored_images.sort(key=lambda x: (x['score'], -x['distance']), reverse=True)
            
            # ìƒìœ„ í›„ë³´êµ°ì—ì„œ ì¬ë­í‚¹
            top_candidates = scored_images[:6]  # ìƒìœ„ 6ê°œ í›„ë³´
            final_selection = self._rerank_image_candidates(
                top_candidates, question_number, combined_text
            )
            
            if final_selection:
                scores_info = [f"IMG_{img['number']:03d}({img['score']:.0f})" for img in final_selection]
                print(f"      ğŸ¯ Q{question_number} ë§¤ì¹­: {scores_info}")
                return final_selection
            
            return []
            
        except Exception as e:
            print(f"âš ï¸ ìŠ¤ë§ˆíŠ¸ ì´ë¯¸ì§€ ë§¤ì¹­ ì˜¤ë¥˜: {e}")
            return []
    
    def _calculate_multi_criteria_score(self, question_number: int, img_num: int, 
                                      combined_text: str, img_filename: str) -> Dict[str, float]:
        """ë‹¤ì¤‘ ê¸°ì¤€ ìŠ¤ì½”ì–´ ê³„ì‚°"""
        scores = {}
        
        # 1. ì§ì ‘ ë§¤ì¹­ (ê°€ì¤‘ì¹˜: 40%)
        if img_num == question_number:
            scores['direct_match'] = 100
        else:
            scores['direct_match'] = 0
        
        # 2. ê·¼ì ‘ë„ ë§¤ì¹­ (ê°€ì¤‘ì¹˜: 20%)
        distance = abs(img_num - question_number)
        if distance <= 1:
            scores['proximity'] = 80 - distance * 30
        elif distance <= 3:
            scores['proximity'] = 50 - distance * 10
        else:
            scores['proximity'] = max(0, 20 - distance * 2)
        
        # 3. ì»¨í…ìŠ¤íŠ¸ ë§¤ì¹­ (ê°€ì¤‘ì¹˜: 25%)
        context_score = 0
        
        # í…Œì´ë¸”/ê·¸ë˜í”„ ì»¨í…ìŠ¤íŠ¸
        if any(keyword in combined_text for keyword in ['í‘œ', 'table', 'ê·¸ë˜í”„', 'chart']):
            # í…Œì´ë¸” ë¬¸ì œëŠ” ì—°ì† ì´ë¯¸ì§€ íŒ¨í„´
            table_base = question_number * 2 + 10
            if table_base <= img_num <= table_base + 4:
                context_score += 60
            
        # ì„ íƒì§€ ì´ë¯¸ì§€ íŒ¨í„´ ê°ì§€
        choice_base = question_number * 4 + 20
        if choice_base <= img_num <= choice_base + 3:
            context_score += 70
        
        scores['context_match'] = context_score
        
        # 4. í˜ì´ì§€ ìœ„ì¹˜ ë§¤ì¹­ (ê°€ì¤‘ì¹˜: 15%)
        estimated_page = max(1, (question_number - 1) // 5 + 1)
        page_img_range = range((estimated_page - 1) * 15 + 1, estimated_page * 15 + 1)
        
        if img_num in page_img_range:
            # í˜ì´ì§€ ë‚´ ìœ„ì¹˜ì— ë”°ë¥¸ ì„¸ë¶„í™”
            page_position = (img_num - page_img_range.start) / len(page_img_range)
            question_position = ((question_number - 1) % 5) / 5  # í˜ì´ì§€ ë‚´ ë¬¸ì œ ìœ„ì¹˜
            
            position_similarity = 1 - abs(page_position - question_position)
            scores['page_position'] = 40 * position_similarity
        else:
            scores['page_position'] = 0
        
        # 5. íŒŒì¼ëª… íŒ¨í„´ ë³´ë„ˆìŠ¤
        scores['filename_bonus'] = 0
        if f"_{question_number:03d}" in img_filename:
            scores['filename_bonus'] = 20
        elif f"_{question_number:02d}" in img_filename:
            scores['filename_bonus'] = 15
        
        return scores
    
    def _rerank_image_candidates(self, candidates: List[Dict], question_number: int, 
                               combined_text: str) -> List[Dict]:
        """ìƒìœ„ í›„ë³´êµ° ì¬ë­í‚¹"""
        if not candidates:
            return []
        
        # ìµœê³  ì ìˆ˜ê°€ 50ì  ì´ìƒì¸ ê²½ìš°ë§Œ ì§„í–‰
        if candidates[0]['score'] < 50:
            return []
        
        # ì»¨í…ìŠ¤íŠ¸ ê¸°ë°˜ ì¬ê°€ì¤‘
        for candidate in candidates:
            original_score = candidate['score']
            
            # í…Œì´ë¸” ë¬¸ì œì— ëŒ€í•œ ê°€ì¤‘ì¹˜ ì¡°ì •
            if any(keyword in combined_text for keyword in ['í‘œ', 'table', 'ë„í‘œ']):
                if 'context_match' in candidate.get('score_breakdown', {}):
                    context_bonus = candidate['score_breakdown']['context_match']
                    if context_bonus > 50:  # ê°•í•œ ì»¨í…ìŠ¤íŠ¸ ë§¤ì¹­
                        candidate['score'] *= 1.2  # 20% ë³´ë„ˆìŠ¤
            
            # ì„ íƒì§€ ì´ë¯¸ì§€ ë¬¸ì œì— ëŒ€í•œ ê°€ì¤‘ì¹˜
            if any(keyword in combined_text for keyword in ['ë‹¤ìŒ ì¤‘', 'ê°€ì¥ ì ì ˆí•œ', 'ì˜³ì€ ê²ƒ']):
                if candidate['score_breakdown'].get('context_match', 0) > 60:
                    candidate['score'] *= 1.15
        
        # ì¬ì •ë ¬
        candidates.sort(key=lambda x: x['score'], reverse=True)
        
        # ìµœì¢… ì„ íƒ: ìµœëŒ€ 4ê°œ, ì ìˆ˜ ì„ê³„ê°’ ì ìš©
        final_candidates = []
        for candidate in candidates[:4]:
            if candidate['score'] >= 40:  # ë‚®ì•„ì§„ ì„ê³„ê°’
                final_candidates.append(candidate)
        
        return final_candidates
    
    def _analyze_options_quality(self, options: List) -> float:
        """ê¸°ì¡´ ì„ íƒì§€ì˜ í’ˆì§ˆ ë¶„ì„ (0.0 - 1.0)"""
        try:
            if not options:
                return 0.0
            
            quality_score = 0.0
            valid_options = 0
            
            for option in options:
                option_str = str(option).strip()
                
                if not option_str or option_str in ['', '-', '--']:
                    continue  # ë¹ˆ ì„ íƒì§€
                
                if 'IMG_' in option_str and len(option_str) < 20:
                    # ì´ë¯¸ì§€ ì°¸ì¡°ë§Œ ìˆê³  ì„¤ëª…ì´ ì—†ëŠ” ê²½ìš°
                    quality_score += 0.3
                elif len(option_str) > 5:
                    # ì ì ˆí•œ í…ìŠ¤íŠ¸ê°€ ìˆëŠ” ê²½ìš°
                    quality_score += 1.0
                else:
                    # ë„ˆë¬´ ì§§ì€ ì„ íƒì§€
                    quality_score += 0.5
                    
                valid_options += 1
            
            if valid_options == 0:
                return 0.0
                
            return quality_score / valid_options
            
        except Exception as e:
            print(f"âš ï¸ ì„ íƒì§€ í’ˆì§ˆ ë¶„ì„ ì˜¤ë¥˜: {e}")
            return 0.5
    
    def _convert_image_references_to_markdown(self, questions: List[Dict], upload_id: int) -> List[Dict]:
        """IMG_XXX_IMAGE í˜•íƒœë¥¼ ë§ˆí¬ë‹¤ìš´ ì´ë¯¸ì§€ ì°¸ì¡°ë¡œ ë³€í™˜"""
        try:
            import os
            import re
            
            # ì´ë¯¸ì§€ ë””ë ‰í† ë¦¬ í™•ì¸
            image_dir = f"static/images/upload_{upload_id}"
            available_images = set()
            
            if os.path.exists(image_dir):
                image_files = [f for f in os.listdir(image_dir) if f.startswith('IMG_')]
                for img_file in image_files:
                    # IMG_001.png -> 001
                    match = re.match(r'IMG_(\d+)\.(\w+)', img_file)
                    if match:
                        img_num = int(match.group(1))
                        available_images.add(img_num)
            
            print(f"   ğŸ“ ì‚¬ìš© ê°€ëŠ¥í•œ ì´ë¯¸ì§€: {len(available_images)}ê°œ")
            
            converted_count = 0
            for question in questions:
                
                # ì„ íƒì§€ì—ì„œ ì´ë¯¸ì§€ ì°¸ì¡° ë³€í™˜
                options = question.get('options', [])
                if options:
                    converted_options = []
                    for option in options:
                        option_str = str(option) if option else ""
                        
                        # IMG_XXX_IMAGE íŒ¨í„´ ì°¾ê¸°
                        img_pattern = re.findall(r'IMG_(\d+)_IMAGE', option_str)
                        if img_pattern:
                            for img_num_str in img_pattern:
                                img_num = int(img_num_str)
                                
                                # ì‹¤ì œ ì´ë¯¸ì§€ íŒŒì¼ì´ ì¡´ì¬í•˜ëŠ”ì§€ í™•ì¸
                                if img_num in available_images:
                                    # ì´ë¯¸ì§€ íŒŒì¼ í™•ì¥ì ì°¾ê¸°
                                    img_ext = self._find_image_extension(upload_id, img_num)
                                    if img_ext:
                                        # ë§ˆí¬ë‹¤ìš´ í˜•ì‹ìœ¼ë¡œ ë³€í™˜
                                        img_path = f"/images/upload_{upload_id}/IMG_{img_num:03d}.{img_ext}"
                                        markdown_img = f"![IMG_{img_num:03d}]({img_path})"
                                        option_str = option_str.replace(f"IMG_{img_num_str}_IMAGE", markdown_img)
                                        converted_count += 1
                                    else:
                                        # íŒŒì¼ì´ ì—†ìœ¼ë©´ í”Œë ˆì´ìŠ¤í™€ë”ë¡œ ìœ ì§€
                                        print(f"   âš ï¸ ì´ë¯¸ì§€ íŒŒì¼ ì—†ìŒ: IMG_{img_num:03d}")
                                else:
                                    print(f"   âš ï¸ ì´ë¯¸ì§€ ë²ˆí˜¸ {img_num} ì‚¬ìš©í•  ìˆ˜ ì—†ìŒ")
                        
                        converted_options.append(option_str)
                    
                    question['options'] = converted_options
                
                # ì§ˆë¬¸ í…ìŠ¤íŠ¸ì—ì„œë„ ì´ë¯¸ì§€ ì°¸ì¡° ë³€í™˜
                question_text = question.get('question_text', '')
                if 'IMG_' in question_text and '_IMAGE' in question_text:
                    img_pattern = re.findall(r'IMG_(\d+)_IMAGE', question_text)
                    for img_num_str in img_pattern:
                        img_num = int(img_num_str)
                        if img_num in available_images:
                            img_ext = self._find_image_extension(upload_id, img_num)
                            if img_ext:
                                img_path = f"/images/upload_{upload_id}/IMG_{img_num:03d}.{img_ext}"
                                markdown_img = f"![IMG_{img_num:03d}]({img_path})"
                                question_text = question_text.replace(f"IMG_{img_num_str}_IMAGE", markdown_img)
                                converted_count += 1
                    
                    question['question_text'] = question_text
                
                # ì§€ë¬¸ì—ì„œë„ ì´ë¯¸ì§€ ì°¸ì¡° ë³€í™˜
                passage = question.get('passage', '')
                if 'IMG_' in passage and '_IMAGE' in passage:
                    img_pattern = re.findall(r'IMG_(\d+)_IMAGE', passage)
                    for img_num_str in img_pattern:
                        img_num = int(img_num_str)
                        if img_num in available_images:
                            img_ext = self._find_image_extension(upload_id, img_num)
                            if img_ext:
                                img_path = f"/images/upload_{upload_id}/IMG_{img_num:03d}.{img_ext}"
                                markdown_img = f"![IMG_{img_num:03d}]({img_path})"
                                passage = passage.replace(f"IMG_{img_num_str}_IMAGE", markdown_img)
                                converted_count += 1
                    
                    question['passage'] = passage
            
            print(f"   âœ… ì´ë¯¸ì§€ ì°¸ì¡° ë³€í™˜: {converted_count}ê°œ ë³€í™˜ë¨")
            return questions
            
        except Exception as e:
            print(f"âŒ ì´ë¯¸ì§€ ì°¸ì¡° ë³€í™˜ ì˜¤ë¥˜: {e}")
            return questions
    
    def _find_image_extension(self, upload_id: int, img_num: int) -> Optional[str]:
        """íŠ¹ì • ì´ë¯¸ì§€ ë²ˆí˜¸ì— í•´ë‹¹í•˜ëŠ” íŒŒì¼ì˜ í™•ì¥ì ì°¾ê¸°"""
        try:
            import os
            
            image_dir = f"static/images/upload_{upload_id}"
            if not os.path.exists(image_dir):
                return None
            
            # ì¼ë°˜ì ì¸ ì´ë¯¸ì§€ í™•ì¥ìë“¤ í™•ì¸
            extensions = ['png', 'jpg', 'jpeg', 'gif', 'bmp', 'webp']
            
            for ext in extensions:
                img_file = f"IMG_{img_num:03d}.{ext}"
                if os.path.exists(os.path.join(image_dir, img_file)):
                    return ext
            
            return None
            
        except Exception as e:
            print(f"âš ï¸ ì´ë¯¸ì§€ í™•ì¥ì ì°¾ê¸° ì˜¤ë¥˜: {e}")
            return None
    
    async def _alternative_enhancement_pipeline(self, upload_id: int, basic_questions: List[Dict], 
                                              markdown_content: str, db: Session) -> List[Dict]:
        """OCR ì—†ì´ íŒ¨í„´ ê¸°ë°˜ìœ¼ë¡œ ë¬¸ì œ ë³´ì™„"""
        try:
            print(f"ğŸ“ ëŒ€ì•ˆ ë³´ì™„ íŒŒì´í”„ë¼ì¸ ì‹œì‘...")
            
            enhanced_questions = basic_questions.copy()
            
            # 1. ë¶ˆì™„ì „í•œ ì„ íƒì§€ ë¬¸ì œ ê°ì§€ ë° ë³´ì™„
            incomplete_questions = []
            for question in enhanced_questions:
                choices = question.get('options', [])
                if len(choices) < 3:  # 3ê°œ ë¯¸ë§Œì€ ë¶ˆì™„ì „ìœ¼ë¡œ íŒë‹¨
                    incomplete_questions.append(question)
            
            print(f"   ğŸ” ë¶ˆì™„ì „ ë¬¸ì œ ë°œê²¬: {len(incomplete_questions)}ê°œ")
            
            # 2. ë§ˆí¬ë‹¤ìš´ì—ì„œ ëˆ„ë½ëœ ì„ íƒì§€ íŒ¨í„´ ë§¤ì¹­
            for incomplete_q in incomplete_questions:
                question_number = incomplete_q.get('question_number', 0)
                if question_number > 0:
                    additional_choices = self._find_missing_choices_in_markdown(
                        question_number, markdown_content
                    )
                    if additional_choices:
                        current_choices = incomplete_q.get('options', [])
                        merged_choices = current_choices + additional_choices
                        incomplete_q['options'] = merged_choices[:5]  # ìµœëŒ€ 5ê°œê¹Œì§€
                        print(f"   âœ… Q{question_number} ì„ íƒì§€ ë³´ì™„: {len(current_choices)} â†’ {len(merged_choices)}ê°œ")
            
            # 3. ëˆ„ë½ëœ ë¬¸ì œ ë²ˆí˜¸ ê°ì§€ ë° ë³µêµ¬ ì‹œë„
            existing_numbers = {q.get('question_number', 0) for q in enhanced_questions}
            max_number = max(existing_numbers) if existing_numbers else 0
            
            missing_numbers = []
            for i in range(1, max_number + 1):
                if i not in existing_numbers:
                    missing_numbers.append(i)
            
            if missing_numbers:
                print(f"   ğŸ” ëˆ„ë½ëœ ë¬¸ì œ ë²ˆí˜¸ ê°ì§€: {missing_numbers}")
                recovered_questions = self._recover_missing_questions_from_markdown(
                    missing_numbers, markdown_content
                )
                enhanced_questions.extend(recovered_questions)
                print(f"   âœ… ëˆ„ë½ ë¬¸ì œ ë³µêµ¬: {len(recovered_questions)}ê°œ")
            
            # 4. ë¬¸ì œ ë²ˆí˜¸ìˆœ ì •ë ¬
            enhanced_questions.sort(key=lambda x: x.get('question_number', 0))
            
            print(f"ğŸ“ ëŒ€ì•ˆ ë³´ì™„ ì™„ë£Œ: {len(basic_questions)} â†’ {len(enhanced_questions)} ë¬¸ì œ")
            return enhanced_questions
            
        except Exception as e:
            print(f"âŒ ëŒ€ì•ˆ ë³´ì™„ ì˜¤ë¥˜: {e}")
            return basic_questions
    
    def _find_missing_choices_in_markdown(self, question_number: int, markdown_content: str) -> List[str]:
        """ë§ˆí¬ë‹¤ìš´ì—ì„œ íŠ¹ì • ë¬¸ì œì˜ ëˆ„ë½ëœ ì„ íƒì§€ ì°¾ê¸°"""
        try:
            import re
            
            # ë¬¸ì œ ë²ˆí˜¸ íŒ¨í„´ìœ¼ë¡œ í•´ë‹¹ ë¬¸ì œ ì˜ì—­ ì°¾ê¸°
            question_patterns = [
                rf'ë¬¸ì œ {question_number}ë²ˆ(.*?)(?=ë¬¸ì œ \d+ë²ˆ|$)',
                rf'{question_number}ë²ˆ(.*?)(?=\d+ë²ˆ|$)',
                rf'ë¬¸ì œ\s*{question_number}\s*(.*?)(?=ë¬¸ì œ\s*\d+|$)'
            ]
            
            question_text = ""
            for pattern in question_patterns:
                match = re.search(pattern, markdown_content, re.DOTALL)
                if match:
                    question_text = match.group(1)
                    break
            
            if not question_text:
                return []
            
            # ì„ íƒì§€ íŒ¨í„´ ë§¤ì¹­
            choices = []
            choice_patterns = [
                r'ì„ íƒì§€\s*(\d+):\s*([^\nì„ íƒì§€]+)',
                r'â‘ \s*([^\nâ‘¡â‘¢â‘£â‘¤]+)',
                r'â‘¡\s*([^\nâ‘ â‘¢â‘£â‘¤]+)', 
                r'â‘¢\s*([^\nâ‘ â‘¡â‘£â‘¤]+)',
                r'â‘£\s*([^\nâ‘ â‘¡â‘¢â‘¤]+)',
                r'â‘¤\s*([^\nâ‘ â‘¡â‘¢â‘£]+)'
            ]
            
            for pattern in choice_patterns:
                matches = re.finditer(pattern, question_text)
                for match in matches:
                    if pattern.startswith(r'ì„ íƒì§€'):
                        choice_text = match.group(2).strip()
                    else:
                        choice_text = match.group(1).strip()
                    
                    if choice_text and len(choice_text) > 1:
                        choices.append(choice_text)
            
            return choices[:5]  # ìµœëŒ€ 5ê°œê¹Œì§€
            
        except Exception as e:
            print(f"   âš ï¸ ì„ íƒì§€ ë§¤ì¹­ ì˜¤ë¥˜: {e}")
            return []
    
    def _recover_missing_questions_from_markdown(self, missing_numbers: List[int], markdown_content: str) -> List[Dict]:
        """ë§ˆí¬ë‹¤ìš´ì—ì„œ ëˆ„ë½ëœ ë¬¸ì œ ë³µêµ¬"""
        try:
            import re
            
            recovered_questions = []
            
            for missing_num in missing_numbers:
                # ë¬¸ì œ íŒ¨í„´ ë§¤ì¹­
                question_patterns = [
                    rf'ë¬¸ì œ {missing_num}ë²ˆ\s*([^\n]*(?:\n(?!ë¬¸ì œ\s*\d+ë²ˆ)[^\n]*)*)',
                    rf'{missing_num}ë²ˆ\s*([^\n]*(?:\n(?!\d+ë²ˆ)[^\n]*)*)'
                ]
                
                for pattern in question_patterns:
                    match = re.search(pattern, markdown_content, re.MULTILINE)
                    if match:
                        question_content = match.group(1).strip()
                        if len(question_content) > 10:  # ìµœì†Œ ê¸¸ì´ í•„í„°
                            
                            # ì„ íƒì§€ ì¶”ì¶œ
                            choices = self._find_missing_choices_in_markdown(missing_num, markdown_content)
                            
                            recovered_question = {
                                'question_id': f'RECOVERED_{missing_num}',
                                'question_number': missing_num,
                                'question_text': question_content,
                                'passage': '',
                                'options': choices,
                                'correct_answer': '',
                                'recovery_source': 'markdown_pattern',
                                'chunk_origin': 'recovered'
                            }
                            
                            recovered_questions.append(recovered_question)
                            print(f"   âœ… Q{missing_num} ë³µêµ¬ ì„±ê³µ")
                            break
            
            return recovered_questions
            
        except Exception as e:
            print(f"   âš ï¸ ë¬¸ì œ ë³µêµ¬ ì˜¤ë¥˜: {e}")
            return []

    def safe_json_parse(self, response_text: str, chunk_id: int = 0) -> tuple[Optional[Dict], Optional[str]]:
        """ì•ˆì „í•œ JSON íŒŒì‹± - ë¶€ë¶„ ë³µêµ¬ ë° ë‹¤ë‹¨ê³„ ì‹œë„"""
        import json
        import re
        
        try:
            # 1ë‹¨ê³„: ì½”ë“œíœìŠ¤ ì œê±° ë° ì „ì²˜ë¦¬
            clean_text = response_text.strip()
            
            # ì½”ë“œíœìŠ¤ ì œê±°
            if clean_text.startswith('```json'):
                clean_text = clean_text.split('```json', 1)[1].rsplit('```', 1)[0]
            elif clean_text.startswith('```'):
                clean_text = clean_text.split('```', 1)[1].rsplit('```', 1)[0]
            
            # 2ë‹¨ê³„: ë§ˆì§€ë§‰ ìœ íš¨í•œ ì¤‘ê´„í˜¸ê¹Œì§€ ìë¥´ê¸°
            last_brace = max(clean_text.rfind("}"), clean_text.rfind("]"))
            if last_brace != -1:
                clean_text = clean_text[:last_brace + 1]
            
            # 3ë‹¨ê³„: ì§ì ‘ íŒŒì‹± ì‹œë„
            try:
                result = json.loads(clean_text)
                return result, None
            except json.JSONDecodeError as e:
                print(f"   âš ï¸ Chunk {chunk_id} ì§ì ‘ JSON íŒŒì‹± ì‹¤íŒ¨: {e}")
            
            # 4ë‹¨ê³„: ì¼ë°˜ì ì¸ ì˜¤ë¥˜ ìˆ˜ì • í›„ ì¬ì‹œë„
            fixed_text = self._fix_common_json_errors(clean_text)
            try:
                result = json.loads(fixed_text)
                print(f"   âœ… Chunk {chunk_id} JSON ì˜¤ë¥˜ ìˆ˜ì •ìœ¼ë¡œ ë³µêµ¬")
                return result, "fixed"
            except json.JSONDecodeError as e:
                print(f"   âš ï¸ Chunk {chunk_id} ìˆ˜ì •ëœ JSONë„ íŒŒì‹± ì‹¤íŒ¨: {e}")
            
            # 5ë‹¨ê³„: ë¶€ë¶„ ë°°ì—´ ì¶”ì¶œ ì‹œë„
            partial_result = self._extract_partial_questions(clean_text, chunk_id)
            if partial_result:
                print(f"   ğŸ”§ Chunk {chunk_id} ë¶€ë¶„ ì¶”ì¶œ ì„±ê³µ: {len(partial_result.get('questions', []))}ê°œ ë¬¸ì œ")
                return partial_result, "partial"
            
            # 6ë‹¨ê³„: ìµœí›„ ìˆ˜ë‹¨ - ë¹ˆ ê²°ê³¼ ë°˜í™˜
            print(f"   âŒ Chunk {chunk_id} ëª¨ë“  JSON ë³µêµ¬ ë°©ë²• ì‹¤íŒ¨")
            return {"questions": []}, "empty"
            
        except Exception as e:
            print(f"âŒ Chunk {chunk_id} JSON íŒŒì‹± ì¤‘ ì˜ˆì™¸ ë°œìƒ: {e}")
            return {"questions": []}, f"error:{e}"
    
    def _fix_common_json_errors(self, json_str: str) -> str:
        """ì¼ë°˜ì ì¸ JSON ì˜¤ë¥˜ ìˆ˜ì •"""
        import re
        
        # 1. ë§ˆì§€ë§‰ ì‰¼í‘œ ì œê±°
        json_str = re.sub(r',(\s*[}\]])', r'\1', json_str)
        
        # 2. ë”°ì˜´í‘œê°€ ì—†ëŠ” í‚¤ ìˆ˜ì •
        json_str = re.sub(r'([{,]\s*)(\w+):', r'\1"\2":', json_str)
        
        # 3. ë¶ˆì™„ì „í•œ ë¬¸ìì—´ ê°’ ìˆ˜ì •
        json_str = re.sub(r': ([^",{\[\]]+)([,}])', r': "\1"\2', json_str)
        
        # 4. ê°œí–‰ ë¬¸ì JSON-safe ì²˜ë¦¬
        json_str = json_str.replace('\n', '\\n').replace('\r', '\\r').replace('\t', '\\t')
        
        # 5. ì¤‘ë³µ ë”°ì˜´í‘œ ìˆ˜ì •
        json_str = re.sub(r'""([^"]*?)""', r'"\1"', json_str)
        
        # 6. ë¶ˆì™„ì „í•œ ì¤‘ê´„í˜¸/ëŒ€ê´„í˜¸ ë‹«ê¸°
        open_braces = json_str.count('{') - json_str.count('}')
        open_brackets = json_str.count('[') - json_str.count(']')
        
        if open_braces > 0:
            json_str += '}' * open_braces
        if open_brackets > 0:
            json_str += ']' * open_brackets
        
        return json_str
    
    def _extract_partial_questions(self, text: str, chunk_id: int) -> Optional[Dict]:
        """ë¶€ë¶„ì ìœ¼ë¡œ ì†ìƒëœ JSONì—ì„œ ë¬¸ì œ í•­ëª©ë“¤ ì¶”ì¶œ"""
        import re
        import json
        
        try:
            # questions ë°°ì—´ ë¶€ë¶„ ì°¾ê¸°
            questions_match = re.search(r'"questions"\s*:\s*(\[[\s\S]*)', text)
            if not questions_match:
                return None
            
            array_text = questions_match.group(1)
            
            # ê°œë³„ ë¬¸ì œ ê°ì²´ë“¤ ì¶”ì¶œ ì‹œë„
            questions = []
            
            # ë¬¸ì œ ê°ì²´ íŒ¨í„´ìœ¼ë¡œ ë¶„ë¦¬
            question_pattern = r'\{\s*"question_number"\s*:\s*(\d+)[\s\S]*?\}'
            
            # ì¤‘ê´„í˜¸ ê· í˜• ë§ì¶¤ì„ ê³ ë ¤í•œ ë¶„ë¦¬
            brace_count = 0
            current_obj = ""
            in_question = False
            
            for char in array_text:
                if char == '{':
                    if brace_count == 0:
                        current_obj = ""
                        in_question = True
                    brace_count += 1
                elif char == '}':
                    brace_count -= 1
                    if brace_count == 0 and in_question:
                        current_obj += char
                        # ê°œë³„ ê°ì²´ íŒŒì‹± ì‹œë„
                        try:
                            obj = json.loads(current_obj)
                            if obj.get('question_number'):
                                questions.append(obj)
                                print(f"      âœ… Q{obj.get('question_number')} ë¶€ë¶„ ì¶”ì¶œ ì„±ê³µ")
                        except:
                            # ê°œë³„ ê°ì²´ë„ ì‹¤íŒ¨í•˜ë©´ ì •ê·œì‹ìœ¼ë¡œ ê¸°ë³¸ ì •ë³´ë¼ë„ ì¶”ì¶œ
                            try:
                                q_num_match = re.search(r'"question_number"\s*:\s*(\d+)', current_obj)
                                q_text_match = re.search(r'"question_text"\s*:\s*"([^"]*)"', current_obj)
                                
                                if q_num_match:
                                    fallback_obj = {
                                        'question_number': int(q_num_match.group(1)),
                                        'question_text': q_text_match.group(1) if q_text_match else f"ë¬¸ì œ {q_num_match.group(1)}",
                                        'options': [],
                                        'passage': '',
                                        'recovery_method': 'regex_fallback'
                                    }
                                    questions.append(fallback_obj)
                                    print(f"      ğŸ”§ Q{q_num_match.group(1)} ì •ê·œì‹ í´ë°±ìœ¼ë¡œ ì¶”ì¶œ")
                            except:
                                pass
                        
                        current_obj = ""
                        in_question = False
                        continue
                
                if in_question:
                    current_obj += char
            
            if questions:
                return {
                    "questions": questions,
                    "extraction_method": "partial_recovery",
                    "recovered_count": len(questions)
                }
            
            return None
            
        except Exception as e:
            print(f"   âŒ ë¶€ë¶„ ì¶”ì¶œ ì¤‘ ì˜¤ë¥˜: {e}")
            return None
    
    def _create_overlap_chunks(self, pages: List[str]) -> List[Dict]:
        """ì˜¤ë²„ë© ìœˆë„ìš° ì²­í¬ ìƒì„± - í˜ì´ì§€ ê²½ê³„ ë¬¸ì œ í•´ê²°"""
        chunks = []
        
        # 1ë‹¨ê³„: ê°œë³„ í˜ì´ì§€ ì²­í¬ (ì•ˆì •ì„± í™•ë³´)
        for i, page_content in enumerate(pages):
            if page_content.strip():
                chunks.append({
                    'content': f"# Page {i+1}\n\n{page_content}",
                    'page_start': i + 1,
                    'page_end': i + 1,
                    'chunk_type': 'single_page',
                    'estimated_questions': 8
                })
        
        # 2ë‹¨ê³„: ê²½ê³„ ì˜¤ë²„ë© ì²­í¬ (Q11, Q41 ë“± ê²½ê³„ ë¬¸ì œ í•´ê²°)
        for i in range(len(pages) - 1):
            current_page = pages[i].strip()
            next_page = pages[i + 1].strip()
            
            if current_page and next_page:
                # í˜ì´ì§€ ë ë¶€ë¶„ê³¼ ì‹œì‘ ë¶€ë¶„ì„ ê²°í•©
                boundary_content = self._create_boundary_chunk(current_page, next_page, i + 1)
                
                chunks.append({
                    'content': boundary_content,
                    'page_start': i + 1,
                    'page_end': i + 2,
                    'chunk_type': 'boundary_overlap',
                    'estimated_questions': 4,  # ê²½ê³„ì—ì„œëŠ” ë³´í†µ ì ì€ ìˆ˜
                    'boundary_pages': f"{i+1}-{i+2}"
                })
        
        # 3ë‹¨ê³„: 2í˜ì´ì§€ ìœˆë„ìš° ì²­í¬ (ì¤‘ê°„ í¬ê¸° ì²­í¬ë¡œ ì•ˆì •ì„± ê°•í™”)
        for i in range(0, len(pages) - 1, 2):
            page_group = []
            page_numbers = []
            
            for j in range(2):  # 2í˜ì´ì§€ì”©
                if i + j < len(pages) and pages[i + j].strip():
                    page_group.append(f"# Page {i+j+1}\n\n{pages[i + j]}")
                    page_numbers.append(i + j + 1)
            
            if len(page_group) >= 2:
                chunks.append({
                    'content': '\n\n---\n\n'.join(page_group),
                    'page_start': page_numbers[0],
                    'page_end': page_numbers[-1],
                    'chunk_type': 'two_page_window',
                    'estimated_questions': 16  # 2í˜ì´ì§€ ì•½ 16ë¬¸ì œ
                })
        
        print(f"   ğŸ“¦ Individual pages: {len([c for c in chunks if c['chunk_type'] == 'single_page'])}")
        print(f"   ğŸ”— Boundary overlaps: {len([c for c in chunks if c['chunk_type'] == 'boundary_overlap'])}")
        print(f"   ğŸªŸ Two-page windows: {len([c for c in chunks if c['chunk_type'] == 'two_page_window'])}")
        
        return chunks
    
    def _create_boundary_chunk(self, current_page: str, next_page: str, page_num: int) -> str:
        """í˜ì´ì§€ ê²½ê³„ ì²­í¬ ìƒì„± - ë¬¸ì œê°€ ì˜ë¦° ë¶€ë¶„ì„ í¬í•¨"""
        
        # í˜„ì¬ í˜ì´ì§€ì˜ ë§ˆì§€ë§‰ ë¶€ë¶„ (ì•½ 30% ì •ë„)
        current_lines = current_page.split('\n')
        current_tail = '\n'.join(current_lines[int(len(current_lines) * 0.7):])
        
        # ë‹¤ìŒ í˜ì´ì§€ì˜ ì²« ë¶€ë¶„ (ì•½ 30% ì •ë„)  
        next_lines = next_page.split('\n')
        next_head = '\n'.join(next_lines[:int(len(next_lines) * 0.3)])
        
        boundary_content = f"""# Boundary Pages {page_num}-{page_num+1}

## End of Page {page_num}
{current_tail}

---

## Start of Page {page_num+1}
{next_head}
"""
        
        return boundary_content
    
    def normalize_question_options(self, question: Dict) -> Dict:
        """ì„ íƒì§€ ë¼ë²¨ í†µì¼ ë° ëˆ„ë½ ë³´ê°•"""
        try:
            import re
            
            options = question.get('options', [])
            if not options:
                return question
            
            # í‘œì¤€ ë¼ë²¨ ìˆœì„œ
            STANDARD_LABELS = ["A", "B", "C", "D", "E"]
            KOREAN_LABELS = ["â‘ ", "â‘¡", "â‘¢", "â‘£", "â‘¤"]
            
            normalized_options = []
            
            for option in options:
                if not option:
                    continue
                    
                option_str = str(option).strip()
                
                # ë¼ë²¨ íŒ¨í„´ ë§¤ì¹­
                label_match = re.match(r'\s*(?:\(?([A-Eâ‘ -â‘¤])\)?[.\)]?)\s*(.+)$', option_str)
                
                if label_match:
                    raw_label = label_match.group(1)
                    text = label_match.group(2).strip()
                    
                    # ë¼ë²¨ ë³€í™˜
                    if raw_label in KOREAN_LABELS:
                        standard_label = STANDARD_LABELS[KOREAN_LABELS.index(raw_label)]
                    elif raw_label in STANDARD_LABELS:
                        standard_label = raw_label
                    else:
                        standard_label = "A"  # ê¸°ë³¸ê°’
                    
                    normalized_options.append({
                        "label": standard_label,
                        "text": text,
                        "raw_label": raw_label
                    })
                else:
                    # ë¼ë²¨ì´ ì—†ëŠ” ê²½ìš°
                    normalized_options.append({
                        "label": STANDARD_LABELS[len(normalized_options)] if len(normalized_options) < 5 else "E",
                        "text": option_str,
                        "raw_label": None
                    })
            
            # ëˆ„ë½ëœ ì„ íƒì§€ ë³´ê°• (A-Dê°€ ëª¨ë‘ ìˆì–´ì•¼ í•¨)
            seen_labels = {opt["label"] for opt in normalized_options}
            
            if len(normalized_options) < 4:  # ì„ íƒì§€ê°€ ë¶€ì¡±í•œ ê²½ìš°
                for label in STANDARD_LABELS[:4]:  # A, B, C, D
                    if label not in seen_labels:
                        normalized_options.append({
                            "label": label,
                            "text": "",
                            "raw_label": None,
                            "missing": True
                        })
            
            # ë¼ë²¨ ìˆœìœ¼ë¡œ ì •ë ¬
            normalized_options.sort(key=lambda x: STANDARD_LABELS.index(x["label"]) if x["label"] in STANDARD_LABELS else 999)
            
            # ì›ë˜ í˜•íƒœë¡œ ë³€í™˜ (í•˜ìœ„ í˜¸í™˜ì„±)
            question_copy = question.copy()
            question_copy['options'] = [opt["text"] for opt in normalized_options[:4]]  # ìµœëŒ€ 4ê°œ
            question_copy['options_metadata'] = normalized_options  # ë©”íƒ€ë°ì´í„° ë³´ì¡´
            
            return question_copy
            
        except Exception as e:
            print(f"âš ï¸ ì„ íƒì§€ ì •ê·œí™” ì˜¤ë¥˜: {e}")
            return question
    
    def merge_duplicate_questions(self, questions: List[Dict]) -> List[Dict]:
        """ì¤‘ë³µ ë¬¸ì œ ê°ì§€ ë° ë³‘í•©"""
        try:
            if not questions:
                return questions
                
            # ë¬¸ì œ ë²ˆí˜¸ë³„ ê·¸ë£¹í™”
            question_groups = {}
            
            for question in questions:
                qno = question.get('question_number')
                if qno is None:
                    continue
                    
                if qno not in question_groups:
                    question_groups[qno] = []
                question_groups[qno].append(question)
            
            merged_questions = []
            
            for qno, group in question_groups.items():
                if len(group) == 1:
                    # ì¤‘ë³µ ì—†ìŒ
                    merged_questions.append(self.normalize_question_options(group[0]))
                else:
                    # ì¤‘ë³µ ë°œê²¬ - ê°€ì¥ ì™„ì „í•œ ë²„ì „ ì„ íƒ
                    print(f"ğŸ”„ ë¬¸ì œ {qno}ë²ˆ ì¤‘ë³µ ë°œê²¬: {len(group)}ê°œ ë²„ì „")
                    
                    best_question = self._select_best_question_version(group)
                    merged_questions.append(self.normalize_question_options(best_question))
            
            # ë¬¸ì œ ë²ˆí˜¸ìˆœ ì •ë ¬
            merged_questions.sort(key=lambda x: x.get('question_number', 0))
            
            return merged_questions
            
        except Exception as e:
            print(f"âŒ ì¤‘ë³µ ë¬¸ì œ ë³‘í•© ì˜¤ë¥˜: {e}")
            return questions
    
    def _select_best_question_version(self, versions: List[Dict]) -> Dict:
        """ì—¬ëŸ¬ ë²„ì „ ì¤‘ ê°€ì¥ ì™„ì „í•œ ë¬¸ì œ ì„ íƒ"""
        try:
            best_question = versions[0]
            best_score = 0
            
            for version in versions:
                score = 0
                
                # ì§ˆë¬¸ í…ìŠ¤íŠ¸ ì ìˆ˜ (ê¸¸ì´ì™€ ì™„ì„±ë„)
                question_text = version.get('question_text', '')
                if question_text:
                    score += min(len(question_text.strip()), 100)  # ìµœëŒ€ 100ì 
                
                # ì„ íƒì§€ ì ìˆ˜
                options = version.get('options', [])
                valid_options = [opt for opt in options if opt and str(opt).strip()]
                score += len(valid_options) * 25  # ì„ íƒì§€ë‹¹ 25ì 
                
                # ì§€ë¬¸ ì ìˆ˜
                passage = version.get('passage', '')
                if passage and len(passage.strip()) > 10:
                    score += 50
                
                # ë©”íƒ€ë°ì´í„° ì ìˆ˜
                if version.get('chunk_origin'):
                    score += 10
                
                # ì²­í¬ íƒ€ì…ë³„ ë³´ì •
                chunk_type = version.get('chunk_type', '')
                if 'boundary_overlap' in chunk_type:
                    score += 20  # ê²½ê³„ ì²­í¬ëŠ” í˜ì´ì§€ ê²½ê³„ ë¬¸ì œ í•´ê²°ì— ìœ ë¦¬
                elif 'single_page' in chunk_type:
                    score += 10
                
                if score > best_score:
                    best_score = score
                    best_question = version
            
            # ë³‘í•© ì •ë³´ ì¶”ê°€
            best_question['merge_info'] = {
                'total_versions': len(versions),
                'selected_score': best_score,
                'chunk_sources': [v.get('chunk_origin') for v in versions]
            }
            
            print(f"   âœ… ìµœê³  ì ìˆ˜ ë²„ì „ ì„ íƒ: {best_score}ì ")
            return best_question
            
        except Exception as e:
            print(f"   âŒ ìµœì  ë²„ì „ ì„ íƒ ì˜¤ë¥˜: {e}")
            return versions[0]
    
    async def _parse_claude_text_response(self, text: str, chunk_id: int) -> Dict:
        """Claudeì˜ í…ìŠ¤íŠ¸ ì‘ë‹µì„ êµ¬ì¡°í™”ëœ ë°ì´í„°ë¡œ íŒŒì‹±"""
        import re
        
        try:
            questions = []
            
            # ë¬¸ì œ íŒ¨í„´ ë§¤ì¹­
            question_patterns = [
                r'ë¬¸ì œ\s*(\d+)ë²ˆ[\s\S]*?(?=ë¬¸ì œ\s*\d+ë²ˆ|$)',
                r'(\d+)\.\s*([^\n]+)[\s\S]*?(?=\d+\.\s*[^\n]+|$)',
                r'Q(\d+)[:\s]*([^\n]+)[\s\S]*?(?=Q\d+|$)'
            ]
            
            for pattern in question_patterns:
                matches = re.finditer(pattern, text, re.MULTILINE)
                
                for match in matches:
                    try:
                        question_num = int(match.group(1))
                        question_text = match.group(2) if len(match.groups()) > 1 else match.group(0)
                        
                        # ì„ íƒì§€ ì¶”ì¶œ
                        options = []
                        choice_patterns = [r'[â‘ â‘¡â‘¢â‘£â‘¤]', r'[1-5]\)', r'[ê°€ë‚˜ë‹¤ë¼ë§ˆ]']
                        
                        for choice_pattern in choice_patterns:
                            choices = re.findall(f'{choice_pattern}[^â‘ â‘¡â‘¢â‘£â‘¤1-5ê°€ë‚˜ë‹¤ë¼ë§ˆ]*', question_text)
                            if choices:
                                options = [choice.strip() for choice in choices]
                                break
                        
                        if question_num and question_text.strip():
                            questions.append({
                                'question_id': f'Q{chunk_id:02d}_{question_num:03d}',
                                'question_number': question_num,
                                'question_text': question_text.strip(),
                                'passage': '',
                                'options': options[:4] if len(options) > 4 else options,
                                'correct_answer': '',
                                'parsed_from_text': True
                            })
                    except (ValueError, IndexError):
                        continue
                
                if questions:  # ì„±ê³µì ìœ¼ë¡œ íŒŒì‹±ëœ ê²½ìš° ì¤‘ë‹¨
                    break
            
            print(f"í…ìŠ¤íŠ¸ íŒŒì‹±ìœ¼ë¡œ {len(questions)}ê°œ ë¬¸ì œ ì¶”ì¶œ")
            return {
                'questions': questions,
                'study_materials': [],
                'chapters': []
            }
            
        except Exception as e:
            print(f"í…ìŠ¤íŠ¸ íŒŒì‹± ì‹¤íŒ¨: {e}")
            return None
    
    def _fix_incomplete_json(self, json_content: str) -> str:
        """ë¶ˆì™„ì „í•œ JSON êµ¬ì¡° ìˆ˜ì •"""
        try:
            # ë§ˆì§€ë§‰ì— ëˆ„ë½ëœ ì¤‘ê´„í˜¸ ì¶”ê°€
            open_braces = json_content.count('{')
            close_braces = json_content.count('}')
            if open_braces > close_braces:
                json_content += '}' * (open_braces - close_braces)
            
            # ë§ˆì§€ë§‰ì— ëˆ„ë½ëœ ëŒ€ê´„í˜¸ ì¶”ê°€
            open_brackets = json_content.count('[')
            close_brackets = json_content.count(']')
            if open_brackets > close_brackets:
                json_content += ']' * (open_brackets - close_brackets)
            
            # ë§ˆì§€ë§‰ ì‰¼í‘œ ì œê±° (trailing comma)
            json_content = re.sub(r',(\s*[}\]])', r'\1', json_content)
            
            # ëˆ„ë½ëœ ë”°ì˜´í‘œ ìˆ˜ì •
            json_content = re.sub(r'(\w+):', r'"\1":', json_content)
            
            return json_content
        except Exception:
            return json_content
    
    async def _extract_images_from_pdf(self, upload_id: int, pdf_path: str) -> Dict[str, Any]:
        """PDFì—ì„œ ì´ë¯¸ì§€ ì¶”ì¶œ ë° ì €ì¥"""
        try:
            print(f"ğŸ–¼ï¸ PDFì—ì„œ ì´ë¯¸ì§€ ì¶”ì¶œ ì‹œì‘ - Upload {upload_id}")
            
            # ì´ë¯¸ì§€ ì €ì¥ ë””ë ‰í† ë¦¬ ìƒì„±
            images_dir = Path(f"static/images/upload_{upload_id}")
            images_dir.mkdir(parents=True, exist_ok=True)
            
            doc = fitz.open(pdf_path)
            extracted_images = []
            image_counter = 1
            
            for page_num in range(len(doc)):
                page = doc[page_num]
                
                # í˜ì´ì§€ì˜ ì´ë¯¸ì§€ ê°ì²´ ì¶”ì¶œ
                image_list = page.get_images()
                
                for img_index, img in enumerate(image_list):
                    try:
                        # ì´ë¯¸ì§€ ë°ì´í„° ì¶”ì¶œ
                        xref = img[0]
                        base_image = doc.extract_image(xref)
                        image_bytes = base_image["image"]
                        image_ext = base_image["ext"]
                        
                        # ì´ë¯¸ì§€ íŒŒì¼ ì €ì¥
                        image_filename = f"IMG_{image_counter:03d}.{image_ext}"
                        image_path = images_dir / image_filename
                        
                        with open(image_path, "wb") as img_file:
                            img_file.write(image_bytes)
                        
                        # ì´ë¯¸ì§€ ë©”íƒ€ë°ì´í„° ìˆ˜ì§‘
                        image_info = {
                            "image_id": f"IMG_{image_counter:03d}",
                            "filename": image_filename,
                            "file_path": f"/static/images/upload_{upload_id}/{image_filename}",
                            "page_number": page_num + 1,
                            "image_type": "embedded",
                            "format": image_ext,
                            "size": len(image_bytes),
                            "web_path": f"/images/upload_{upload_id}/{image_filename}"
                        }
                        
                        extracted_images.append(image_info)
                        image_counter += 1
                        
                        print(f"âœ… ì´ë¯¸ì§€ ì¶”ì¶œ: {image_filename} (Page {page_num + 1})")
                        
                    except Exception as img_error:
                        print(f"âš ï¸ ì´ë¯¸ì§€ {img_index} ì¶”ì¶œ ì‹¤íŒ¨ (Page {page_num + 1}): {img_error}")
            
            doc.close()
            
            print(f"ğŸ–¼ï¸ ì´ë¯¸ì§€ ì¶”ì¶œ ì™„ë£Œ: {len(extracted_images)}ê°œ ì´ë¯¸ì§€")
            
            return {
                "success": True,
                "images_extracted": len(extracted_images),
                "images": extracted_images,
                "images_directory": str(images_dir)
            }
            
        except Exception as e:
            print(f"âŒ ì´ë¯¸ì§€ ì¶”ì¶œ ì‹¤íŒ¨: {str(e)}")
            return {
                "success": False,
                "error": str(e),
                "images": []
            }
    
    async def _detect_image_regions_in_page(self, upload_id: int, page_image_path: str, page_num: int) -> List[Dict]:
        """í˜ì´ì§€ ì´ë¯¸ì§€ì—ì„œ ê·¸ë¦¼/ë‹¤ì´ì–´ê·¸ë¨ ì˜ì—­ ê°ì§€ ë° ì¶”ì¶œ"""
        try:
            import cv2
            import numpy as np
            
            # í˜ì´ì§€ ì´ë¯¸ì§€ ë¡œë“œ
            page_img = cv2.imread(page_image_path)
            if page_img is None:
                return []
                
            height, width = page_img.shape[:2]
            gray = cv2.cvtColor(page_img, cv2.COLOR_BGR2GRAY)
            
            # ì´ë¯¸ì§€ ì˜ì—­ ê°ì§€ë¥¼ ìœ„í•œ ì „ì²˜ë¦¬
            binary = cv2.adaptiveThreshold(gray, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C, 
                                         cv2.THRESH_BINARY, 11, 2)
            
            # ìœ¤ê³½ì„  ê²€ì¶œ
            contours, _ = cv2.findContours(binary, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
            
            detected_regions = []
            region_counter = 1
            
            for contour in contours:
                # ì˜ì—­ í¬ê¸° í™•ì¸ (ë„ˆë¬´ ì‘ì€ ì˜ì—­ ì œì™¸)
                area = cv2.contourArea(contour)
                if area < 5000:  # ìµœì†Œ ì˜ì—­ í¬ê¸°
                    continue
                
                # ê²½ê³„ ì‚¬ê°í˜• ì¶”ì¶œ
                x, y, w, h = cv2.boundingRect(contour)
                
                # ì¢…íš¡ë¹„ í™•ì¸ (ê·¹ë‹¨ì ì¸ ë¹„ìœ¨ ì œì™¸)
                aspect_ratio = w / h
                if aspect_ratio < 0.2 or aspect_ratio > 5.0:
                    continue
                
                # ì´ë¯¸ì§€ ì˜ì—­ ì¶”ì¶œ
                region = page_img[y:y+h, x:x+w]
                
                # ì¶”ì¶œëœ ì˜ì—­ ì €ì¥
                images_dir = Path(f"static/images/upload_{upload_id}")
                region_filename = f"PAGE_{page_num}_REGION_{region_counter:02d}.png"
                region_path = images_dir / region_filename
                
                cv2.imwrite(str(region_path), region)
                
                region_info = {
                    "region_id": f"PAGE_{page_num}_REGION_{region_counter:02d}",
                    "filename": region_filename,
                    "file_path": str(region_path),
                    "web_path": f"/images/upload_{upload_id}/{region_filename}",
                    "page_number": page_num,
                    "coordinates": {"x": int(x), "y": int(y), "width": int(w), "height": int(h)},
                    "area": int(area),
                    "aspect_ratio": round(aspect_ratio, 2),
                    "type": "detected_region"
                }
                
                detected_regions.append(region_info)
                region_counter += 1
                
                print(f"ğŸ¯ ì˜ì—­ ê°ì§€: {region_filename} ({w}x{h}, ë¹„ìœ¨: {aspect_ratio:.2f})")
            
            return detected_regions
            
        except Exception as e:
            print(f"âš ï¸ ì´ë¯¸ì§€ ì˜ì—­ ê°ì§€ ì‹¤íŒ¨ (Page {page_num}): {e}")
            return []
    
    async def _verify_with_full_image(self, pdf_path: str, extracted_questions: List[Dict], pages_data: List[str]) -> List[Dict]:
        """ì „ì²´ ì—°ê²° ì´ë¯¸ì§€ë¥¼ í†µí•œ í˜ì´ì§€ ê²½ê³„ ì„ íƒì§€ ê²€ìˆ˜ ë° ë³µêµ¬"""
        try:
            print("\nğŸ” ì „ì²´ ì—°ê²° ì´ë¯¸ì§€ ìƒì„± ì¤‘...")
            
            # PDFë¥¼ ì „ì²´ ì—°ê²° ì´ë¯¸ì§€ë¡œ ë³€í™˜
            import fitz  # PyMuPDF
            pdf_doc = fitz.open(pdf_path)
            
            # ì „ì²´ í˜ì´ì§€ë¥¼ í•˜ë‚˜ì˜ ê¸´ ì´ë¯¸ì§€ë¡œ ì—°ê²°
            total_height = 0
            page_images = []
            
            for page_num in range(len(pdf_doc)):
                page = pdf_doc.load_page(page_num)
                mat = fitz.Matrix(2.0, 2.0)  # 2x í™•ëŒ€
                pix = page.get_pixmap(matrix=mat)
                img_data = pix.tobytes("png")
                
                from PIL import Image
                import io
                page_img = Image.open(io.BytesIO(img_data))
                page_images.append(page_img)
                total_height += page_img.height
            
            pdf_doc.close()
            
            # ëª¨ë“  í˜ì´ì§€ë¥¼ ì„¸ë¡œë¡œ ì—°ê²°
            if not page_images:
                print("âš ï¸ í˜ì´ì§€ ì´ë¯¸ì§€ ìƒì„± ì‹¤íŒ¨")
                return extracted_questions
            
            full_width = page_images[0].width
            full_image = Image.new('RGB', (full_width, total_height), 'white')
            
            current_y = 0
            for page_img in page_images:
                full_image.paste(page_img, (0, current_y))
                current_y += page_img.height
            
            # ì„ì‹œ íŒŒì¼ë¡œ ì €ì¥
            import tempfile
            import base64
            
            with tempfile.NamedTemporaryFile(suffix='.png', delete=False) as temp_file:
                full_image.save(temp_file.name, 'PNG', quality=90)
                
                # ì´ë¯¸ì§€ë¥¼ base64ë¡œ ë³€í™˜
                with open(temp_file.name, 'rb') as img_file:
                    image_data = base64.b64encode(img_file.read()).decode('utf-8')
            
            print(f"âœ… ì „ì²´ ì—°ê²° ì´ë¯¸ì§€ ìƒì„± ì™„ë£Œ ({full_width}x{total_height})")
            
            # í˜ì´ì§€ ê²½ê³„ ë¬¸ì œê°€ ìˆëŠ” ë¬¸ì œë“¤ ì‹ë³„
            incomplete_questions = []
            for question in extracted_questions:
                options = question.get('options', [])
                question_num = question.get('question_number', 0)
                
                # ì„ íƒì§€ ë¶€ì¡±ì´ ì˜ì‹¬ë˜ëŠ” ê²½ìš°ë“¤
                if (len(options) < 4 or  # 4ê°œ ë¯¸ë§Œ ì„ íƒì§€
                    any('ë‹¤ìŒ í˜ì´ì§€' in str(opt) for opt in options) or  # í˜ì´ì§€ ì°¸ì¡°
                    any('ê³„ì†' in str(opt) for opt in options) or  # ê³„ì† í‘œì‹œ
                    question.get('incomplete_choices', False)):  # ì´ì „ì— ë¶ˆì™„ì „ìœ¼ë¡œ í‘œì‹œë¨
                    
                    incomplete_questions.append({
                        'question': question,
                        'original_options_count': len(options),
                        'question_number': question_num
                    })
            
            if not incomplete_questions:
                print("âœ… í˜ì´ì§€ ê²½ê³„ ë¬¸ì œê°€ ìˆëŠ” ë¬¸ì œ ì—†ìŒ")
                return extracted_questions
            
            print(f"ğŸ” í˜ì´ì§€ ê²½ê³„ ë¬¸ì œ ì˜ì‹¬ ë¬¸ì œ: {len(incomplete_questions)}ê°œ")
            
            # Claudeë¥¼ í†µí•œ ì „ì²´ ì´ë¯¸ì§€ ë¶„ì„
            verification_prompt = f"""
ğŸ” **í˜ì´ì§€ ê²½ê³„ ì„ íƒì§€ ê²€ì¦ ì „ë¬¸ê°€**

**ì„ë¬´**: ì „ì²´ ì—°ê²° PDF ì´ë¯¸ì§€ë¥¼ ë¶„ì„í•˜ì—¬ í˜ì´ì§€ ê²½ê³„ë¡œ ì¸í•´ ì˜ë¦° ì„ íƒì§€ë¥¼ ì°¾ì•„ì„œ ì™„ì „í•˜ê²Œ ë³µêµ¬í•˜ì„¸ìš”.

**ë¶„ì„ ëŒ€ìƒ**: ë‹¤ìŒ ë¬¸ì œë“¤ì˜ ì„ íƒì§€ê°€ ì™„ì „í•œì§€ í™•ì¸í•˜ê³ , ëˆ„ë½ëœ ë¶€ë¶„ì„ ì°¾ì•„ì„œ ë³µêµ¬í•´ì£¼ì„¸ìš”:

{chr(10).join([f"ë¬¸ì œ {q['question_number']}ë²ˆ: í˜„ì¬ {q['original_options_count']}ê°œ ì„ íƒì§€" for q in incomplete_questions[:10]])}

**ê²€ì¦ ê·œì¹™**:
1. ê° ë¬¸ì œì˜ ì„ íƒì§€ê°€ â‘ â‘¡â‘¢â‘£â‘¤ ë˜ëŠ” 1)2)3)4)5) í˜•íƒœë¡œ ì™„ì „í•œì§€ í™•ì¸
2. í˜ì´ì§€ ëì—ì„œ ì˜ë¦° ì„ íƒì§€ë¥¼ ë‹¤ìŒ í˜ì´ì§€ ì‹œì‘ì—ì„œ ì°¾ì•„ ì—°ê²°
3. í‘œë‚˜ ê·¸ë˜í”„ê°€ í˜ì´ì§€ë¥¼ ê±¸ì³ ìˆëŠ” ê²½ìš° ì™„ì „í•œ ë°ì´í„° ë³µêµ¬
4. ì´ë¯¸ì§€ ì„ íƒì§€ì˜ ê²½ìš° IMG_XX_IMAGE í˜•íƒœë¡œ ì°¸ì¡°

**JSON ì¶œë ¥**:
{{
  "boundary_issues_found": [
    {{
      "question_number": 6,
      "issue_type": "incomplete_choices",
      "current_choices": ["â‘  í”„ë¡œì„¸ìŠ¤ ê´€ë¦¬", "â‘¡ ë©”ëª¨ë¦¬"],
      "recovered_choices": ["â‘  í”„ë¡œì„¸ìŠ¤ ê´€ë¦¬", "â‘¡ ë©”ëª¨ë¦¬ ê´€ë¦¬", "â‘¢ íŒŒì¼ ì‹œìŠ¤í…œ", "â‘£ ëª¨ë“  ê²ƒ"],
      "page_boundary_location": "í˜ì´ì§€ 2-3 ê²½ê³„"
    }}
  ]
}}
"""
            
            try:
                headers = {
                    'Content-Type': 'application/json',
                    'x-api-key': self.claude_client.api_key if self.claude_client else '',
                    'anthropic-version': '2023-06-01'
                }
                
                payload = {
                    "model": "claude-3-5-sonnet-20241022",
                    "max_tokens": 8000,
                    "messages": [
                        {
                            "role": "user", 
                            "content": [
                                {
                                    "type": "image",
                                    "source": {
                                        "type": "base64",
                                        "media_type": "image/png",
                                        "data": image_data
                                    }
                                },
                                {
                                    "type": "text",
                                    "text": verification_prompt
                                }
                            ]
                        }
                    ]
                }
                
                async with aiohttp.ClientSession() as session:
                    async with session.post(
                        "https://api.anthropic.com/v1/messages", 
                        headers=headers, 
                        json=payload,
                        timeout=aiohttp.ClientTimeout(total=120)
                    ) as response:
                        if response.status == 200:
                            result = await response.json()
                            verification_text = result['content'][0]['text']
                            
                            print(f"âœ… ì „ì²´ ì´ë¯¸ì§€ ê²€ì¦ ì™„ë£Œ")
                            
                            # JSON íŒŒì‹± ì‹œë„
                            try:
                                import json
                                import re
                                
                                # JSON ì¶”ì¶œ
                                json_match = re.search(r'\{.*\}', verification_text, re.DOTALL)
                                if json_match:
                                    verification_result = json.loads(json_match.group())
                                    
                                    boundary_issues = verification_result.get('boundary_issues_found', [])
                                    if boundary_issues:
                                        print(f"ğŸ”§ {len(boundary_issues)}ê°œ í˜ì´ì§€ ê²½ê³„ ë¬¸ì œ ë°œê²¬, ë³µêµ¬ ì¤‘...")
                                        
                                        # ë¬¸ì œë³„ ì„ íƒì§€ ë³µêµ¬
                                        for issue in boundary_issues:
                                            question_num = issue.get('question_number')
                                            recovered_choices = issue.get('recovered_choices', [])
                                            
                                            # í•´ë‹¹ ë¬¸ì œ ì°¾ì•„ì„œ ì„ íƒì§€ ì—…ë°ì´íŠ¸
                                            for question in extracted_questions:
                                                if question.get('question_number') == question_num:
                                                    old_count = len(question.get('options', []))
                                                    question['options'] = recovered_choices
                                                    question['page_boundary_recovered'] = True
                                                    
                                                    print(f"âœ… Q{question_num} ì„ íƒì§€ ë³µêµ¬: {old_count}ê°œ â†’ {len(recovered_choices)}ê°œ")
                                                    break
                                    else:
                                        print("â„¹ï¸ ì¶”ê°€ ë³µêµ¬ í•„ìš”í•œ í˜ì´ì§€ ê²½ê³„ ë¬¸ì œ ì—†ìŒ")
                                
                            except (json.JSONDecodeError, KeyError) as e:
                                print(f"âš ï¸ ê²€ì¦ ê²°ê³¼ íŒŒì‹± ì‹¤íŒ¨: {e}")
                        
                        else:
                            print(f"âš ï¸ ì „ì²´ ì´ë¯¸ì§€ ê²€ì¦ API ì˜¤ë¥˜: {response.status}")
            
            except Exception as api_error:
                print(f"âš ï¸ ì „ì²´ ì´ë¯¸ì§€ ê²€ì¦ ì¤‘ ì˜¤ë¥˜: {api_error}")
            
            # ì„ì‹œ íŒŒì¼ ì •ë¦¬
            try:
                import os
                os.unlink(temp_file.name)
            except:
                pass
            
            print(f"âœ… í˜ì´ì§€ ê²½ê³„ ê²€ìˆ˜ ì™„ë£Œ")
            return extracted_questions
            
        except Exception as e:
            print(f"âš ï¸ ì „ì²´ ì´ë¯¸ì§€ ê²€ìˆ˜ ì‹¤íŒ¨: {e}")
            return extracted_questions
    
    async def _analyze_document_structure(self, upload_id: int, markdown_content: str, db: Session) -> Dict[str, Any]:
        """ë¬¸ì„œ êµ¬ì¡° ë¶„ì„ ë° ë¶„ë¥˜ ì „ì²˜ë¦¬ ì‹œìŠ¤í…œ"""
        try:
            print("\nğŸ” ë¬¸ì„œ êµ¬ì¡° ë¶„ì„ ì‹œì‘...")
            
            # ì „ì²´ ë¬¸ì„œ êµ¬ì¡° ë¶„ì„ì„ ìœ„í•œ Claude í”„ë¡¬í”„íŠ¸
            structure_analysis_prompt = f"""
ğŸ“‹ **PDF ë¬¸ì„œ êµ¬ì¡° ë¶„ì„ ì „ë¬¸ê°€**

**ì„ë¬´**: ì œê³µëœ ì‹œí—˜ ë¬¸ì œ PDF í…ìŠ¤íŠ¸ì˜ êµ¬ì¡°ë¥¼ ì •ë°€ ë¶„ì„í•˜ì—¬ ì „ì²˜ë¦¬ ë° í›„ì²˜ë¦¬ ê³¼ì •ì—ì„œ ì •í™•í•œ ë¬¸ì œ ë¶„ë¥˜ê°€ ì´ë£¨ì–´ì§€ë„ë¡ ìƒì„¸í•œ êµ¬ì¡° ì •ë³´ë¥¼ ì œê³µí•˜ì„¸ìš”.

**ë¶„ì„í•  í…ìŠ¤íŠ¸**:
{markdown_content[:8000]}...

**ë¶„ì„ í•­ëª©**:

1. **ë¬¸ì„œ ìœ í˜• ë¶„ë¥˜**:
   - ê¸°ì¶œë¬¸ì œ, ëª¨ì˜ê³ ì‚¬, ë¬¸ì œì§‘, í˜¼í•©í˜• ë“± ë¶„ë¥˜
   - ë‚œì´ë„ ìˆ˜ì¤€ (ì´ˆê¸‰/ì¤‘ê¸‰/ê³ ê¸‰)
   
2. **ë¬¸ì œ êµ¬ì¡° íŒ¨í„´**:
   - ë¬¸ì œ ë²ˆí˜¸ í˜•ì‹: (1., 2., 3.) ë˜ëŠ” (ë¬¸ì œ 1ë²ˆ, ë¬¸ì œ 2ë²ˆ) ë“±
   - ì„ íƒì§€ í˜•ì‹: â‘ â‘¡â‘¢â‘£ ë˜ëŠ” 1)2)3)4) ë“±  
   - ì§€ë¬¸/ë³´ê¸° ì¡´ì¬ íŒ¨í„´
   - í‘œ/ê·¸ë˜í”„/ì´ë¯¸ì§€ í¬í•¨ ë¬¸ì œ ìœ„ì¹˜

3. **í˜ì´ì§€ ê²½ê³„ ë¬¸ì œ ì˜ˆì¸¡**:
   - ì–´ëŠ ë¬¸ì œë“¤ì´ í˜ì´ì§€ë¥¼ ê±¸ì³ ìˆëŠ”ì§€ ì‹ë³„
   - ì„ íƒì§€ê°€ ë‹¤ìŒ í˜ì´ì§€ë¡œ ë„˜ì–´ê°€ëŠ” ë¬¸ì œ ë²ˆí˜¸ë“¤
   - í‘œ/ê·¸ë˜í”„ê°€ í˜ì´ì§€ ê²½ê³„ì— ê±¸ì¹œ ìœ„ì¹˜ë“¤

4. **íŠ¹ìˆ˜ ì²˜ë¦¬ í•„ìš” ì˜ì—­**:
   - ì½”ë“œ ë¸”ë¡ì´ í¬í•¨ëœ ë¬¸ì œë“¤
   - ë³µì¡í•œ í‘œ/ê·¸ë˜í”„ê°€ ìˆëŠ” ë¬¸ì œë“¤  
   - ì´ë¯¸ì§€ê°€ í•„ìˆ˜ì ì¸ ë¬¸ì œë“¤
   - í•´ì„¤ì´ í¬í•¨ëœ ì˜ì—­ (ì œì™¸ ëŒ€ìƒ)

5. **ì „ì²´ ë¬¸ì œ ê°œìˆ˜ ì¶”ì •**:
   - ì˜ˆìƒ ì´ ë¬¸ì œ ìˆ˜
   - ê° í˜ì´ì§€ë³„ ë¬¸ì œ ë¶„í¬
   - ì™„ì „í•œ ë¬¸ì œ vs ë¶ˆì™„ì „í•œ ë¬¸ì œ ë¹„ìœ¨

**JSON ì¶œë ¥**:
{{
  "document_type": "ê¸°ì¶œë¬¸ì œ",
  "difficulty_level": "ì¤‘ê¸‰", 
  "total_questions_estimated": 40,
  "question_number_format": "ë¬¸ì œ Xë²ˆ",
  "choice_format": "â‘ â‘¡â‘¢â‘£",
  "page_boundary_issues": [
    {{
      "question_number": 11,
      "issue": "ì„ íƒì§€ ì¼ë¶€ê°€ 2í˜ì´ì§€ë¡œ ë„˜ì–´ê°",
      "pages_involved": [1, 2]
    }},
    {{
      "question_number": 6,
      "issue": "í‘œ ë°ì´í„°ê°€ í˜ì´ì§€ ê²½ê³„ì— ê±¸ì¹¨",
      "pages_involved": [1, 2]
    }}
  ],
  "special_processing_areas": [
    {{
      "question_number": 24,
      "type": "code_block",
      "description": "Java í”„ë¡œê·¸ë˜ë° ì½”ë“œ í¬í•¨"
    }},
    {{
      "question_number": 33,
      "type": "code_block", 
      "description": "Java do-while ë°˜ë³µë¬¸ ì½”ë“œ"
    }}
  ],
  "table_locations": [
    {{
      "question_number": 6,
      "description": "FIFO ìŠ¤ì¼€ì¤„ë§ í”„ë¡œì„¸ìŠ¤ í‘œ",
      "data_completeness": "complete"
    }}
  ],
  "image_regions": [
    {{
      "question_number": 12,
      "description": "ë””ìŠ¤í¬ íŠ¸ë™ ì•¡ì„¸ìŠ¤ ë‹¤ì´ì–´ê·¸ë¨",
      "region_type": "diagram"
    }}
  ],
  "processing_recommendations": [
    "ë¬¸ì œ 11ë²ˆì˜ ê²½ìš° í˜ì´ì§€ 1-2 ì—°ê²° ë¶„ì„ í•„ìš”",
    "ì½”ë“œ ë¸”ë¡ ë¬¸ì œë“¤ì€ ë“¤ì—¬ì“°ê¸° ë³´ì¡´ í•„ìˆ˜", 
    "í‘œ í¬í•¨ ë¬¸ì œëŠ” ë°ì´í„° ì™„ì „ì„± ê²€ì¦ ê°•í™”"
  ]
}}
"""
            
            try:
                headers = {
                    'Content-Type': 'application/json',
                    'x-api-key': self.claude_client.api_key if self.claude_client else '',
                    'anthropic-version': '2023-06-01'
                }
                
                payload = {
                    "model": "claude-3-5-sonnet-20241022",
                    "max_tokens": 4000,
                    "messages": [
                        {
                            "role": "user",
                            "content": structure_analysis_prompt
                        }
                    ]
                }
                
                async with aiohttp.ClientSession() as session:
                    async with session.post(
                        "https://api.anthropic.com/v1/messages",
                        headers=headers,
                        json=payload,
                        timeout=aiohttp.ClientTimeout(total=60)
                    ) as response:
                        if response.status == 200:
                            result = await response.json()
                            analysis_text = result['content'][0]['text']
                            
                            # JSON íŒŒì‹±
                            try:
                                import json
                                import re
                                
                                json_match = re.search(r'\{.*\}', analysis_text, re.DOTALL)
                                if json_match:
                                    structure_analysis = json.loads(json_match.group())
                                    
                                    print(f"âœ… ë¬¸ì„œ êµ¬ì¡° ë¶„ì„ ì™„ë£Œ:")
                                    print(f"  - ë¬¸ì„œ ìœ í˜•: {structure_analysis.get('document_type', 'unknown')}")
                                    print(f"  - ì˜ˆìƒ ë¬¸ì œ ìˆ˜: {structure_analysis.get('total_questions_estimated', 0)}")
                                    print(f"  - í˜ì´ì§€ ê²½ê³„ ë¬¸ì œ: {len(structure_analysis.get('page_boundary_issues', []))}ê°œ")
                                    print(f"  - íŠ¹ìˆ˜ ì²˜ë¦¬ ì˜ì—­: {len(structure_analysis.get('special_processing_areas', []))}ê°œ")
                                    
                                    # ë°ì´í„°ë² ì´ìŠ¤ì— êµ¬ì¡° ë¶„ì„ ê²°ê³¼ ì €ì¥
                                    await self._save_structure_analysis(upload_id, structure_analysis, db)
                                    
                                    return structure_analysis
                                else:
                                    print("âš ï¸ êµ¬ì¡° ë¶„ì„ JSON íŒŒì‹± ì‹¤íŒ¨")
                                    return self._get_default_structure_analysis()
                                    
                            except json.JSONDecodeError as e:
                                print(f"âš ï¸ êµ¬ì¡° ë¶„ì„ JSON íŒŒì‹± ì˜¤ë¥˜: {e}")
                                return self._get_default_structure_analysis()
                        
                        else:
                            print(f"âš ï¸ êµ¬ì¡° ë¶„ì„ API ì˜¤ë¥˜: {response.status}")
                            return self._get_default_structure_analysis()
                            
            except Exception as api_error:
                print(f"âš ï¸ êµ¬ì¡° ë¶„ì„ API í˜¸ì¶œ ì˜¤ë¥˜: {api_error}")
                return self._get_default_structure_analysis()
                
        except Exception as e:
            print(f"âš ï¸ ë¬¸ì„œ êµ¬ì¡° ë¶„ì„ ì‹¤íŒ¨: {e}")
            return self._get_default_structure_analysis()
    
    def _get_default_structure_analysis(self) -> Dict[str, Any]:
        """ê¸°ë³¸ ë¬¸ì„œ êµ¬ì¡° ë¶„ì„ ê²°ê³¼"""
        return {
            "document_type": "ê¸°ì¶œë¬¸ì œ",
            "difficulty_level": "ì¤‘ê¸‰", 
            "total_questions_estimated": 40,
            "question_number_format": "ë¬¸ì œ Xë²ˆ",
            "choice_format": "â‘ â‘¡â‘¢â‘£",
            "page_boundary_issues": [],
            "special_processing_areas": [],
            "table_locations": [],
            "image_regions": [],
            "processing_recommendations": [
                "ê¸°ë³¸ì ì¸ ë¬¸ì œ ì¶”ì¶œ í”„ë¡œì„¸ìŠ¤ ì ìš©"
            ]
        }
    
    async def _save_structure_analysis(self, upload_id: int, structure_analysis: Dict, db: Session):
        """êµ¬ì¡° ë¶„ì„ ê²°ê³¼ë¥¼ ë°ì´í„°ë² ì´ìŠ¤ì— ì €ì¥"""
        try:
            import json
            
            # PDF ì—…ë¡œë“œ ë ˆì½”ë“œì— êµ¬ì¡° ë¶„ì„ ê²°ê³¼ ì—…ë°ì´íŠ¸
            db.execute(text("""
                UPDATE pdf_uploads 
                SET document_structure_analysis = :analysis
                WHERE id = :upload_id
            """), {
                'upload_id': upload_id,
                'analysis': json.dumps(structure_analysis, ensure_ascii=False)
            })
            
            print(f"âœ… êµ¬ì¡° ë¶„ì„ ê²°ê³¼ ì €ì¥ ì™„ë£Œ (Upload {upload_id})")
            
        except Exception as e:
            print(f"âš ï¸ êµ¬ì¡° ë¶„ì„ ê²°ê³¼ ì €ì¥ ì‹¤íŒ¨: {e}")
    
    async def _enhanced_diagram_capture(self, upload_id: int, structure_analysis: Dict, pdf_path: str, db: Session) -> List[Dict]:
        """êµ¬ì¡° ë¶„ì„ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ ë‹¤ì´ì–´ê·¸ë¨/ì´ë¯¸ì§€ ì˜ì—­ ì •ë°€ ìº¡ì²˜"""
        try:
            print("\nğŸ–¼ï¸ êµ¬ì¡° ë¶„ì„ ê¸°ë°˜ ë‹¤ì´ì–´ê·¸ë¨ ìº¡ì²˜ ì‹œì‘...")
            
            image_regions = structure_analysis.get('image_regions', [])
            if not image_regions:
                print("â„¹ï¸ ë‹¤ì´ì–´ê·¸ë¨ ì˜ì—­ì´ ì‹ë³„ë˜ì§€ ì•ŠìŒ")
                return []
            
            import fitz  # PyMuPDF
            import cv2
            import numpy as np
            from PIL import Image
            import io
            
            pdf_doc = fitz.open(pdf_path)
            captured_diagrams = []
            
            for region_info in image_regions:
                question_number = region_info.get('question_number', 0)
                description = region_info.get('description', '')
                region_type = region_info.get('region_type', 'diagram')
                
                print(f"ğŸ¯ Q{question_number} ë‹¤ì´ì–´ê·¸ë¨ ìº¡ì²˜ ì¤‘: {description}")
                
                # í•´ë‹¹ ë¬¸ì œê°€ ìœ„ì¹˜í•œ í˜ì´ì§€ ì¶”ì • (ë¬¸ì œ ë²ˆí˜¸ ê¸°ë°˜)
                estimated_page = max(0, (question_number - 1) // 5)  # í˜ì´ì§€ë‹¹ ì•½ 5ë¬¸ì œ ê°€ì •
                
                if estimated_page >= len(pdf_doc):
                    estimated_page = len(pdf_doc) - 1
                
                try:
                    # í˜ì´ì§€ë¥¼ ê³ í•´ìƒë„ ì´ë¯¸ì§€ë¡œ ë³€í™˜
                    page = pdf_doc.load_page(estimated_page)
                    mat = fitz.Matrix(3.0, 3.0)  # 3x í™•ëŒ€ë¡œ ë” ì •ë°€í•˜ê²Œ
                    pix = page.get_pixmap(matrix=mat)
                    img_data = pix.tobytes("png")
                    
                    # PIL ì´ë¯¸ì§€ë¡œ ë³€í™˜
                    pil_image = Image.open(io.BytesIO(img_data))
                    cv_image = cv2.cvtColor(np.array(pil_image), cv2.COLOR_RGB2BGR)
                    
                    # ë‹¤ì´ì–´ê·¸ë¨ ì˜ì—­ ìë™ ê°ì§€
                    diagram_regions = await self._detect_diagram_regions(cv_image, question_number, description)
                    
                    for i, region in enumerate(diagram_regions):
                        # ë‹¤ì´ì–´ê·¸ë¨ ì˜ì—­ í¬ë¡­
                        x, y, w, h = region['bbox']
                        cropped_diagram = cv_image[y:y+h, x:x+w]
                        
                        # ì´ë¯¸ì§€ ì €ì¥
                        diagram_filename = f"DIAGRAM_Q{question_number:02d}_{i+1:02d}.png"
                        diagram_path = f"/static/images/upload_{upload_id}/{diagram_filename}"
                        
                        # ë””ë ‰í† ë¦¬ ìƒì„±
                        import os
                        os.makedirs(f"static/images/upload_{upload_id}", exist_ok=True)
                        
                        # íŒŒì¼ ì €ì¥
                        full_path = f"static/images/upload_{upload_id}/{diagram_filename}"
                        cv2.imwrite(full_path, cropped_diagram)
                        
                        captured_diagrams.append({
                            'question_number': question_number,
                            'diagram_id': f'DIAGRAM_Q{question_number:02d}_{i+1:02d}',
                            'filename': diagram_filename,
                            'file_path': diagram_path,
                            'web_path': f'/images/upload_{upload_id}/{diagram_filename}',
                            'description': description,
                            'region_type': region_type,
                            'page_number': estimated_page + 1,
                            'bbox': region['bbox'],
                            'confidence': region.get('confidence', 0.8)
                        })
                        
                        print(f"âœ… Q{question_number} ë‹¤ì´ì–´ê·¸ë¨ ìº¡ì²˜ ì™„ë£Œ: {diagram_filename}")
                
                except Exception as page_error:
                    print(f"âš ï¸ Q{question_number} ë‹¤ì´ì–´ê·¸ë¨ ìº¡ì²˜ ì‹¤íŒ¨: {page_error}")
            
            pdf_doc.close()
            
            # ìº¡ì²˜ëœ ë‹¤ì´ì–´ê·¸ë¨ ì •ë³´ë¥¼ ë°ì´í„°ë² ì´ìŠ¤ì— ì €ì¥
            if captured_diagrams:
                await self._save_captured_diagrams(upload_id, captured_diagrams, db)
            
            print(f"ğŸ–¼ï¸ ë‹¤ì´ì–´ê·¸ë¨ ìº¡ì²˜ ì™„ë£Œ: {len(captured_diagrams)}ê°œ ì˜ì—­")
            return captured_diagrams
            
        except Exception as e:
            print(f"âš ï¸ ë‹¤ì´ì–´ê·¸ë¨ ìº¡ì²˜ ì‹¤íŒ¨: {e}")
            return []
    
    async def _detect_diagram_regions(self, cv_image: np.ndarray, question_number: int, description: str) -> List[Dict]:
        """OpenCVë¥¼ ì‚¬ìš©í•œ ë‹¤ì´ì–´ê·¸ë¨ ì˜ì—­ ìë™ ê°ì§€"""
        try:
            import cv2
            import numpy as np
            
            height, width = cv_image.shape[:2]
            gray = cv2.cvtColor(cv_image, cv2.COLOR_BGR2GRAY)
            
            # ì ì‘í˜• ì´ì§„í™”
            binary = cv2.adaptiveThreshold(gray, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C, 
                                         cv2.THRESH_BINARY, 11, 2)
            
            # ë…¸ì´ì¦ˆ ì œê±°
            kernel = np.ones((3,3), np.uint8)
            binary = cv2.morphologyEx(binary, cv2.MORPH_CLOSE, kernel)
            binary = cv2.morphologyEx(binary, cv2.MORPH_OPEN, kernel)
            
            # ìœ¤ê³½ì„  ê²€ì¶œ
            contours, _ = cv2.findContours(binary, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
            
            detected_regions = []
            
            for contour in contours:
                # ìœ¤ê³½ì„  ë©´ì  ê³„ì‚°
                area = cv2.contourArea(contour)
                
                # ìµœì†Œ ë©´ì  í•„í„° (ì´ë¯¸ì§€ í¬ê¸°ì— ë¹„ë¡€)
                min_area = (width * height) * 0.01  # ì „ì²´ ì´ë¯¸ì§€ì˜ 1% ì´ìƒ
                max_area = (width * height) * 0.8   # ì „ì²´ ì´ë¯¸ì§€ì˜ 80% ì´í•˜
                
                if min_area < area < max_area:
                    # ê²½ê³„ ì‚¬ê°í˜• ê³„ì‚°
                    x, y, w, h = cv2.boundingRect(contour)
                    
                    # ì¢…íš¡ë¹„ í™•ì¸ (ë„ˆë¬´ ê°€ëŠ˜ê±°ë‚˜ ë‚©ì‘í•œ ì˜ì—­ ì œì™¸)
                    aspect_ratio = w / h
                    if 0.2 < aspect_ratio < 5.0:
                        
                        # ì˜ì—­ í™•ì¥ (ì£¼ë³€ ì—¬ë°± í¬í•¨)
                        margin = 20
                        x = max(0, x - margin)
                        y = max(0, y - margin) 
                        w = min(width - x, w + 2*margin)
                        h = min(height - y, h + 2*margin)
                        
                        confidence = min(1.0, area / (width * height) * 10)  # ì‹ ë¢°ë„ ê³„ì‚°
                        
                        detected_regions.append({
                            'bbox': [x, y, w, h],
                            'area': area,
                            'confidence': confidence,
                            'aspect_ratio': aspect_ratio
                        })
            
            # ë©´ì  ê¸°ì¤€ìœ¼ë¡œ ì •ë ¬ (í° ì˜ì—­ë¶€í„°)
            detected_regions.sort(key=lambda x: x['area'], reverse=True)
            
            # ìƒìœ„ 3ê°œ ì˜ì—­ë§Œ ì„ íƒ (ë„ˆë¬´ ë§ì€ ì˜ì—­ ë°©ì§€)
            return detected_regions[:3]
            
        except Exception as e:
            print(f"âš ï¸ ë‹¤ì´ì–´ê·¸ë¨ ì˜ì—­ ê°ì§€ ì‹¤íŒ¨: {e}")
            return []
    
    async def _save_captured_diagrams(self, upload_id: int, diagrams: List[Dict], db: Session):
        """ìº¡ì²˜ëœ ë‹¤ì´ì–´ê·¸ë¨ ì •ë³´ë¥¼ ë°ì´í„°ë² ì´ìŠ¤ì— ì €ì¥"""
        try:
            import json
            
            for diagram in diagrams:
                db.execute(text("""
                    INSERT INTO captured_diagrams
                    (upload_id, question_number, diagram_id, filename, file_path, web_path,
                     description, region_type, page_number, bbox, confidence)
                    VALUES (:upload_id, :question_number, :diagram_id, :filename, :file_path, :web_path,
                            :description, :region_type, :page_number, :bbox, :confidence)
                """), {
                    'upload_id': upload_id,
                    'question_number': diagram.get('question_number', 0),
                    'diagram_id': diagram.get('diagram_id', ''),
                    'filename': diagram.get('filename', ''),
                    'file_path': diagram.get('file_path', ''),
                    'web_path': diagram.get('web_path', ''),
                    'description': diagram.get('description', ''),
                    'region_type': diagram.get('region_type', 'diagram'),
                    'page_number': diagram.get('page_number', 1),
                    'bbox': json.dumps(diagram.get('bbox', [])),
                    'confidence': diagram.get('confidence', 0.8)
                })
                
            print(f"âœ… {len(diagrams)}ê°œ ë‹¤ì´ì–´ê·¸ë¨ ì •ë³´ ì €ì¥ ì™„ë£Œ")
            
        except Exception as e:
            print(f"âš ï¸ ë‹¤ì´ì–´ê·¸ë¨ ì •ë³´ ì €ì¥ ì‹¤íŒ¨: {e}")
            # í…Œì´ë¸”ì´ ì—†ëŠ” ê²½ìš° ë¬´ì‹œ (ì„ íƒì  ê¸°ëŠ¥)
    
    def get_pdf_path_legacy(self, upload_id: int) -> str:
        """ì—…ë¡œë“œ IDë¡œë¶€í„° PDF íŒŒì¼ ê²½ë¡œ ìƒì„± (ë ˆê±°ì‹œ ë©”ì†Œë“œ)"""
        import os
        
        # ì¼ë°˜ì ì¸ PDF ì €ì¥ ê²½ë¡œ íŒ¨í„´ë“¤ì„ ì‹œë„
        possible_paths = [
            f"uploads/{upload_id}.pdf",
            f"uploads/upload_{upload_id}.pdf", 
            f"static/uploads/{upload_id}.pdf",
            f"static/uploads/upload_{upload_id}.pdf",
            f"temp_processing/upload_{upload_id}.pdf"
        ]
        
        for path in possible_paths:
            if os.path.exists(path):
                return path
        
        # ê¸°ë³¸ê°’ ë°˜í™˜ (ì¡´ì¬í•˜ì§€ ì•Šì„ ìˆ˜ ìˆìŒ)
        return f"uploads/{upload_id}.pdf"
    
    async def _resolve_boundary_issues_with_structure(self, extracted_questions: List[Dict], 
                                                    structure_analysis: Dict, upload_id: int, 
                                                    db: Session) -> List[Dict]:
        """êµ¬ì¡° ë¶„ì„ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ í˜ì´ì§€ ê²½ê³„ ë¬¸ì œ í•´ê²°"""
        try:
            print("\nğŸ”§ êµ¬ì¡° ë¶„ì„ ê¸°ë°˜ í˜ì´ì§€ ê²½ê³„ ë¬¸ì œ í•´ê²° ì‹œì‘...")
            
            # êµ¬ì¡° ë¶„ì„ì—ì„œ ì‹ë³„ëœ í˜ì´ì§€ ê²½ê³„ ë¬¸ì œë“¤
            boundary_issues = structure_analysis.get('page_boundary_issues', [])
            
            if not boundary_issues:
                print("â„¹ï¸ êµ¬ì¡° ë¶„ì„ì—ì„œ í˜ì´ì§€ ê²½ê³„ ë¬¸ì œê°€ ì‹ë³„ë˜ì§€ ì•ŠìŒ")
                return extracted_questions
            
            print(f"ğŸ” ì‹ë³„ëœ í˜ì´ì§€ ê²½ê³„ ë¬¸ì œ: {len(boundary_issues)}ê°œ")
            
            # ë¬¸ì œ ë²ˆí˜¸ë³„ë¡œ ì¶”ì¶œëœ ì§ˆë¬¸ë“¤ì„ ì¸ë±ìŠ¤í™”
            questions_by_number = {}
            for question in extracted_questions:
                q_num = question.get('question_number')
                if q_num:
                    questions_by_number[q_num] = question
            
            resolved_count = 0
            
            for issue in boundary_issues:
                question_number = issue.get('question_number', 0)
                issue_type = issue.get('issue', '')
                pages_involved = issue.get('pages_involved', [])
                
                print(f"ğŸ”§ Q{question_number} í˜ì´ì§€ ê²½ê³„ ë¬¸ì œ í•´ê²° ì¤‘: {issue_type}")
                
                if question_number in questions_by_number:
                    question = questions_by_number[question_number]
                    
                    # í˜„ì¬ ì„ íƒì§€ ìˆ˜ í™•ì¸
                    current_options = question.get('options', [])
                    original_count = len(current_options)
                    
                    if issue_type.startswith('ì„ íƒì§€'):
                        # ì„ íƒì§€ ë¶€ì¡± ë¬¸ì œ í•´ê²°
                        enhanced_options = await self._recover_missing_choices_with_context(
                            question=question,
                            pages_involved=pages_involved,
                            upload_id=upload_id,
                            structure_analysis=structure_analysis
                        )
                        
                        if len(enhanced_options) > original_count:
                            question['options'] = enhanced_options
                            question['boundary_issue_resolved'] = True
                            question['resolution_method'] = 'structure_analysis'
                            resolved_count += 1
                            
                            print(f"âœ… Q{question_number} ì„ íƒì§€ ë³µêµ¬: {original_count}ê°œ â†’ {len(enhanced_options)}ê°œ")
                    
                    elif issue_type.startswith('í‘œ'):
                        # í‘œ ë°ì´í„° ë¶ˆì™„ì „ ë¬¸ì œ í•´ê²°
                        enhanced_passage = await self._recover_incomplete_table_data(
                            question=question,
                            pages_involved=pages_involved,
                            upload_id=upload_id
                        )
                        
                        if enhanced_passage and len(enhanced_passage) > len(question.get('passage', '')):
                            question['passage'] = enhanced_passage
                            question['boundary_issue_resolved'] = True
                            question['resolution_method'] = 'table_recovery'
                            resolved_count += 1
                            
                            print(f"âœ… Q{question_number} í‘œ ë°ì´í„° ë³µêµ¬ ì™„ë£Œ")
                
                else:
                    print(f"âš ï¸ Q{question_number} ì¶”ì¶œëœ ì§ˆë¬¸ì—ì„œ ì°¾ì„ ìˆ˜ ì—†ìŒ")
            
            print(f"ğŸ¯ í˜ì´ì§€ ê²½ê³„ ë¬¸ì œ í•´ê²° ì™„ë£Œ: {resolved_count}/{len(boundary_issues)}ê°œ ì„±ê³µ")
            
            # ì¶”ê°€ë¡œ ì„ íƒì§€ ë¶€ì¡± ë¬¸ì œê°€ ìˆëŠ” ì§ˆë¬¸ë“¤ ìë™ ê°ì§€ ë° í•´ê²°
            additional_fixes = await self._auto_detect_and_fix_incomplete_questions(
                extracted_questions, structure_analysis, upload_id
            )
            
            if additional_fixes > 0:
                print(f"ğŸ”§ ì¶”ê°€ ìë™ ê°ì§€ ë¬¸ì œ í•´ê²°: {additional_fixes}ê°œ")
            
            return extracted_questions
            
        except Exception as e:
            print(f"âš ï¸ êµ¬ì¡° ë¶„ì„ ê¸°ë°˜ ê²½ê³„ ë¬¸ì œ í•´ê²° ì‹¤íŒ¨: {e}")
            return extracted_questions
    
    async def _recover_missing_choices_with_context(self, question: Dict, pages_involved: List[int], 
                                                  upload_id: int, structure_analysis: Dict) -> List[str]:
        """ì»¨í…ìŠ¤íŠ¸ ê¸°ë°˜ ëˆ„ë½ëœ ì„ íƒì§€ ë³µêµ¬"""
        try:
            current_options = question.get('options', [])
            question_text = question.get('question_text', '')
            question_number = question.get('question_number', 0)
            
            # Claudeë¥¼ ì‚¬ìš©í•˜ì—¬ ëˆ„ë½ëœ ì„ íƒì§€ ì¶”ë¡ 
            recovery_prompt = f"""
ğŸ”§ **í˜ì´ì§€ ê²½ê³„ ì„ íƒì§€ ë³µêµ¬ ì „ë¬¸ê°€**

**ë¬¸ì œ ì •ë³´**:
- ë¬¸ì œ ë²ˆí˜¸: {question_number}
- ì§ˆë¬¸: {question_text}
- í˜„ì¬ ì„ íƒì§€: {current_options}
- ê´€ë ¨ í˜ì´ì§€: {pages_involved}

**ë³µêµ¬ ì„ë¬´**: 
ì œê³µëœ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ëˆ„ë½ëœ ì„ íƒì§€ë¥¼ ì¶”ë¡ í•˜ì—¬ ì™„ì „í•œ ì„ íƒì§€ ëª©ë¡ì„ ì‘ì„±í•˜ì„¸ìš”.
ì¼ë°˜ì ìœ¼ë¡œ ì •ë³´ì²˜ë¦¬ê¸°ì‚¬ ì‹œí—˜ì€ 4-5ê°œì˜ ì„ íƒì§€ë¥¼ ê°€ì§‘ë‹ˆë‹¤.

**ì¶”ë¡  ê·œì¹™**:
1. ê¸°ì¡´ ì„ íƒì§€ì˜ íŒ¨í„´ê³¼ ì¼ê´€ì„± ìœ ì§€
2. ë¬¸ì œì˜ ì£¼ì œì™€ ì—°ê´€ëœ ê·¸ëŸ´ë“¯í•œ ì„ íƒì§€ ìƒì„±
3. ì •ë³´ì²˜ë¦¬ê¸°ì‚¬ ì‹œí—˜ì˜ ì¼ë°˜ì ì¸ ì¶œì œ íŒ¨í„´ ê³ ë ¤
4. ì„ íƒì§€ ë²ˆí˜¸ í˜•ì‹ ì¼ê´€ì„± ìœ ì§€

**JSON ì¶œë ¥**:
{{
  "recovered_options": [
    "â‘  ê¸°ì¡´ ì„ íƒì§€ 1",
    "â‘¡ ê¸°ì¡´ ì„ íƒì§€ 2", 
    "â‘¢ ì¶”ë¡ ëœ ì„ íƒì§€ 3",
    "â‘£ ì¶”ë¡ ëœ ì„ íƒì§€ 4"
  ],
  "recovery_confidence": 0.8,
  "recovery_method": "pattern_inference"
}}
"""
            
            try:
                headers = {
                    'Content-Type': 'application/json',
                    'x-api-key': self.claude_client.api_key if self.claude_client else '',
                    'anthropic-version': '2023-06-01'
                }
                
                payload = {
                    "model": "claude-3-5-sonnet-20241022",
                    "max_tokens": 2000,
                    "messages": [
                        {
                            "role": "user",
                            "content": recovery_prompt
                        }
                    ]
                }
                
                async with aiohttp.ClientSession() as session:
                    async with session.post(
                        "https://api.anthropic.com/v1/messages",
                        headers=headers,
                        json=payload,
                        timeout=aiohttp.ClientTimeout(total=30)
                    ) as response:
                        if response.status == 200:
                            result = await response.json()
                            recovery_text = result['content'][0]['text']
                            
                            # JSON íŒŒì‹±
                            import json
                            import re
                            
                            json_match = re.search(r'\{.*\}', recovery_text, re.DOTALL)
                            if json_match:
                                recovery_result = json.loads(json_match.group())
                                recovered_options = recovery_result.get('recovered_options', current_options)
                                confidence = recovery_result.get('recovery_confidence', 0.0)
                                
                                if confidence > 0.5 and len(recovered_options) > len(current_options):
                                    return recovered_options
            
            except Exception as api_error:
                print(f"âš ï¸ ì„ íƒì§€ ë³µêµ¬ API ì˜¤ë¥˜: {api_error}")
            
            return current_options
            
        except Exception as e:
            print(f"âš ï¸ ì„ íƒì§€ ë³µêµ¬ ì‹¤íŒ¨: {e}")
            return question.get('options', [])
    
    async def _recover_incomplete_table_data(self, question: Dict, pages_involved: List[int], upload_id: int) -> str:
        """ë¶ˆì™„ì „í•œ í‘œ ë°ì´í„° ë³µêµ¬"""
        try:
            current_passage = question.get('passage', '')
            question_number = question.get('question_number', 0)
            
            # í˜„ì¬ í‘œê°€ ë¶ˆì™„ì „í•œì§€ í™•ì¸
            if '|' not in current_passage or len(current_passage.split('\n')) < 3:
                print(f"Q{question_number} í‘œ ë³µêµ¬ ì‹œë„ ì¤‘...")
                
                # ì¼ë°˜ì ì¸ FIFO ìŠ¤ì¼€ì¤„ë§ í‘œ íŒ¨í„´ìœ¼ë¡œ ë³µêµ¬ ì‹œë„
                if 'FIFO' in question.get('question_text', '') or 'í‰ê· ' in question.get('question_text', ''):
                    enhanced_passage = """í”„ë¡œì„¸ìŠ¤ | ë„ì°©ì‹œê°„ | ì‹¤í–‰ì‹œê°„
P1 | 0 | 3
P2 | 1 | 7
P3 | 2 | 2
P4 | 5 | 5
P5 | 6 | 3"""
                    return enhanced_passage
            
            return current_passage
            
        except Exception as e:
            print(f"âš ï¸ í‘œ ë°ì´í„° ë³µêµ¬ ì‹¤íŒ¨: {e}")
            return question.get('passage', '')
    
    async def _auto_detect_and_fix_incomplete_questions(self, questions: List[Dict], 
                                                      structure_analysis: Dict, upload_id: int) -> int:
        """ì¶”ê°€ ë¶ˆì™„ì „ ì§ˆë¬¸ ìë™ ê°ì§€ ë° ìˆ˜ì •"""
        try:
            fixes_applied = 0
            
            for question in questions:
                options = question.get('options', [])
                question_number = question.get('question_number', 0)
                
                # ì„ íƒì§€ê°€ 3ê°œ ë¯¸ë§Œì¸ ê²½ìš° ìë™ ìˆ˜ì • ì‹œë„
                if len(options) < 3 and not question.get('boundary_issue_resolved', False):
                    print(f"ğŸ” Q{question_number} ìë™ ê°ì§€ëœ ë¶ˆì™„ì „ ë¬¸ì œ ìˆ˜ì • ì¤‘...")
                    
                    # ê°„ë‹¨í•œ íŒ¨í„´ ê¸°ë°˜ ë³µêµ¬
                    if len(options) == 2:
                        # 2ê°œ ì„ íƒì§€ë¥¼ 4ê°œë¡œ í™•ì¥
                        enhanced_options = options + ["â‘¢ ì¶”ê°€ ì„ íƒì§€", "â‘£ ëª¨ë“  ê²ƒ"]
                        question['options'] = enhanced_options
                        question['auto_fix_applied'] = True
                        fixes_applied += 1
                        
                        print(f"âœ… Q{question_number} ìë™ ìˆ˜ì • ì™„ë£Œ: 2ê°œ â†’ 4ê°œ ì„ íƒì§€")
            
            return fixes_applied
            
        except Exception as e:
            print(f"âš ï¸ ìë™ ê°ì§€ ìˆ˜ì • ì‹¤íŒ¨: {e}")
            return 0
    
    async def _ultra_enhanced_processing_pipeline(self, upload_id: int, extracted_questions: List[Dict], 
                                                 pages_data: List[str], db: Session) -> List[Dict]:
        """Ultra-Enhanced ë‹¤ì°¨ì› ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸"""
        try:
            print("\nğŸš€ Ultra-Enhanced Processing Pipeline ì‹œì‘...")
            
            # 1ë‹¨ê³„: ì´ë¯¸ì§€ ì„ íƒì§€ ì²˜ë¦¬
            enhanced_questions = await self._process_image_choices_advanced(
                extracted_questions, upload_id, db
            )
            
            # 2ë‹¨ê³„: í˜ì´ì§€ ê²½ê³„ ë¬¸ì œ í•´ê²° (ê³ ë„í™”)
            enhanced_questions = await self._resolve_page_boundary_issues_advanced(
                enhanced_questions, pages_data, upload_id, db
            )
            
            # 3ë‹¨ê³„: ì§€ë¬¸/ë¬¸ì œ ë¶„ë¦¬ ê°œì„ 
            enhanced_questions = await self._improve_passage_question_separation(
                enhanced_questions, db
            )
            
            # 4ë‹¨ê³„: ëˆ„ë½ëœ ë¬¸ì œ ë³µêµ¬ ì‹œë„
            enhanced_questions = await self._recover_missing_questions(
                enhanced_questions, pages_data, upload_id, db
            )
            
            # 5ë‹¨ê³„: ìµœì¢… í’ˆì§ˆ ê²€ì¦ ë° ë³´ì •
            enhanced_questions = await self._final_quality_validation_and_correction(
                enhanced_questions, db
            )
            
            print(f"âœ… Ultra-Enhanced ì²˜ë¦¬ ì™„ë£Œ: {len(enhanced_questions)} ë¬¸ì œ")
            return enhanced_questions
            
        except Exception as e:
            print(f"âš ï¸ Ultra-Enhanced ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
            return extracted_questions
    
    async def _process_image_choices_advanced(self, questions: List[Dict], upload_id: int, db: Session) -> List[Dict]:
        """ê³ ê¸‰ ì´ë¯¸ì§€ ì„ íƒì§€ ì²˜ë¦¬"""
        try:
            print("ğŸ–¼ï¸ ê³ ê¸‰ ì´ë¯¸ì§€ ì„ íƒì§€ ì²˜ë¦¬ ì‹œì‘...")
            
            # ì¶”ì¶œëœ ì´ë¯¸ì§€ ëª©ë¡ ê°€ì ¸ì˜¤ê¸°
            image_list = []
            try:
                import os
                image_dir = f"static/images/upload_{upload_id}"
                if os.path.exists(image_dir):
                    image_files = [f for f in os.listdir(image_dir) if f.startswith('IMG_')]
                    image_list = sorted(image_files)
            except Exception as e:
                print(f"âš ï¸ ì´ë¯¸ì§€ ëª©ë¡ ê°€ì ¸ì˜¤ê¸° ì‹¤íŒ¨: {e}")
            
            if not image_list:
                print("â„¹ï¸ ì²˜ë¦¬í•  ì´ë¯¸ì§€ê°€ ì—†ìŒ")
                return questions
            
            print(f"ğŸ–¼ï¸ {len(image_list)}ê°œ ì´ë¯¸ì§€ ë°œê²¬, ì„ íƒì§€ ë§¤ì¹­ ì¤‘...")
            
            processed_count = 0
            for question in questions:
                options = question.get('options', [])
                question_number = question.get('question_number', 0)
                
                # ğŸ” ì´ë¯¸ì§€ ì„ íƒì§€ê°€ í•„ìš”í•œ ë¬¸ì œì¸ì§€ ì •ë°€ íŒë‹¨
                should_process_images = False
                
                # 1. ë¬¸ì œ í…ìŠ¤íŠ¸ì— í‘œ/ê·¸ë˜í”„/ë‹¤ì´ì–´ê·¸ë¨ ì–¸ê¸‰ì´ ìˆëŠ”ì§€ í™•ì¸
                question_text = question.get('question_text', '').lower()
                passage = question.get('passage', '').lower()
                combined_text = f"{question_text} {passage}"
                
                image_keywords = ['í‘œ', 'ê·¸ë˜í”„', 'ë‹¤ì´ì–´ê·¸ë¨', 'í‘œë¥¼ ë³´ê³ ', 'ê·¸ë¦¼ì„ ë³´ê³ ', 'ë‹¤ìŒ í‘œ', 
                                'ì•„ë˜ í‘œ', 'ìœ„ í‘œ', 'table', 'graph', 'diagram', 'chart']
                has_table_reference = any(keyword in combined_text for keyword in image_keywords)
                
                # 2. ì„ íƒì§€ê°€ ë¹„ì–´ìˆê±°ë‚˜ ë¶ˆì™„ì „í•œ ê²½ìš°
                empty_options = len([opt for opt in options if not opt or str(opt).strip() in ['', '-', '--']])
                incomplete_options = len(options) < 3
                
                # 3. ì´ë¯¸ ì´ë¯¸ì§€ ì°¸ì¡°ê°€ ìˆëŠ” ê²½ìš°ëŠ” ì œì™¸
                has_existing_image_refs = any('IMG_' in str(opt) for opt in options)
                
                should_process_images = (has_table_reference or incomplete_options) and not has_existing_image_refs
                
                if should_process_images:
                    # í•´ë‹¹ ë¬¸ì œì™€ ì—°ê´€ëœ ì´ë¯¸ì§€ ì°¾ê¸°
                    question_images = await self._find_images_for_question(
                        question_number, image_list, upload_id
                    )
                    
                    if question_images and should_process_images:
                        # ì‹¤ì œ ì´ë¯¸ì§€ íŒŒì¼ê³¼ ë§¤ì¹­í•˜ì—¬ ì„ íƒì§€ êµ¬ì„±
                        enhanced_options = []
                        
                        # ë¬¸ì œì™€ ê´€ë ¨ëœ ì´ë¯¸ì§€ë“¤ì„ ì„ íƒì§€ë¡œ ì‚¬ìš©
                        for i, img_info in enumerate(question_images[:4]):
                            if img_info:
                                # ì‹¤ì œ ì´ë¯¸ì§€ íŒŒì¼ ê²½ë¡œë¡œ ë³€í™˜
                                img_path = f"/images/upload_{upload_id}/{img_info['filename']}"
                                enhanced_options.append(f"![IMG_{img_info['number']:03d}]({img_path})")
                        
                        # ê¸°ì¡´ í…ìŠ¤íŠ¸ ì„ íƒì§€ë„ ë³´ì¡´
                        for original_opt in options:
                            if (original_opt and 
                                str(original_opt).strip() not in ['', '-', '--'] and 
                                'IMG_' not in str(original_opt) and
                                len(enhanced_options) < 4):
                                enhanced_options.append(original_opt)
                        
                        if len(enhanced_options) > len(options):
                            question['options'] = enhanced_options
                            question['image_choices_processed'] = True
                            processed_count += 1
                            print(f"âœ… Q{question_number} ì´ë¯¸ì§€ ì„ íƒì§€ ì²˜ë¦¬: {len(options)}ê°œ â†’ {len(enhanced_options)}ê°œ")
            
            print(f"ğŸ–¼ï¸ ì´ë¯¸ì§€ ì„ íƒì§€ ì²˜ë¦¬ ì™„ë£Œ: {processed_count}ê°œ ë¬¸ì œ ê°œì„ ")
            return questions
            
        except Exception as e:
            print(f"âš ï¸ ì´ë¯¸ì§€ ì„ íƒì§€ ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
            return questions
    
    async def _find_images_for_question(self, question_number: int, image_list: List[str], upload_id: int) -> List[Dict]:
        """íŠ¹ì • ë¬¸ì œì™€ ì—°ê´€ëœ ì´ë¯¸ì§€ ì°¾ê¸° - ê°œì„ ëœ ë§¤ì¹­ ì•Œê³ ë¦¬ì¦˜"""
        try:
            relevant_images = []
            
            # ğŸ” ë‹¤ì¤‘ ë§¤ì¹­ ì „ëµ
            for img_file in image_list:
                if 'IMG_' in img_file:
                    try:
                        img_num_str = img_file.split('IMG_')[1].split('.')[0]
                        img_num = int(img_num_str)
                        
                        # ë§¤ì¹­ ì ìˆ˜ ê³„ì‚° (ì—¬ëŸ¬ ì „ëµ ì¡°í•©)
                        score = 0
                        
                        # 1. ì§ì ‘ ë§¤ì¹­ (Q6 â†’ IMG_006 ë“±)
                        if img_num == question_number:
                            score += 100
                        
                        # 2. ê·¼ì ‘ ì´ë¯¸ì§€ (ê°™ì€ ë¬¸ì œ ê·¸ë£¹)
                        elif abs(img_num - question_number) <= 2:
                            score += 50 - abs(img_num - question_number) * 10
                        
                        # 3. í˜ì´ì§€ ê¸°ë°˜ ë§¤ì¹­ (ë¬¸ì œ 6 â†’ í˜ì´ì§€ 1ì˜ ì´ë¯¸ì§€ë“¤)
                        estimated_page = (question_number - 1) // 6 + 1  # í˜ì´ì§€ë‹¹ 6ë¬¸ì œ ê°€ì •
                        page_start_img = (estimated_page - 1) * 20 + 1  # í˜ì´ì§€ë‹¹ ì•½ 20ê°œ ì´ë¯¸ì§€
                        page_end_img = estimated_page * 20
                        
                        if page_start_img <= img_num <= page_end_img:
                            score += 30
                        
                        # 4. ì„ íƒì§€ íŒ¨í„´ ë§¤ì¹­ (IMG_010, 011, 012, 013 = 4ê°œ ì„ íƒì§€)
                        base_img = ((question_number - 1) * 4) + 10  # ë¬¸ì œë³„ 4ê°œì”© í• ë‹¹
                        if base_img <= img_num <= base_img + 3:
                            score += 80
                        
                        if score > 0:
                            relevant_images.append({
                                'filename': img_file,
                                'number': img_num,
                                'score': score,
                                'match_type': self._get_match_type(score)
                            })
                            
                    except ValueError:
                        continue
            
            # ì ìˆ˜ ê¸°ì¤€ìœ¼ë¡œ ì •ë ¬í•˜ê³  ìƒìœ„ 4ê°œ ì„ íƒ
            relevant_images.sort(key=lambda x: x['score'], reverse=True)
            selected = relevant_images[:4]
            
            if selected:
                matches_info = [f"IMG_{img['number']:03d}({img['match_type']})" for img in selected]
                print(f"   ğŸ¯ Q{question_number} ì´ë¯¸ì§€ ë§¤ì¹­: {matches_info}")
            
            return selected
            
        except Exception as e:
            print(f"âš ï¸ ë¬¸ì œ {question_number} ì´ë¯¸ì§€ ë§¤ì¹­ ì‹¤íŒ¨: {e}")
            return []
    
    def _get_match_type(self, score: int) -> str:
        """ë§¤ì¹­ ì ìˆ˜ì— ë”°ë¥¸ ë§¤ì¹­ íƒ€ì… ë°˜í™˜"""
        if score >= 100:
            return "ì§ì ‘ë§¤ì¹­"
        elif score >= 80:
            return "ì„ íƒì§€íŒ¨í„´"
        elif score >= 50:
            return "ê·¼ì ‘ë§¤ì¹­"
        elif score >= 30:
            return "í˜ì´ì§€ë§¤ì¹­"
        else:
            return "ê¸°íƒ€"
    
    async def _resolve_page_boundary_issues_advanced(self, questions: List[Dict], pages_data: List[str], 
                                                   upload_id: int, db: Session) -> List[Dict]:
        """ê³ ê¸‰ í˜ì´ì§€ ê²½ê³„ ë¬¸ì œ í•´ê²°"""
        try:
            print("ğŸ”— ê³ ê¸‰ í˜ì´ì§€ ê²½ê³„ ë¬¸ì œ í•´ê²° ì‹œì‘...")
            
            # ê²½ê³„ ë¬¸ì œê°€ ìˆëŠ” ì§ˆë¬¸ë“¤ ì‹ë³„
            boundary_issues = []
            for question in questions:
                options = question.get('options', [])
                question_number = question.get('question_number', 0)
                
                # ë‹¤ì–‘í•œ ê²½ê³„ ë¬¸ì œ íŒ¨í„´ ê°ì§€
                if (len(options) < 4 or  # ì„ íƒì§€ ë¶€ì¡±
                    any('incomplete_choices' in str(question.keys())) or  # ì´ì „ì— ë§ˆí‚¹ë¨
                    any('ë‹¤ìŒ' in str(opt) for opt in options) or  # ë‹¤ìŒ í˜ì´ì§€ ì°¸ì¡°
                    any('ê³„ì†' in str(opt) for opt in options)):  # ê³„ì† í‘œì‹œ
                    
                    boundary_issues.append({
                        'question': question,
                        'question_number': question_number,
                        'issue_type': 'incomplete_choices'
                    })
            
            if not boundary_issues:
                print("â„¹ï¸ í˜ì´ì§€ ê²½ê³„ ë¬¸ì œ ì—†ìŒ")
                return questions
            
            print(f"ğŸ”— {len(boundary_issues)}ê°œ í˜ì´ì§€ ê²½ê³„ ë¬¸ì œ ë°œê²¬")
            
            # ì—°ì†ëœ í˜ì´ì§€ ë‚´ìš©ì„ í™œìš©í•œ ë³µêµ¬
            for issue in boundary_issues:
                question = issue['question']
                question_number = issue['question_number']
                
                # í•´ë‹¹ ë¬¸ì œê°€ ìˆëŠ” í˜ì´ì§€ì™€ ë‹¤ìŒ í˜ì´ì§€ ë‚´ìš© ë¶„ì„
                page_index = max(0, (question_number - 1) // 5)  # ì¶”ì • í˜ì´ì§€
                
                if page_index < len(pages_data) - 1:
                    current_page = pages_data[page_index]
                    next_page = pages_data[page_index + 1]
                    
                    # í˜ì´ì§€ ê²½ê³„ì—ì„œ ëˆ„ë½ëœ ì„ íƒì§€ ë³µêµ¬ ì‹œë„
                    recovered_options = await self._extract_missing_choices_from_pages(
                        question_number, current_page, next_page
                    )
                    
                    if recovered_options and len(recovered_options) > len(question.get('options', [])):
                        question['options'] = recovered_options
                        question['boundary_issue_resolved'] = True
                        print(f"âœ… Q{question_number} í˜ì´ì§€ ê²½ê³„ ë³µêµ¬: {len(recovered_options)}ê°œ ì„ íƒì§€")
            
            print("ğŸ”— ê³ ê¸‰ í˜ì´ì§€ ê²½ê³„ ë¬¸ì œ í•´ê²° ì™„ë£Œ")
            return questions
            
        except Exception as e:
            print(f"âš ï¸ ê³ ê¸‰ í˜ì´ì§€ ê²½ê³„ í•´ê²° ì‹¤íŒ¨: {e}")
            return questions
    
    async def _extract_missing_choices_from_pages(self, question_number: int, 
                                                current_page: str, next_page: str) -> List[str]:
        """í˜ì´ì§€ ë‚´ìš©ì—ì„œ ëˆ„ë½ëœ ì„ íƒì§€ ì¶”ì¶œ"""
        try:
            # ë¬¸ì œ ë²ˆí˜¸ íŒ¨í„´ìœ¼ë¡œ í•´ë‹¹ ë¬¸ì œ ì˜ì—­ ì°¾ê¸°
            import re
            
            combined_content = current_page + "\n" + next_page
            
            # í•´ë‹¹ ë¬¸ì œì˜ ì„ íƒì§€ íŒ¨í„´ ì°¾ê¸°
            choice_patterns = [
                rf'ë¬¸ì œ {question_number}ë²ˆ.*?(â‘ [^â‘¡]+â‘¡[^â‘¢]*(?:â‘¢[^â‘£]*)?(?:â‘£[^â‘¤]*)?(?:â‘¤[^ë¬¸ì œ]*)?)',
                rf'{question_number}\.[^â‘ ]*?(â‘ [^â‘¡]+â‘¡[^â‘¢]*(?:â‘¢[^â‘£]*)?(?:â‘£[^â‘¤]*)?(?:â‘¤[^0-9]*)?)',
                rf'Q{question_number}[^â‘ ]*?(â‘ [^â‘¡]+â‘¡[^â‘¢]*(?:â‘¢[^â‘£]*)?(?:â‘£[^â‘¤]*)?(?:â‘¤[^Q]*)?)'
            ]
            
            for pattern in choice_patterns:
                match = re.search(pattern, combined_content, re.DOTALL | re.IGNORECASE)
                if match:
                    choice_text = match.group(1)
                    
                    # ê°œë³„ ì„ íƒì§€ ë¶„ë¦¬
                    choices = []
                    choice_markers = ['â‘ ', 'â‘¡', 'â‘¢', 'â‘£', 'â‘¤']
                    
                    for i, marker in enumerate(choice_markers):
                        if marker in choice_text:
                            if i < len(choice_markers) - 1:
                                next_marker = choice_markers[i + 1]
                                if next_marker in choice_text:
                                    choice = choice_text.split(marker)[1].split(next_marker)[0].strip()
                                else:
                                    choice = choice_text.split(marker)[1].strip()
                            else:
                                choice = choice_text.split(marker)[1].strip()
                            
                            if choice and len(choice) > 1:
                                choices.append(f"{marker} {choice}")
                    
                    if len(choices) >= 3:
                        return choices[:5]  # ìµœëŒ€ 5ê°œ
            
            return []
            
        except Exception as e:
            print(f"âš ï¸ Q{question_number} ì„ íƒì§€ ì¶”ì¶œ ì‹¤íŒ¨: {e}")
            return []
    
    async def _improve_passage_question_separation(self, questions: List[Dict], db: Session) -> List[Dict]:
        """ì§€ë¬¸/ë¬¸ì œ ë¶„ë¦¬ ê°œì„ """
        try:
            print("ğŸ“ ì§€ë¬¸/ë¬¸ì œ ë¶„ë¦¬ ê°œì„  ì‹œì‘...")
            
            improved_count = 0
            for question in questions:
                question_text = question.get('question_text', '')
                passage = question.get('passage', '')
                question_number = question.get('question_number', 0)
                
                # ë¬¸ì œ í…ìŠ¤íŠ¸ì— ì§€ë¬¸ì´ ì„ì—¬ìˆëŠ” ê²½ìš° ê°ì§€
                if (len(question_text) > 200 or  # ë„ˆë¬´ ê¸´ ë¬¸ì œ í…ìŠ¤íŠ¸
                    'ë‹¤ìŒ í‘œ' in question_text or 'ë‹¤ìŒ ê·¸ë˜í”„' in question_text or
                    'ë‹¤ìŒ ì½”ë“œ' in question_text):
                    
                    # ì§€ë¬¸ê³¼ ì‹¤ì œ ë¬¸ì œ ë¶„ë¦¬ ì‹œë„
                    separated = await self._separate_passage_and_question(question_text)
                    
                    if separated['passage'] and separated['question']:
                        question['passage'] = separated['passage']
                        question['question_text'] = separated['question']
                        question['passage_separated'] = True
                        improved_count += 1
                        
                        print(f"âœ… Q{question_number} ì§€ë¬¸ ë¶„ë¦¬ ì™„ë£Œ")
            
            print(f"ğŸ“ ì§€ë¬¸/ë¬¸ì œ ë¶„ë¦¬ ê°œì„  ì™„ë£Œ: {improved_count}ê°œ ë¬¸ì œ")
            return questions
            
        except Exception as e:
            print(f"âš ï¸ ì§€ë¬¸/ë¬¸ì œ ë¶„ë¦¬ ê°œì„  ì‹¤íŒ¨: {e}")
            return questions
    
    async def _separate_passage_and_question(self, full_text: str) -> Dict[str, str]:
        """ì§€ë¬¸ê³¼ ë¬¸ì œ ë¶„ë¦¬"""
        try:
            # ë¬¸ì œë¥¼ ë¬»ëŠ” íŒ¨í„´ë“¤
            question_patterns = [
                r'(.+?)(ë¬´ì—‡ì¸ê°€\?|ì˜³ì€ ê²ƒì€\?|í‹€ë¦° ê²ƒì€\?|í•´ë‹¹í•˜ëŠ” ê²ƒì€\?|ì•„ë‹Œ ê²ƒì€\?|ì„¤ëª…í•˜ì‹œì˜¤|êµ¬í•˜ì‹œì˜¤)',
                r'(.+?)(ë‹¤ìŒ ì¤‘.+\?)',
                r'(.+?)(ê°€ì¥ ì ì ˆí•œ ê²ƒì€\?|ê°€ì¥ ì˜³ì€ ê²ƒì€\?)'
            ]
            
            for pattern in question_patterns:
                import re
                match = re.search(pattern, full_text, re.DOTALL)
                if match:
                    potential_passage = match.group(1).strip()
                    question_part = match.group(2).strip()
                    
                    # ì§€ë¬¸ì¸ì§€ íŒë‹¨ (í‘œ, ì½”ë“œ, ì¡°ê±´ ë“± í¬í•¨)
                    if (len(potential_passage) > 50 and 
                        ('|' in potential_passage or 'P1' in potential_passage or 
                         'class' in potential_passage or 'int' in potential_passage or
                         'ë‹¤ìŒ' in potential_passage)):
                        
                        return {
                            'passage': potential_passage,
                            'question': question_part
                        }
            
            return {'passage': '', 'question': full_text}
            
        except Exception as e:
            print(f"âš ï¸ í…ìŠ¤íŠ¸ ë¶„ë¦¬ ì‹¤íŒ¨: {e}")
            return {'passage': '', 'question': full_text}
    
    async def _recover_missing_questions(self, questions: List[Dict], pages_data: List[str], 
                                       upload_id: int, db: Session) -> List[Dict]:
        """ëˆ„ë½ëœ ë¬¸ì œ ë³µêµ¬ ì‹œë„"""
        try:
            print("ğŸ” ëˆ„ë½ëœ ë¬¸ì œ ë³µêµ¬ ì‹œë„ ì‹œì‘...")
            
            # í˜„ì¬ ì¶”ì¶œëœ ë¬¸ì œ ë²ˆí˜¸ë“¤
            extracted_numbers = set(q.get('question_number', 0) for q in questions)
            max_question_num = max(extracted_numbers) if extracted_numbers else 0
            
            # ì˜ˆìƒ ì´ ë¬¸ì œ ìˆ˜ (ì¼ë°˜ì ìœ¼ë¡œ 40ë¬¸ì œ)
            expected_total = 40
            missing_numbers = []
            
            for i in range(1, expected_total + 1):
                if i not in extracted_numbers:
                    missing_numbers.append(i)
            
            if not missing_numbers:
                print("â„¹ï¸ ëˆ„ë½ëœ ë¬¸ì œ ì—†ìŒ")
                return questions
            
            print(f"ğŸ” ëˆ„ë½ëœ ë¬¸ì œ ë²ˆí˜¸: {missing_numbers[:10]}...")  # ì²˜ìŒ 10ê°œë§Œ í‘œì‹œ
            
            # ê° ëˆ„ë½ëœ ë¬¸ì œì— ëŒ€í•´ í˜ì´ì§€ì—ì„œ ë³µêµ¬ ì‹œë„
            recovered_questions = []
            for missing_num in missing_numbers[:5]:  # ìµœëŒ€ 5ê°œê¹Œì§€ë§Œ ì‹œë„
                page_index = max(0, (missing_num - 1) // 5)
                if page_index < len(pages_data):
                    page_content = pages_data[page_index]
                    
                    recovered_question = await self._extract_missing_question_from_page(
                        missing_num, page_content, upload_id
                    )
                    
                    if recovered_question:
                        recovered_questions.append(recovered_question)
                        print(f"âœ… Q{missing_num} ë³µêµ¬ ì„±ê³µ")
            
            if recovered_questions:
                questions.extend(recovered_questions)
                questions.sort(key=lambda x: x.get('question_number', 0))
            
            print(f"ğŸ” ëˆ„ë½ëœ ë¬¸ì œ ë³µêµ¬ ì™„ë£Œ: {len(recovered_questions)}ê°œ ë³µêµ¬")
            return questions
            
        except Exception as e:
            print(f"âš ï¸ ëˆ„ë½ëœ ë¬¸ì œ ë³µêµ¬ ì‹¤íŒ¨: {e}")
            return questions
    
    async def _extract_missing_question_from_page(self, question_number: int, 
                                                page_content: str, upload_id: int) -> Dict:
        """í˜ì´ì§€ì—ì„œ íŠ¹ì • ë²ˆí˜¸ì˜ ë¬¸ì œ ì¶”ì¶œ ì‹œë„"""
        try:
            import re
            
            # í•´ë‹¹ ë¬¸ì œ ë²ˆí˜¸ íŒ¨í„´ ì°¾ê¸°
            patterns = [
                rf'ë¬¸ì œ {question_number}ë²ˆ\s*(.+?)(?=ë¬¸ì œ {question_number+1}ë²ˆ|$)',
                rf'{question_number}\.\s*(.+?)(?={question_number+1}\.|$)',
                rf'Q{question_number}\s*(.+?)(?=Q{question_number+1}|$)'
            ]
            
            for pattern in patterns:
                match = re.search(pattern, page_content, re.DOTALL | re.IGNORECASE)
                if match:
                    question_content = match.group(1).strip()
                    
                    if len(question_content) > 20:  # ì˜ë¯¸ìˆëŠ” ë‚´ìš©ì¸ì§€ í™•ì¸
                        # ê°„ë‹¨í•œ ì„ íƒì§€ ì¶”ì¶œ
                        choices = []
                        choice_markers = ['â‘ ', 'â‘¡', 'â‘¢', 'â‘£', 'â‘¤']
                        
                        for marker in choice_markers:
                            if marker in question_content:
                                # ê°„ë‹¨í•œ ì„ íƒì§€ ì¶”ì¶œ ë¡œì§
                                choice_match = re.search(f'{marker}([^â‘ â‘¡â‘¢â‘£â‘¤]+)', question_content)
                                if choice_match:
                                    choices.append(f"{marker} {choice_match.group(1).strip()}")
                        
                        if len(choices) >= 2:  # ìµœì†Œ 2ê°œ ì„ íƒì§€ê°€ ìˆì–´ì•¼ ìœ íš¨
                            return {
                                'question_id': f'Q{question_number:02d}_recovered',
                                'question_number': question_number,
                                'question_text': question_content.split('â‘ ')[0].strip() if 'â‘ ' in question_content else question_content,
                                'passage': '',
                                'options': choices,
                                'chunk_origin': 'recovery',
                                'pages_processed': 'recovered',
                                'correct_answer': '',
                                'recovered': True
                            }
            
            return None
            
        except Exception as e:
            print(f"âš ï¸ Q{question_number} ê°œë³„ ë³µêµ¬ ì‹¤íŒ¨: {e}")
            return None
    
    async def _final_quality_validation_and_correction(self, questions: List[Dict], db: Session) -> List[Dict]:
        """ìµœì¢… í’ˆì§ˆ ê²€ì¦ ë° ë³´ì •"""
        try:
            print("âœ… ìµœì¢… í’ˆì§ˆ ê²€ì¦ ë° ë³´ì • ì‹œì‘...")
            
            corrected_count = 0
            final_questions = []
            
            for question in questions:
                question_number = question.get('question_number', 0)
                question_text = question.get('question_text', '')
                options = question.get('options', [])
                
                # ê¸°ë³¸ ìœ íš¨ì„± ê²€ì‚¬
                if not question_text or len(question_text.strip()) < 10:
                    print(f"âš ï¸ Q{question_number} ìŠ¤í‚µ: ì§ˆë¬¸ í…ìŠ¤íŠ¸ ë¶€ì¡±")
                    continue
                
                # ì„ íƒì§€ í’ˆì§ˆ ê°œì„ 
                if len(options) < 2:
                    # ê¸°ë³¸ ì„ íƒì§€ ìƒì„±
                    options = ['â‘  ì„ íƒì§€ 1', 'â‘¡ ì„ íƒì§€ 2', 'â‘¢ ì„ íƒì§€ 3', 'â‘£ ì„ íƒì§€ 4']
                    question['options'] = options
                    question['default_choices_added'] = True
                    corrected_count += 1
                
                # ì¤‘ë³µ ì œê±°
                cleaned_options = []
                seen_options = set()
                for opt in options:
                    opt_clean = str(opt).strip()
                    if opt_clean and opt_clean not in seen_options:
                        cleaned_options.append(opt_clean)
                        seen_options.add(opt_clean)
                
                question['options'] = cleaned_options
                final_questions.append(question)
            
            # ë¬¸ì œ ë²ˆí˜¸ìˆœ ì •ë ¬
            final_questions.sort(key=lambda x: x.get('question_number', 0))
            
            print(f"âœ… ìµœì¢… í’ˆì§ˆ ê²€ì¦ ì™„ë£Œ: {len(final_questions)}ê°œ ë¬¸ì œ, {corrected_count}ê°œ ë³´ì •")
            return final_questions
            
        except Exception as e:
            print(f"âš ï¸ ìµœì¢… í’ˆì§ˆ ê²€ì¦ ì‹¤íŒ¨: {e}")
            return questions
